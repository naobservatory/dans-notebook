{
  "hash": "db4857305cd4c44dc4af59ae7711f006",
  "result": {
    "markdown": "---\ntitle: NAO Cost Estimate MVP -- Adding noise\nauthor: Dan Rice\ndate: '2024-03-04'\nformat:\n  html:\n    code-fold: false\n    toc: true\nfilters:\n  - black-formatter\n---\n\n## Background\n\nPreviously, we have worked out a deterministic model of the sequencing depth required to detect a novel pandemic virus by the time it reaches a fixed cumulative incidence (number of people ever infected).\nHowever, in the real world, there are a number of sources of noise that will affect our ability to detect a virus.\nIn this post, we ask the question:\n**How much sequencing is required to have an $x$% chance to detect a virus by the time it reaches a fixed cumulative incidence, given a particular model of the noise?**\n\n## Model\n\nIt is useful to categorize noise sources by how they behave as various parameters like the sequencing depth change.\nFor example, three types of noise that are relevant to our problem are:\n\n1. Noise whose coefficient of variation decreases as the sequencing depth increases.\n   This includes poisson counting noise in the number of reads mapping to a sequence,\n   due to finite sampling effects.\n2. Noise whose coefficient of variation goes to a constant value as the sequencing depth increases.\n   For example, the relative abundances in a sequencing library depend on biases in enrichment efficiency.\n   If there is a class of abundant sequences that are efficiently enriched by our lab protocol, random variation in the abundance of that class of sequences will generate noise in the counts of all the other sequences that is only weakly dependent on the total read depth.\n3. Noise that depends on the number of people contributing to a sample.\n   For example, in the limit where each sample is taken from a single person,\n   the noise in the counts of reads mapping to the pandemic virus will be dominated by whether that single person is infected or not.\n\nIn the following, we consider noise classes 1. and 2.\nWe neglect 3 for the moment because in well-mixed municipal wastewater samples, we expect this to be a small effect.\nHowever, similar analysis to that presented here could be applied in that case as well.\n\nWe will consider a sequence of samples indexed by $i$, where the random variable $Y_i$ represents the number of reads corresponding to the pandemic virus in sample $i$.\nWe model $Y_i$ as independent draws from a Poisson mixture distribution:\n$$\nY_i \\sim \\text{Poisson}(X_i),\n$$\nwhere $X_i$ is a latent variable that represents excess noise not accounted for by the Poisson model.\nTo connect this model to our previous deterministic model, we set the mean of $X_i$ to the number of reads in the deterministic model:\n\n$$\nE[X_i] = \\mu_i = \\frac{n b}{N} e^{r(t_0 + j \\delta t)}\n$$\n\nwhere:\n\n- $n$ is the sequencing depth\n- $b$ is the P2RA factor\n- $N$ is the population size\n- $r$ is the growth rate of the virus\n- $t_0$ is the time of the first sample after the start of the pandemic\n- $\\delta t$ is the time between samples.\n\nNote that for simplicity this is assuming instantaneous grab sampling, which is a good approximation to 24-hr composite sampling.\n\nRecall that in our detection model, we declare a virus to be detected when the cumulative number of reads matching the virus cross a threshold.\nThus, to calculate the probability of detection, we need to calculate the probability that the cumulative number of reads ($\\sum_{j=0}^{i} Y_j$) is greater than the threshold value $\\hat{K}$.\nWe will proceed in two steps:\n\n1. Calculate the cumulant generating function of the random variable $Y = \\sum_j Y_j$.\n   This is natural to calculate because the CGF of a sum of independent random variables is the sum of their individual CGFs.\n2. Approximate the cumulative distribution function (CDF) of $Y$ from a finite set of cumulants using the [Cornish-Fisher expansion](https://en.wikipedia.org/wiki/Cornish%E2%80%93Fisher_expansion).\n   In this notebook, we will explore under what conditions we can truncate the Cornish-Fisher expansion at a certain number of terms.\n\n## Cumulant generating function of the cumulative read count, $Y$\n\nThe cumulant generating function $K_Y$ of random variable $Y$ is given by the log of its moment generating function:\n\n$$\nK_Y(z) = \\log \\mathbb{E}[e^{zY}].\n$$\n\nIf $Y_i$ is Poisson distributed with random mean $X_i$,\n\n$$\n\\begin{align}\nK_{Y_i}(z) & = \\log \\mathbb{E}\\left[\\mathbb{E}[e^{zY_{i}} | X_i]\\right] \\\\\n       & = \\log \\mathbb{E}\\left[\\exp X_i (e^{z} - 1)\\right] \\\\\n       & = K_{X_i} \\left(e^{z} - 1\\right),\n\\end{align}\n$$\nwhere the second line uses the moment-generating fuction of a Poisson random variable,\nand $K_{X_i}$ is the CGF of $X_i$.\n\nIf we assume that the $Y_i$ are independent of one another, then we can add their CGFs to get the CGF of the cumulative read count:\n$$\n\\begin{align}\nK_Y(z) & = K_{\\sum_i Y_i}(z) \\\\\n       & = \\sum_i K_{Y_i}(z) \\\\\n       & = \\sum_i K_{X_i}(e^z - 1) \\\\\n       & = K_{X}(e^z - 1),\n\\end{align}\n$$\nwhere we define $X \\equiv \\sum_i X_i$.\n\nThe last equation tells us how to combine the cumulants of $X$ to get the cumulants of $Y$.\nLet the cumulants of $Y$ be denoted $\\kappa_1, \\kappa_2, \\ldots$ and the cumulants of $X$ by $\\chi_1, \\chi_2, \\ldots$.\nExpanding the CGF gives:\n$$\n\\begin{align}\nK_Y(z) & = K_X(e^z - 1) \\\\\n       & = \\chi_1 (e^z - 1) + \\chi_2 \\frac{{(e^z-1)}^2}{2} + \\chi_3 \\frac{{(e^z-1)}^3}{3!} + \\cdots \\\\\n       & = \\chi_1 \\sum_{j=1}^{\\infty} \\frac{z^j}{j!} + \\chi_2 \\frac{{\\left(\\sum_{j=1}^{\\infty} \\frac{z^j}{j!}\\right)}^2}{2} + \\chi_3 \\frac{{\\left(\\sum_{j=1}^{\\infty} \\frac{z^j}{j!}\\right)}^3}{3!} + \\cdots \\\\\n\\end{align}\n$$\nThen, by equating powers of $z$, we find\n$$\n\\begin{align}\n\\kappa_1 & = \\chi_1 \\\\\n\\kappa_2 & = \\chi_1 + \\chi_2 \\\\\n\\kappa_3 & = \\chi_1 + 3 \\chi_2 + \\chi_3 \\\\\n\\kappa_4 & = \\chi_1 + 7 \\chi_2 + 6 \\chi_3 + \\chi_4 \\\\\n         & \\cdots\n\\end{align}\n$$\nThis cumulant series has the intuitive property that if $X \\to \\lambda$ constant, $\\chi_\\alpha \\to \\lambda \\delta_{\\alpha, 1}$ and $\\kappa_\\alpha \\to \\chi_1$.\nThat is, $Y \\to \\text{Poisson}(\\lambda)$.\nFor random $X$, in constrast, all of the cumulants of $Y$ are increased from their Poisson value of $\\chi_1$ by the cumulants of $X$.\nIn particular, the variance of $Y$, $\\kappa_2$ is equal to the Poisson variance $\\chi_1$ plus the variance of $X$, $\\chi_2$.\n\n### Cumulants of the latent variable $X$\n\nIt remains to find the cumulants of $X$, $\\chi_\\alpha$.\nFor this, we need to specify a distribution for the latent variables at each sample, $X_i$.\nFor simplicity, we will choose the Gamma distribution, which allows us to vary the mean and variance independently.\nWe will parameterize the distribution by its mean $\\mu$, and inverse dispersion $\\phi$.\nIn standard shape-scale parameterization, we have:\n$$\nX_i \\sim \\text{Gamma}(\\phi, \\mu_i / \\phi)\n$$\nwhere we assume that the inverse dispersion is constant in time.\nNote that the coefficient of variation of $X_i$ is $\\phi^{-1/2}$, independent of $\\mu_i$.\n\nThe gamma distribution has CGF:\n$$\n\\begin{align}\nK_{X_i}(z) & = - \\phi \\log(1 - \\frac{\\mu_i}{\\phi} z) \\\\\n           & = \\phi \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha} {\\left(\\frac{\\mu_i}{\\phi}z\\right)}^{\\alpha}\n\\end{align}\n$$\n\nBy the summation property of CGFs, we have the CGF of $X = \\sum_j X_j$ at time $t_i$:\n$$\n\\begin{align}\nK_{X}(z) & = \\sum_{j=0}^i K_{X_j}(z) \\\\\n         & = \\phi \\sum_{j=0}^i \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha} {\\left(\\frac{\\mu_i}{\\phi}z\\right)}^{\\alpha} \\\\\n         & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} \\left( \\sum_{j=0}^{i} \\mu_j^\\alpha \\right) z^\\alpha.\n\\end{align}\n$$\nBecause the prevalence is growing exponentially in our model, $\\mu_j$ is growing exponentially (see above).\nLetting $A \\equiv \\frac{nb}{N} e^{rt_0}$, we have $\\mu_j = A e^{rj\\delta t}$ and thus\n\n$$\n\\begin{align}\nK_X(z) & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} \\left(\\sum_{j=0}^{i} A^\\alpha e^{\\alpha r j \\delta t} \\right) z^\\alpha \\\\\n       & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} A^\\alpha \\left(\\frac{e^{\\alpha r (i+1) \\delta t} - 1}{e^{\\alpha r \\delta t} - 1} \\right) z^\\alpha \\\\\n       & \\approx \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi}{\\alpha^2 r \\delta t} {\\left(A e^{r i \\delta t}\\right)}^\\alpha \\left(\\frac{\\alpha r \\delta t e^{\\alpha r \\delta t}}{e^{\\alpha r \\delta t} - 1} \\right) z^\\alpha \\\\\n       & \\approx \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi}{\\alpha^2 r \\delta t} {\\left(A e^{r i \\delta t}\\right)}^\\alpha z^\\alpha\n\\end{align}\n$$\nWhere the first approximation requries the prevalence to be large compared to one, and the second approximation requires $\\alpha r \\delta t \\ll 1$.\n(TODO: generalize the second approximation.)\n\nIt is illuminating to parameterize the distribution of $X$ by its mean, $\\mu$, and a shape parameter $\\nu$:\n$$\n\\begin{align}\n\\mu & \\equiv K_X'(0) \\\\\n    & = \\frac{A e^{r i \\delta t}}{r \\delta t}. \\\\\n\\nu & \\equiv \\frac{\\phi}{r \\delta t}.\n\\end{align}\n$$\nSubstituting these into the equation above gives\n$$\nK_X(z) = \\nu \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha^2} {\\left(\\frac{\\mu}{\\nu}\\right)}^\\alpha z^\\alpha.\n$$\n(Note that this is similar to the CGF of the gamma distribution but with the logarithm replaced by a dilogarithm.)\n\nFinally, examination of the CGF yields the cumulants of $X$:\n$$\n\\chi_\\alpha = \\frac{(\\alpha - 1)!}{\\alpha} \\nu {\\left(\\frac{\\mu}{\\nu}\\right)}^{\\alpha}.\n$$\n\nWe can say a few things about this result:\n\n- By construction, the mean of $X$, $\\mu$, is our previous deterministic prediction for the counts.\n  (In the small $r \\delta t$ limit).\n- The shape parameter $\\nu$ controls the dispersion of $X$. $\\text{Var}[X] = \\chi_2 = \\frac{\\mu^2}{2\\nu}$. That is: larger $\\nu$ means a smaller coefficient of variation.\n- $\\nu$ is controled by the latent inverse dispersion $\\phi$ and the scaled sampling interval $r \\delta t$. Smaller sampling inverval means we take more independent samples per unit time, which reduces the variance of the sum.\n- $\\frac{\\mu}{\\nu}$ is a pure scale parameter of the distribution of $X$.\n\n### Cumulants of the cumulative counts $Y$\n\nSubstituting our equation for the cumulants of $X$ $\\chi_\\alpha$ into the equation for the cumulants of $Y$ above gives\n$$\n\\begin{align}\n\\kappa_1 & = \\mu \\\\\n\\kappa_2 & = \\mu + \\frac{1}{2} \\frac{\\mu^2}{\\nu} \\\\\n\\kappa_3 & = \\mu + \\frac{3}{2} \\frac{\\mu^2}{\\nu} + \\frac{2}{3} \\frac{\\mu^3}{\\nu^2} \\\\\n\\kappa_4 & = \\mu + \\frac{7}{2} \\frac{\\mu^2}{\\nu} + 4 \\frac{\\mu^3}{\\nu^2} + \\frac{3}{2} \\frac{\\mu^4}{\\nu^3}. \\\\\n\\end{align}\n$$\n\nWe have two regimes, controled by the parameter $\\mu / \\nu$:\n\n- If $\\frac{\\mu}{\\nu} \\ll 1$, the Poisson noise dominates and $\\kappa_\\alpha \\approx \\mu$.\n- If $\\frac{\\mu}{\\nu} \\gg 1$, the latent noise dominates and $\\kappa_\\alpha \\approx \\chi_\\alpha$.\n\nFor higher cumulants, the separation between the regimes becomes less clean (i.e. it takes a smaller/larger $\\mu/\\nu$ for one term to dominate.)\n\nIn terms of model parameters:\n\n- More frequent samples (smaller $\\delta t$, larger $\\nu$) pushes us toward the Poisson-dominant regime.\n- More variable latent abundances (smaller $\\phi$, smaller $\\nu$) pushes us toward the latent-dominant regime. \n- A higher threshold of detection (larger $\\mu$) pushes us toward the latent-dominant regime. \n\n## The Cornish-Fisher expansion of the quantiles of $Y$\n\nUltimately, our goal is to estimate the probability that $Y > \\hat{K}$, the detection threshold.\nThus, we need to estimate the CDF of $Y$ from its cumulants $\\kappa_\\alpha$.\nFor that we will use the Cornish-Fisher expansion.\nThe idea behind the expansion is to start by approximating the CDF as that of a Gaussian random variable with the correct mean and variance, and then iteratively adjust it for higher-order cumulants (skew, kurtosis, etc).\nIt is defined as follows:\n\nThe quantile function $y(p)$ (i.e. the value for which $\\text{CDF}(y) = p$) is approximated by $y(p) \\approx \\kappa_1 + {\\kappa_2}^{1/2} w_p$, where\n(TODO: fix formatting here)\n$$\n\\begin{align}\nw_p & = x \\\\\n    & + \\gamma_1 h_1(x) \\\\\n    & + \\gamma_2 h_2(x) + {\\gamma_1}^2 h_{11}(x) \\\\\n    & + \\cdots \\\\\nx   & = \\Phi^{-1}(p) \\\\\n\\gamma_{\\alpha-2} & = \\frac{\\kappa_\\alpha}{{\\kappa_2}^{\\alpha/2}} \\\\\nh_1(x) & = \\frac{\\text{He}_2(x)}{6} \\\\\nh_2(x) & = \\frac{\\text{He}_3(x)}{24} \\\\\nh_{11}(x) & = - \\frac{2\\text{He}_3(x) + \\text{He}_1(x)}{36} \\\\\n\\end{align}\n$$\nWhere $\\Phi$ is the CGF of the standard normal distribution and $\\text{He}$ are the probabilists' Hermite polynomials. \nNote that each line of the sum must be included as a whole for the approximation to be valid at that level.\n\nFor fixed $x$ (and therefore fixed quantile), the relative sizes of the terms of the expansion are controlled by the coefficients $\\gamma$.\nIn the Poisson-dominated regime:\n$$\n\\begin{align}\n\\gamma_1 & = \\mu^{-1/2} \\\\\n\\gamma_2 & = \\gamma_1^2 = \\mu^{-1} \\\\\n\\end{align}\n$$\nso truncating the expansion at a few terms should work well when $\\mu > 1$.\nSince we're interested in having a significant probability of detection (e.g. >90%), and our threshold of detection is at least one read, this is not a very limiting requirement.\n\nIn the latent-noise-dominated regime:\n$$\n\\begin{align}\n\\gamma_1 & = \\frac{2^{5/2}}{3} \\nu^{-1/2} \\\\\n\\gamma_2 & = 6 \\nu^{-1}\n\\end{align}\n$$\nso truncating the series at a few terms should work well when $\\nu \\gg 1$.\nThis is also not very limiting, because $\\nu$ can be large either by making $\\phi$ large (which is likely unless our data source is extremely noisy) or by sampling frequently so that $r \\delta t$ is small.\n\nNotice that in the latent regime, the only $\\nu$ (which controls the latent coefficient of variation) matters for the validity of the expansion, not $\\mu$.\nHowever, for this regime to hold $\\mu \\gg \\nu > 1$.\nIf we put our requirements for the two regimes together, we see that the approximation is good if both $\\mu > 1$ and $\\nu \\gg 1$.\n[TODO: write this more clearly and illustrate with figure.]\n\n## Numerical illustrations (to-do)\n\n## Implications for cost\n\nSketch of implications:\n\nIn the Poisson-dominated regime, a Gaussian approximation is pretty good, so\n$$\ny(p) \\approx \\mu + \\mu^{1/2} \\Phi^{-1}(p)\n$$\nSet $p$ to be one minus the target probability of detection, set $y(p) = \\hat{K}$, the detection threshold.\nSolve for $\\mu$.\nThen compare this $\\mu$ to the deterministic model.\nWill predict that there's a delay in detection due to having to wait for the mean to be larger than the threshold to have a high probability of detection.\n\nIn the Latent-dominated regime, the terms of the Cornish-Fisher expansion just depend on $\\nu$, not on $\\mu$, so:\n$$\n\\begin{align}\ny(p) & \\approx \\mu + \\frac{\\mu}{{(2\\nu)}^{1/2}} w_p(\\nu) \\\\\n     & = \\mu \\left(1 +\\frac{1}{{(2\\nu)}^{1/2}} w_p(\\nu)\\right) \\\\\n\\end{align}\n$$\nCan caluclate $w_p(\\nu)$ to as high order as we need, then solve for $\\mu$ as in the Poisson regime \nBecause $w_p$ will be negative (since $p$ is small), this will inflate the required $\\mu$ for by a factor.\n\n## Playground (ignore)\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import poisson, norm, nbinom\nimport matplotlib.pyplot as plt\n\nk = np.arange(40)\nlamb = 4\n\nplt.plot(k, poisson.cdf(k, lamb))\nplt.plot(k, norm.cdf(k, loc=lamb, scale=np.sqrt(lamb)))\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-2-output-1.png){width=571 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\np = np.arange(0.01, 0.20, 0.001)\nx = norm.ppf(p)\nfor lamb in [1, 2, 4, 10, 20]:\n    plt.plot(p, poisson.ppf(p, lamb), label=\"Poisson\")\n    plt.plot(p, lamb + np.sqrt(lamb) * x, label=\"Normal\")\n    plt.plot(p, lamb + np.sqrt(lamb) * x + (x**2 - 1) / 6, label=\"Cornish\")\n    plt.legend()\n    plt.title(f\"$\\lambda = {lamb}$\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-3-output-1.png){width=582 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-3-output-2.png){width=582 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-3-output-3.png){width=582 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-3-output-4.png){width=558 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-3-output-5.png){width=566 height=431}\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\np = np.arange(0.01, 0.50, 0.001)\nx = norm.ppf(p)\nfor lamb in [1, 2, 4, 10, 20]:\n    plt.plot(p, poisson.ppf(p, lamb), label=\"Poisson\")\n    plt.plot(p, lamb + np.sqrt(lamb) * x, label=\"Normal\")\n    plt.plot(p, lamb + np.sqrt(lamb) * x + (x**2 - 1) / 6, label=\"Cornish\")\n    plt.legend()\n    plt.title(f\"$\\lambda = {lamb}$\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-4-output-1.png){width=582 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-4-output-2.png){width=582 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-4-output-3.png){width=558 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-4-output-4.png){width=566 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-4-output-5.png){width=566 height=431}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nlamb = np.arange(1, 20, 1)\nfor p in [0.05, 0.1, 0.1587, 0.2]:\n    x = norm.ppf(p)\n    plt.plot(lamb, poisson.ppf(p, lamb), label=\"Poisson\")\n    plt.plot(lamb, lamb + np.sqrt(lamb) * x, label=\"Normal\")\n    plt.plot(lamb, lamb + np.sqrt(lamb) * x + (x**2 - 1) / 6, label=\"Cornish\")\n    plt.legend()\n    plt.xlabel(\"$\\lambda$\")\n    plt.ylabel(f\"$p = {p}$ quartile\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-5-output-1.png){width=587 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-5-output-2.png){width=587 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-5-output-3.png){width=587 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-5-output-4.png){width=587 height=432}\n:::\n:::\n\n\nLooks like the Cornish-Fisher adjustment doesn't really matter for $p > 0.05$.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nr = np.log(2) / 7\nphi = 2\ndelta_t = 3\np = 0.1\nx = norm.ppf(p)\nm = np.arange(0, 100, 1)\nprint(r * delta_t / (2 * phi))\nplt.plot(m, m)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.07426576934570842\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-6-output-2.png){width=575 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nr = np.log(2) / 7\nphi = 2\ndelta_t = 6\np = 0.1\nx = norm.ppf(p)\nm = np.arange(0, 100, 1)\nprint(r * delta_t / (2 * phi))\nplt.plot(m, m)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.14853153869141683\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-7-output-2.png){width=575 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nr = np.log(2) / 7\nphi = 50\ndelta_t = 3\np = 0.1\nx = norm.ppf(p)\nm = np.arange(0, 100, 1)\nprint(r * delta_t / (2 * phi))\nplt.plot(m, m)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0029706307738283366\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-8-output-2.png){width=575 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nr = np.log(2) / 7\nphi = 2\ndelta_t = 3\np = 0.1\nx = norm.ppf(p)\nm = np.arange(0, 100, 1)\nprint(x)\nprint(np.sqrt(r * delta_t / (2 * phi)))\nprint(r * delta_t / (2 * phi))\nprint((x**2 - 1) / 6)\nplt.plot(m, m)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x + m * r * delta_t / (3 * phi) * (x**2 - 1) / 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-1.2815515655446004\n0.2725174661296197\n0.07426576934570842\n0.10706240252496935\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-9-output-2.png){width=575 height=411}\n:::\n:::\n\n\n### Gamma-Poisson mixture\n\nQuestion: When is the Gaussian approximation to the PPF good enough and when does third-order Cornish-Fisher help?\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef cornish_fisher_ppf(p, mean, variance, skew=0.0):\n    x = norm.ppf(p)\n    return mean + np.sqrt(variance) * (x + skew * (x**2 - 1) / 6)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\np = np.arange(0.001, 0.20, 0.001)\nfor lamb in [1.0, 2.0, 4.0, 8.0]:\n    dist = poisson(lamb)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.legend()\n    plt.title(f\"$\\lambda = {lamb}$\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-11-output-1.png){width=582 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-11-output-2.png){width=582 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-11-output-3.png){width=569 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-11-output-4.png){width=569 height=431}\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef nbinom2(mean, phi):\n    n = phi\n    p = phi / (mean + phi)\n    return nbinom(n, p)\n\nphi = 4\nfor mean in [0.5, 1.0, 2.0, 4.0]:\n    dist = nbinom2(mean, phi)\n    m, v = (mean, mean + mean**2 / phi)\n    print(np.allclose(dist.stats(\"mv\"), (m, v)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\nTrue\nTrue\nTrue\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\np = np.arange(0.001, 0.20, 0.001)\nmus = np.logspace(0, 8, 9, base=2)\n\nphi = 100\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.ylim([0, mu])\n    plt.legend()\n    plt.title(f\"$\\mu = {mu}$\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-1.png){width=571 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-2.png){width=579 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-3.png){width=571 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-4.png){width=558 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-5.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-6.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-7.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-8.png){width=575 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-13-output-9.png){width=575 height=432}\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\np = np.arange(0.001, 0.20, 0.001)\nphi = 4\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.ylim([0, mu])\n    plt.legend()\n    plt.title(f\"$\\mu = {mu}$\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-1.png){width=571 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-2.png){width=579 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-3.png){width=571 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-4.png){width=558 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-5.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-6.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-7.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-8.png){width=575 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-14-output-9.png){width=575 height=432}\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\np = np.arange(0.001, 0.20, 0.001)\nphi = 2\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.ylim([0, mu])\n    plt.legend()\n    plt.title(f\"$\\mu = {mu}$\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-1.png){width=571 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-2.png){width=579 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-3.png){width=571 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-4.png){width=558 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-5.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-6.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-7.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-8.png){width=575 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-15-output-9.png){width=575 height=432}\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\np = np.arange(0.001, 0.999, 0.001)\n\nphi = 1\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.ylim([0, 3*mu])\n    plt.legend()\n    plt.title(f\"$\\mu = {mu}$\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-1.png){width=571 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-2.png){width=558 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-3.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-4.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-5.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-6.png){width=566 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-7.png){width=575 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-8.png){width=575 height=432}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-16-output-9.png){width=575 height=432}\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\np = 0.1\nmus = np.logspace(0, 8, 9, base=2)\nphi = 100\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(mu, dist.ppf(p), '.', color=\"C0\", label=\"Poisson\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v), '.', color=\"C1\", label=\"Normal\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v, s), '.', color=\"C2\", label=\"Cornish\")\nplt.ylim([0.1, mus[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-17-output-1.png){width=580 height=412}\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\np = 0.1\nmus = np.logspace(0, 8, 9, base=2)\nphi = 4\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(mu, dist.ppf(p), '.', color=\"C0\", label=\"Poisson\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v), '.', color=\"C1\", label=\"Normal\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v, s), '.', color=\"C2\", label=\"Cornish\")\nplt.ylim([0.1, mus[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-18-output-1.png){width=580 height=412}\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\np = 0.1\nmus = np.logspace(0, 8, 9, base=2)\nphi = 2\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(mu, dist.ppf(p), '.', color=\"C0\", label=\"Poisson\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v), '.', color=\"C1\", label=\"Normal\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v, s), '.', color=\"C2\", label=\"Cornish\")\nplt.ylim([0.1, mus[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-19-output-1.png){width=580 height=412}\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\np = 0.1\nmus = np.logspace(0, 8, 9, base=2)\nphi = 1\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(mu, dist.ppf(p), '.', color=\"C0\", label=\"Poisson\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v), '.', color=\"C1\", label=\"Normal\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v, s), '.', color=\"C2\", label=\"Cornish\")\nplt.ylim([0.1, mus[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-20-output-1.png){width=580 height=412}\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nx = norm.ppf(0.1)\nprint(6 * x / (x**2 - 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-11.970136437445571\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\np = 0.1\n# mus = np.logspace(0, 8, 9, base=2)\nmu = 40\nphis = np.logspace(0, 7, 8, base=2)\nfor phi in phis:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(phi, dist.ppf(p), '.', color=\"C0\", label=\"Poisson\")\n    plt.plot(phi, cornish_fisher_ppf(p, m, v), '.', color=\"C1\", label=\"Normal\")\n    plt.plot(phi, cornish_fisher_ppf(p, m, v, s), '.', color=\"C2\", label=\"Cornish\")\n# plt.ylim([0.1, phis[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-22-output-1.png){width=572 height=412}\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\np = 0.1\n# mus = np.logspace(0, 8, 9, base=2)\nmu = 10\nphis = np.logspace(0, 7, 8, base=2)\nfor phi in phis:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(phi, dist.ppf(p), '.', color=\"C0\", label=\"Poisson\")\n    plt.plot(phi, cornish_fisher_ppf(p, m, v), '.', color=\"C1\", label=\"Normal\")\n    plt.plot(phi, cornish_fisher_ppf(p, m, v, s), '.', color=\"C2\", label=\"Cornish\")\n# plt.ylim([0.1, phis[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-23-output-1.png){width=580 height=412}\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nfrom numpy.polynomial.hermite_e import HermiteE\n\ndef cf_approx(shape, order: int = 3):\n    poly = HermiteE((0, 1))\n    if order > 2:\n        poly += 1.9 * shape**(-1/2) * HermiteE((0, 0, 1/6))\n    if order > 3:\n        poly += 6 * shape**(-1) * HermiteE((0, 0, 1/24))\n        poly += 1.9**2 * shape**(-1) * HermiteE((0, -1/36, 0, -2/36))\n    if order > 4:\n        raise ValueError\n    return poly\n\np = np.arange(0.01, 1.0, 0.01)\n# x = np.arange(-2, 2, 0.01)\nx = norm.ppf(p)\n\nshape = 8\nplt.plot(p, cf_approx(shape, order = 2)(x))\nplt.plot(p, cf_approx(shape, order = 3)(x))\nplt.plot(p, cf_approx(shape, order = 4)(x))\nplt.title(shape)\nplt.show()\n\nshape = 4\nplt.plot(p, cf_approx(shape, order = 2)(x))\nplt.plot(p, cf_approx(shape, order = 3)(x))\nplt.plot(p, cf_approx(shape, order = 4)(x))\nplt.title(shape)\nplt.show()\n\nshape = 1\nplt.plot(p, cf_approx(shape, order = 2)(x))\nplt.plot(p, cf_approx(shape, order = 3)(x))\nplt.plot(p, cf_approx(shape, order = 4)(x))\nplt.title(shape)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-24-output-1.png){width=569 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-24-output-2.png){width=569 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](2024-02-22_StochasticMVP_files/figure-html/cell-24-output-3.png){width=569 height=431}\n:::\n:::\n\n\n",
    "supporting": [
      "2024-02-22_StochasticMVP_files"
    ],
    "filters": [],
    "includes": {}
  }
}