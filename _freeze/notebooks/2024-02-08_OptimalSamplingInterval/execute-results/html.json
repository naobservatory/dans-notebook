{
  "hash": "c4853e816aedaeb85d65f693c88c2e9a",
  "result": {
    "markdown": "---\ntitle: NAO Cost Estimate MVP -- Optimizing the sampling interval\nauthor: Dan Rice\ndate: '2024-02-13'\nformat:\n  html:\n    code-fold: false\n    toc: true\nfilters:\n  - black-formatter\n---\n\n## Background\n\nSee [previous notebook](https://data.securebio.org/dans-notebook/notebooks/2024-02-02_CostEstimateMVP.html).\nThe goal of this notebook is to use our simple, deterministic cost estimate to answer the question:\n\n> How often should we process and sequence samples?\n\nWe want to understand the tradeoff between:\n\n1. catching the virus earlier by sampling more frequently, and\n1. saving on processing costs by sampling less frequently.\n\nTo this end, we posit a two-component cost model:\n\n* Per-read sequencing costs, and\n* Per-sample processing costs\n\nand find the optimal sampling interval $\\delta t$ that minimizes total costs,\nwhile sequencing to sufficient depth per sample $n$ to detect a virus by cumulative incidence $\\hat{c}$.\n\n## A two-component cost model\n\nConsider the cost averaged over a long time interval $T$ in which we will take many samples.\nIf we collect and process samples every $\\delta t$ days, we will take $T / \\delta t$ samples in this interval.\nIf we sample $n$ reads per sample, our total sequencing depth is $n \\frac{\\delta t}{T}$ reads.\nAssume that our costs can be divided into a per-sample cost $d_s$ (including costs of collection, transportation, and processing for sequencing) and a per-read cost $d_r$ of sequencing.\n(Note: the $d$ is sort of awkward because we've already used $c$ for \"cumulative incidence\".\nYou can think of it as standing for \"dollars\".)\n\nWe will seek to minimize the total cost of detection:\n$$\nd_{\\text{tot}} = d_s \\frac{T}{\\delta t} + d_r \\frac{nT}{\\delta t}.\n$$\nEquivalently, we can divide by the arbitrary time-interval $T$ to get the total rate of spending:\n$$\n\\frac{d_{\\text{tot}}}{T} = \\frac{d_s}{\\delta t} + d_r \\frac{n}{\\delta t}.\n$$\n\nIn our [previous post](https://data.securebio.org/dans-notebook/notebooks/2024-02-02_CostEstimateMVP.html),\nwe found that the read depth per time required to detect a virus by the time it reaches cumulative incidence $\\hat{c}$ is:\n\n$$\n\\begin{align}\n\\frac{n}{\\delta t} & = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}}\\right) e^{r t_d} f(r \\delta t) \\\\\n                   & = A f(r \\delta t),\n\\end{align}\n$$\n\nwhere the function $f$ depends on the sampling scheme,\nand we've defined the constant $A$ to contain all the terms that do not depend on $\\delta t$.\nSubstituting this into the rate of spending, we have:\n$$\n\\frac{d_{\\text{tot}}}{T} = \\frac{d_s}{\\delta t} + A d_r f(r \\delta t).\n$$\n\nIn the next section, we will find the value of $\\delta t$ that minimizes the rate of spending.\n\n### Limitations of the two-component model\n\n* We assume that we process each sample as it comes in. In practice, we could stockpile a set of $m$ samples and process them simultaneously.\n  This would require splitting out the cost of sampling from the cost of sample prep.\n* We do not consider the fact that sequencing (and presumably to some extent sample prep) unit costs decrease with greater depth.\n  (I.e., it's cheaper per-read to do bigger runs.)\n* We neglect the \"batch\" effects of sequencing. Typically you buy sequencing in units of \"lanes\" rather than asking for an arbitrary number of reads. This will introduce threshold effects, where we want to batach our samples to use lanes efficiently.\n* We do not account for fixed costs that accumulate per unit time regardless of our sampling and sequencing protocols.\n  These do not affect the optimization here, but they do add to the total cost of the system.\n\n## Optimizing the sampling interval\n\nTo find the optimal $\\delta t$, we look for a zero of the derivative of spending rate:\n\n$$\n\\begin{align}\n\\frac{d}{d \\delta t} \\frac{d_{\\text{tot}}}{T} & = - \\frac{d_s}{{\\delta t}^2} + A d_r r f'(r\\delta t).\n\\end{align}\n$$\n\nSetting the right-hand side equal to zero and rearranging gives:\n\n$$\nr {\\delta t}^2 f'(r \\delta t) = \\frac{d_s}{d_r} A^{-1}\n$$\n\nTo get any farther, we need to specify $f$ and therefore a sampling scheme.\n[Note: If we give some general properties of $f$, we can say some things here that are general to the sampling scheme]\n\n### Grab sampling\n\nWe first consider grab sampling, where the entire sample is collected at the sampling time.\nIn that case, we have:\n$$\n\\begin{align}\nf(x) & = \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & = 1 + \\frac{x^2}{12} + \\mathcal{O}(x^3).\n\\end{align}\n$$\nWe are particularly interested in the small-$x$ regime:\nThe depth required becomes exponentially large when $r \\delta t \\gg 1$,\nso it is likely that the optimal interval satisfies $r \\delta t \\lesssim 1$.\nWe can check this for self-consistency in any specific numerical examples.\n\nThis gives us the derivative:\n$$\nf'(x) \\approx \\frac{x}{6}.\n$$\n\nUsing this in our optimization equation yields:\n$$\n{(r \\delta t)}^3 \\approx 6 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}.\n$$\n\n",
    "supporting": [
      "2024-02-08_OptimalSamplingInterval_files"
    ],
    "filters": [],
    "includes": {}
  }
}