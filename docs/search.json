[
  {
    "objectID": "notebooks/2023-10-05-qubit.html",
    "href": "notebooks/2023-10-05-qubit.html",
    "title": "2023-10-05 Qubit data exploration",
    "section": "",
    "text": "Trying to understand the Qubit output."
  },
  {
    "objectID": "notebooks/2023-10-05-qubit.html#objectives",
    "href": "notebooks/2023-10-05-qubit.html#objectives",
    "title": "2023-10-05 Qubit data exploration",
    "section": "",
    "text": "Trying to understand the Qubit output."
  },
  {
    "objectID": "notebooks/2023-10-05-qubit.html#preliminary-work",
    "href": "notebooks/2023-10-05-qubit.html#preliminary-work",
    "title": "2023-10-05 Qubit data exploration",
    "section": "Preliminary work",
    "text": "Preliminary work\nAri exported the data from a recent (somewhat arbitrarily selected) run to a flash drive and put it here.\nOlivia looked up the concentration of the standards and said:\n\nThe concentration of Std 1 is 0 ng/uL (TE buffer). Std 2 is 10ng/uL of rRNA (also in TE buffer).\n\nThe Assay Kit User guide is here"
  },
  {
    "objectID": "notebooks/2023-10-05-qubit.html#data-import",
    "href": "notebooks/2023-10-05-qubit.html#data-import",
    "title": "2023-10-05 Qubit data exploration",
    "section": "Data import",
    "text": "Data import\n\nlibrary(here)\n\nhere() starts at /Users/dan/notebook\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata_dir &lt;- here(\"_data\", \"qubit\")\ndata_raw &lt;- read_csv(here(data_dir, \"QubitData_10-05-2023_08-32-08.csv\"))\n\nRows: 14 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Run ID, Assay Name, Test Name, Test Date, Qubit tube conc. units, O...\ndbl (7): Qubit tube conc., Original sample conc., Sample Volume (uL), Diluti...\nlgl (2): Std 3 RFU, Green RFU\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data_raw)\n\nRows: 14\nColumns: 17\n$ `Run ID`                      &lt;chr&gt; \"2023-09-11_165152\", \"2023-09-11_165152\"…\n$ `Assay Name`                  &lt;chr&gt; \"RNA High Sensitivity\", \"RNA High Sensit…\n$ `Test Name`                   &lt;chr&gt; \"Sample_#230911-165447\", \"Sample_#230911…\n$ `Test Date`                   &lt;chr&gt; \"09/11/2023 04:54:47 PM\", \"09/11/2023 04…\n$ `Qubit tube conc.`            &lt;dbl&gt; 123.0, 422.0, 415.0, 294.0, 287.0, 328.0…\n$ `Qubit tube conc. units`      &lt;chr&gt; \"ng/mL\", \"ng/mL\", \"ng/mL\", \"ng/mL\", \"ng/…\n$ `Original sample conc.`       &lt;dbl&gt; 4.92, 16.90, 16.60, 11.80, 11.50, 13.10,…\n$ `Original sample conc. units` &lt;chr&gt; \"ng/uL\", \"ng/uL\", \"ng/uL\", \"ng/uL\", \"ng/…\n$ `Sample Volume (uL)`          &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5\n$ `Dilution Factor`             &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, …\n$ `Std 1 RFU`                   &lt;dbl&gt; 82.37, 82.37, 82.37, 82.37, 82.37, 82.37…\n$ `Std 2 RFU`                   &lt;dbl&gt; 1514.1, 1514.1, 1514.1, 1514.1, 1514.1, …\n$ `Std 3 RFU`                   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Excitation                    &lt;chr&gt; \"Red\", \"Red\", \"Red\", \"Red\", \"Red\", \"Red\"…\n$ Emission                      &lt;chr&gt; \"Far Red\", \"Far Red\", \"Far Red\", \"Far Re…\n$ `Green RFU`                   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Far Red RFU`                 &lt;dbl&gt; 475.18, 1329.59, 1313.26, 993.90, 975.18…\n\n\n\ndata &lt;- data_raw |&gt;\n  mutate(delta_rfu = `Far Red RFU` - `Std 1 RFU`)"
  },
  {
    "objectID": "notebooks/2023-10-05-qubit.html#concentration-curves",
    "href": "notebooks/2023-10-05-qubit.html#concentration-curves",
    "title": "2023-10-05 Qubit data exploration",
    "section": "Concentration curves",
    "text": "Concentration curves\nMy first guess is that the relationship between concentration and RFU should be linear.\n\ndata |&gt;\n  ggplot(mapping = aes(y = `Far Red RFU`, x = `Qubit tube conc.`)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIt is not.\n\ndata |&gt;\n  ggplot(mapping = aes(y = delta_rfu, x = `Qubit tube conc.`)) +\n  geom_point() +\n  # scale_y_continuous(trans=\"log10\") +\n  # scale_x_continuous(trans=\"log10\") +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "notebooks/2023-10-11-qpcr_standard_curvs.html",
    "href": "notebooks/2023-10-11-qpcr_standard_curvs.html",
    "title": "2023-10-11 Analyze qPCR standard curves",
    "section": "",
    "text": "Test out the standard curves from qPCR standard curve troubleshooting."
  },
  {
    "objectID": "notebooks/2023-10-11-qpcr_standard_curvs.html#objectives",
    "href": "notebooks/2023-10-11-qpcr_standard_curvs.html#objectives",
    "title": "2023-10-11 Analyze qPCR standard curves",
    "section": "",
    "text": "Test out the standard curves from qPCR standard curve troubleshooting."
  },
  {
    "objectID": "notebooks/2023-10-11-qpcr_standard_curvs.html#preliminary-work",
    "href": "notebooks/2023-10-11-qpcr_standard_curvs.html#preliminary-work",
    "title": "2023-10-11 Analyze qPCR standard curves",
    "section": "Preliminary work",
    "text": "Preliminary work\nExported csv files from Olivia’s eds file uploads."
  },
  {
    "objectID": "notebooks/2023-10-11-qpcr_standard_curvs.html#data-import",
    "href": "notebooks/2023-10-11-qpcr_standard_curvs.html#data-import",
    "title": "2023-10-11 Analyze qPCR standard curves",
    "section": "Data import",
    "text": "Data import\n\nlibrary(here)\n\nhere() starts at /Users/dan/notebook\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\n\n\ndata_dir &lt;-\n  here(\"~\", \"airport\", \"[2023-10-05] qPCR standard curve troubleshooting\")\nfilename_pattern &lt;- \"_Results_\"\n\n\ncol_types &lt;- list(\n  Target = col_character(),\n  Cq = col_double()\n)\nraw_data &lt;- list.files(\n  here(data_dir, \"qpcr\"),\n  pattern = filename_pattern,\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind()\n\n[1] \"/Users/dan/airport/[2023-10-05] qPCR standard curve troubleshooting/qpcr/2023-10-05_StdCurve_CalTest_Results_20231012_125325.csv\"\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\nprint(raw_data)\n\n# A tibble: 69 × 21\n    Well `Well Position` Omit  Sample Target Task     Reporter Quencher\n   &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1     1 A1              FALSE 100000 Cov2   STANDARD FAM      NFQ-MGB \n 2     2 A2              FALSE 100000 Cov2   STANDARD FAM      NFQ-MGB \n 3     3 A3              FALSE 100000 Cov2   STANDARD FAM      NFQ-MGB \n 4     5 A5              FALSE 100000 Noro   STANDARD FAM      NFQ-MGB \n 5     6 A6              FALSE 100000 Noro   STANDARD FAM      NFQ-MGB \n 6     7 A7              FALSE 100000 Noro   STANDARD FAM      NFQ-MGB \n 7     9 A9              FALSE 800000 PMMoV  STANDARD VIC      NFQ-MGB \n 8    10 A10             FALSE 800000 PMMoV  STANDARD VIC      NFQ-MGB \n 9    11 A11             FALSE 800000 PMMoV  STANDARD VIC      NFQ-MGB \n10    13 B1              FALSE  10000 Cov2   STANDARD FAM      NFQ-MGB \n# ℹ 59 more rows\n# ℹ 13 more variables: `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;\n\n\n\nraw_data |&gt; count(Target)\n\n# A tibble: 3 × 2\n  Target     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Cov2      24\n2 Noro      21\n3 PMMoV     24\n\n\n\ntidy_data &lt;- raw_data |&gt;\n  mutate(\n    # group = str_extract(Sample, \"^[0-9]\"),\n    # replicate = str_extract(Sample, \"[A-Z]$\"),\n    quantity = as.double(Sample),\n  ) |&gt;\n  glimpse()\n\nRows: 69\nColumns: 22\n$ Well                    &lt;dbl&gt; 1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 1…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A5\", \"A6\", \"A7\", \"A9\", \"A10…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Sample                  &lt;dbl&gt; 1e+05, 1e+05, 1e+05, 1e+05, 1e+05, 1e+05, 8e+0…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Noro\", \"Noro\", \"Noro\"…\n$ Task                    &lt;chr&gt; \"STANDARD\", \"STANDARD\", \"STANDARD\", \"STANDARD\"…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"VIC…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.336953, 1.342341, 1.346511, 1.543840, 1.5263…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 19.98854, 20.39666, 21.45209, 25.67237, 27.788…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9907778, 0.9907965, 0.9919563, 0.9951665, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 20.61243, 20.61243, 20.61243, 26.99896, 26.998…\n$ `Cq SD`                 &lt;dbl&gt; 0.75525700, 0.75525700, 0.75525700, 1.15574184…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.1399036, 0.1399036, 0.1399036, 0.2375444, 0.…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 15, 15, 16, 20, 21, 22, 16, 16, 16, 18, 18, 20…\n$ quantity                &lt;dbl&gt; 1e+05, 1e+05, 1e+05, 1e+05, 1e+05, 1e+05, 8e+0…\n\n\n\namp_data &lt;- list.files(\n  here(data_dir, \"qpcr\"),\n  pattern = \"Amplification Data\",\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind() |&gt;\n  left_join(tidy_data, by = join_by(Well, `Well Position`, Sample, Omit, Target)) |&gt;\n  glimpse()\n\n[1] \"/Users/dan/airport/[2023-10-05] qPCR standard curve troubleshooting/qpcr/2023-10-05_StdCurve_CalTest_Amplification Data_20231012_125325.csv\"\n\n\nWarning: The following named parsers don't match the column names: Cq\n\n\nRows: 2,760\nColumns: 25\n$ Well                    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\"…\n$ `Cycle Number`          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Rn                      &lt;dbl&gt; 0.6318336, 0.6287087, 0.6199436, 0.6148189, 0.…\n$ dRn                     &lt;dbl&gt; 0.0186775648, 0.0154212920, 0.0065248104, 0.00…\n$ Sample                  &lt;dbl&gt; 1e+05, 1e+05, 1e+05, 1e+05, 1e+05, 1e+05, 1e+0…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Task                    &lt;chr&gt; \"STANDARD\", \"STANDARD\", \"STANDARD\", \"STANDARD\"…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.336953, 1.336953, 1.336953, 1.336953, 1.3369…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 19.98854, 19.98854, 19.98854, 19.98854, 19.988…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9907778, 0.9907778, 0.9907778, 0.9907778, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 20.61243, 20.61243, 20.61243, 20.61243, 20.612…\n$ `Cq SD`                 &lt;dbl&gt; 0.755257, 0.755257, 0.755257, 0.755257, 0.7552…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.1399036, 0.1399036, 0.1399036, 0.1399036, 0.…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15…\n$ quantity                &lt;dbl&gt; 1e+05, 1e+05, 1e+05, 1e+05, 1e+05, 1e+05, 1e+0…"
  },
  {
    "objectID": "notebooks/2023-10-11-qpcr_standard_curvs.html#standard-curves",
    "href": "notebooks/2023-10-11-qpcr_standard_curvs.html#standard-curves",
    "title": "2023-10-11 Analyze qPCR standard curves",
    "section": "Standard curves",
    "text": "Standard curves\n\ntidy_data |&gt;\n  filter(Task == \"STANDARD\") |&gt;\n  ggplot(mapping = aes(\n    x = quantity,\n    y = Cq,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  scale_x_log10() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(facets = ~Target, scales = \"free\")\n\nWarning: Removed 6 rows containing non-finite values (`stat_summary()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 6 rows containing non-finite values (`stat_smooth()`).\n\n\n\n\n\n\nfits &lt;- tibble()\n# Note: no standard for norovirus\nfor (target in unique(tidy_data$Target)) {\n  fit &lt;- lm(Cq ~ log10(quantity),\n    data = filter(tidy_data, Task == \"STANDARD\", Target == target)\n  ) |&gt;\n    tidy() |&gt;\n    mutate(Target = target, efficiency = 10^-(1 / estimate) - 1)\n  fits &lt;- bind_rows(fits, fit)\n}\nprint(fits |&gt; filter(term == \"log10(quantity)\"))\n\n# A tibble: 3 × 7\n  term            estimate std.error statistic  p.value Target efficiency\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 log10(quantity)    -2.89    0.103      -28.0 2.69e-16 Cov2         1.22\n2 log10(quantity)    -2.42    0.180      -13.4 1.37e- 8 Noro         1.59\n3 log10(quantity)    -3.00    0.0535     -56.0 1.18e-21 PMMoV        1.15\n\n\nNotes:\n\nPMMoV looks fine. Efficiency is a bit high and lowest concentration point is a bit noisy, but basically good.\nNorovirus looks messy and efficiency is way too high.\nCov2 is intermediate"
  },
  {
    "objectID": "notebooks/2023-10-11-qpcr_standard_curvs.html#amplification-curves",
    "href": "notebooks/2023-10-11-qpcr_standard_curvs.html#amplification-curves",
    "title": "2023-10-11 Analyze qPCR standard curves",
    "section": "Amplification curves",
    "text": "Amplification curves\n\nplot_amp &lt;- function(data, color) {\n  ggplot(data, aes(x = `Cycle Number`, y = dRn)) +\n    geom_line(mapping = aes(\n      color = as.factor({{ color }}),\n      group = Well,\n    )) +\n    scale_y_log10(limits = c(1e-3, 1e1))\n}\n\nruler &lt;- function(y0_from, num_rules) {\n  y0 &lt;- 10^seq(from = y0_from, by = -1, length.out = num_rules)\n  rules &lt;- crossing(`Cycle Number` = amp_data$`Cycle Number`, y0 = y0) |&gt;\n    mutate(y = y0 * 2^`Cycle Number`)\n  geom_line(\n    data = rules,\n    mapping = aes(y = y, group = y0),\n    color = \"black\"\n  )\n}\n\nplot_amp_with_ruler &lt;- function(target, y0_from, num_rules) {\n  amp_data |&gt;\n    filter(!is.na(quantity), Target == target) |&gt;\n    plot_amp(quantity) +\n    ruler(y0_from, num_rules) +\n    geom_line(mapping = aes(\n      x = `Cycle Number`,\n      y = Threshold\n    ), color = \"Grey\") +\n    labs(title = target)\n}\n\n\nPMMoV\n\nplot_amp_with_ruler(\"PMMoV\", -7, 7)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 131 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 196 rows containing missing values (`geom_line()`).\n\n\n\n\n\nCurves look pretty clean (maybe a little steep) but the spacing is off on the dilutions. They look a bit under-diluted, which would explain the slightly too large efficiency.\n\n\nSARS-CoV-2\n\nplot_amp_with_ruler(\"Cov2\", -7, 7)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 50 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 196 rows containing missing values (`geom_line()`).\n\n\n\n\n\nCurves are not as consistent as PMMoV and spacing is still too narrow. Slopes look ok.\n\n\nNorovirus\n\nplot_amp_with_ruler(\"Noro\", -8.5, 6)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 100 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 173 rows containing missing values (`geom_line()`).\n\n\n\n\n\nLook really messy. Not spaced properly. Possibly have a slow phase and then a faster phase. Could this be because the PCR template doesn’t match the standard?"
  },
  {
    "objectID": "notebooks/2023-10-11-qpcr_standard_curvs.html#raw-rn",
    "href": "notebooks/2023-10-11-qpcr_standard_curvs.html#raw-rn",
    "title": "2023-10-11 Analyze qPCR standard curves",
    "section": "Raw Rn",
    "text": "Raw Rn\n\nplot_rn &lt;- function(data, target) {\n  data |&gt;\n    filter(Task == \"STANDARD\", Target == target) |&gt;\n    ggplot(aes(x = `Cycle Number`, y = Rn)) +\n    geom_line(mapping = aes(\n      color = as.factor(quantity),\n      group = Well,\n    )) +\n    scale_y_log10() +\n    labs(title = target)\n}\n\nDon’t really know what to make of these:\n\namp_data |&gt; plot_rn(\"PMMoV\")\n\n\n\namp_data |&gt; plot_rn(\"Cov2\")\n\n\n\namp_data |&gt; plot_rn(\"Noro\")\n\n\n\n\n\nNTC\n\namp_data |&gt;\n  filter(Task == \"NTC\") |&gt;\n  ggplot(aes(x = `Cycle Number`, y = Rn)) +\n  geom_line(mapping = aes(color = Target, group = Well))"
  },
  {
    "objectID": "notebooks/2023-09-28-qpcr_qc.html",
    "href": "notebooks/2023-09-28-qpcr_qc.html",
    "title": "2023-09-28 qPCR Quality Control",
    "section": "",
    "text": "Putting together a standard set of quality control checks and plots for qPCR. Includes standard curves and dilution series."
  },
  {
    "objectID": "notebooks/2023-09-28-qpcr_qc.html#objectives",
    "href": "notebooks/2023-09-28-qpcr_qc.html#objectives",
    "title": "2023-09-28 qPCR Quality Control",
    "section": "",
    "text": "Putting together a standard set of quality control checks and plots for qPCR. Includes standard curves and dilution series."
  },
  {
    "objectID": "notebooks/2023-09-28-qpcr_qc.html#preliminary-work",
    "href": "notebooks/2023-09-28-qpcr_qc.html#preliminary-work",
    "title": "2023-09-28 qPCR Quality Control",
    "section": "Preliminary work",
    "text": "Preliminary work\nThis is the same data as in this analysis."
  },
  {
    "objectID": "notebooks/2023-09-28-qpcr_qc.html#data-import",
    "href": "notebooks/2023-09-28-qpcr_qc.html#data-import",
    "title": "2023-09-28 qPCR Quality Control",
    "section": "Data import",
    "text": "Data import\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\n\n\nread_data &lt;- function(dir, pattern, col_types) {\n  list.files(\n    dir,\n    pattern = pattern,\n    full.names = TRUE,\n  ) |&gt;\n    map(function(f) {\n      read_csv(f, skip = 23, col_types = col_types, ) |&gt;\n        mutate(plate = str_extract(basename(f), \"(.*)_(.*)_[0-9]{8}_[0-9]{6}\\\\.csv\", group = 1))\n    }) |&gt;\n    list_rbind() |&gt;\n    separate_wider_regex(\n      `Well Position`,\n      c(well_row = \"[A-Z]+\", well_col = \"[0-9]+\"),\n      cols_remove = FALSE,\n    ) |&gt;\n    mutate(well_col = as.integer(well_col))\n}\n\nThere were some wells mixed up between the intended plate layout and the actual plate for Norovirus. See Results doc.\n\ncorrected_samples &lt;- tribble(\n  ~`Well Position`, ~Sample,\n  \"F7\",             \"empty\",\n  \"F8\",             \"empty\",\n  \"F9\",             \"empty\",\n  \"F10\",            \"empty\",\n  \"F11\",            \"empty\",\n  \"F12\",            \"empty\",\n  \"G7\",             \"6C/100\",\n  \"G8\",             \"6C/10\",\n  \"G9\",             \"7C/100\",\n  \"G10\",            \"7C/10\",\n  \"G11\",            \"3C/100\",\n  \"G12\",            \"3C/10\",\n  \"H7\",             \"1C/100\",\n  \"H8\",             \"1C/10\",\n  \"H9\",             \"1C/100\",\n  \"H10\",            \"1C/10\",\n  \"H11\",            \"NTC\",\n  \"H12\",            \"NTC\",\n) |&gt;\n  add_column(plate = \"2023-09-14_Noro_Extractions\")\ncorrect_samples &lt;- function(df, corrected_samples) {\n  left_join(df, corrected_samples, by = join_by(`Well Position`, plate)) |&gt;\n    mutate(Sample = ifelse(is.na(Sample.y), Sample.x, Sample.y), .keep = \"unused\")\n}\n\n\ndata_dir &lt;-\n  \"~/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/\"\n\n\nmetadata_file &lt;- paste0(\n  data_dir,\n  \"[2023-09-11] Extraction Experiment 2 templates and results\",\n  \" - sampleMetadata.csv\"\n)\nmetadata &lt;- read_csv(metadata_file) |&gt; glimpse()\n\nRows: 21 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Sample_ID, Extraction_kit, Short_kit, Elution_format, Brand, NA_Target\ndbl (2): Kit Batch, Elution_volume\nlgl (1): FP_sampleID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRows: 21\nColumns: 9\n$ Sample_ID      &lt;chr&gt; \"1A\", \"1B\", \"1C\", \"2A\", \"2B\", \"2C\", \"3A\", \"3B\", \"3C\", \"…\n$ Extraction_kit &lt;chr&gt; \"Zymo quick-RNA\", \"Zymo quick-RNA\", \"Zymo quick-RNA\", \"…\n$ Short_kit      &lt;chr&gt; \"1_ZR\", \"1_ZR\", \"1_ZR\", \"2_ZD\", \"2_ZD\", \"2_ZD\", \"3_IPL\"…\n$ `Kit Batch`    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3…\n$ Elution_volume &lt;dbl&gt; 30, 30, 30, 100, 100, 100, 100, 100, 100, 60, 60, 60, 1…\n$ Elution_format &lt;chr&gt; \"15*2\", \"15*2\", \"15*2\", \"50*2\", \"50*2\", \"50*2\", \"50*2\",…\n$ Brand          &lt;chr&gt; \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Invitr…\n$ NA_Target      &lt;chr&gt; \"RNA\", \"RNA\", \"RNA\", \"DNA+RNA\", \"DNA+RNA\", \"DNA+RNA\", \"…\n$ FP_sampleID    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\n\ncol_types &lt;- list(\n  Target = col_character(),\n  Cq = col_double()\n)\nres_data &lt;- read_data(\n  paste0(data_dir, \"qpcr\"),\n  pattern = \"Results\",\n  col_types = col_types\n) |&gt;\n  correct_samples(corrected_samples) |&gt;\n  separate_wider_regex(\n    Sample,\n    c(Sample_ID = \".+\", \"/\", dilution = \"[0-9]+$\"),\n    too_few = \"align_start\",\n  ) |&gt;\n  mutate(\n    replicate = str_extract(Sample_ID, \"[A-Z]$\"),\n    quantity = as.double(Sample_ID),\n    dilution = as.integer(dilution) |&gt; replace_na(1),\n  ) |&gt;\n  left_join(metadata, by = join_by(Sample_ID)) |&gt;\n  glimpse()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `quantity = as.double(Sample_ID)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nRows: 369\nColumns: 35\n$ Well                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ well_row                &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ well_col                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"U…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"NO_…\n$ `Amp Score`             &lt;dbl&gt; 1.1701024, 1.0212594, 1.1604009, 1.1057531, 1.…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 32.44077, 35.24757, 33.72678, 34.44077, 34.704…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9451320, 0.9773032, 0.9759027, 0.9634213, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 33.80504, 33.80504, 33.80504, 34.31332, 34.313…\n$ `Cq SD`                 &lt;dbl&gt; 1.4050310, 1.4050310, 1.4050310, 0.4683633, 0.…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 29, 32, 30, 31, 31, 31, 31, 31, 31, 21, 21, 21…\n$ plate                   &lt;chr&gt; \"2023-09-14_Cov2_extractions\", \"2023-09-14_Cov…\n$ Sample_ID               &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"3C\", \"3C\", \"3C\", \"6B\", \"6B\"…\n$ dilution                &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ replicate               &lt;chr&gt; \"A\", \"A\", \"A\", \"C\", \"C\", \"C\", \"B\", \"B\", \"B\", N…\n$ quantity                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, 1000, 1000…\n$ Extraction_kit          &lt;chr&gt; \"Zymo quick-RNA\", \"Zymo quick-RNA\", \"Zymo quic…\n$ Short_kit               &lt;chr&gt; \"1_ZR\", \"1_ZR\", \"1_ZR\", \"3_IPL\", \"3_IPL\", \"3_I…\n$ `Kit Batch`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 3, 3, 3, NA, NA, NA, 1, 1, 1…\n$ Elution_volume          &lt;dbl&gt; 30, 30, 30, 100, 100, 100, 80, 80, 80, NA, NA,…\n$ Elution_format          &lt;chr&gt; \"15*2\", \"15*2\", \"15*2\", \"50*2\", \"50*2\", \"50*2\"…\n$ Brand                   &lt;chr&gt; \"Zymo\", \"Zymo\", \"Zymo\", \"Invitrogen\", \"Invitro…\n$ NA_Target               &lt;chr&gt; \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA…\n$ FP_sampleID             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\namp_data &lt;- read_data(\n  paste0(data_dir, \"qpcr\"),\n  pattern = \"Amplification Data\",\n  col_types = col_types\n) |&gt;\n  correct_samples(corrected_samples) |&gt;\n  mutate(\n    replicate = str_extract(Sample, \"[A-Z]\"),\n    Sample_ID = str_split_i(Sample, \"/\", 1),\n    dilution = str_split_i(Sample, \"/\", 2) |&gt; as.integer() |&gt; replace_na(1),\n    quantity = as.double(Sample),\n  ) |&gt;\n  left_join(metadata, by = join_by(Sample_ID)) |&gt;\n  glimpse()\n\nWarning: The following named parsers don't match the column names: Cq\nThe following named parsers don't match the column names: Cq\nThe following named parsers don't match the column names: Cq\nThe following named parsers don't match the column names: Cq\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `quantity = as.double(Sample)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nRows: 14,760\nColumns: 23\n$ Well            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ well_row        &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ well_col        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ `Well Position` &lt;chr&gt; \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", …\n$ `Cycle Number`  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ Target          &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Rn              &lt;dbl&gt; 0.8770601, 0.8700446, 0.8562693, 0.8477768, 0.8438748,…\n$ dRn             &lt;dbl&gt; 3.107913e-02, 2.618482e-02, 1.453075e-02, 8.159439e-03…\n$ Omit            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ plate           &lt;chr&gt; \"2023-09-14_Cov2_extractions\", \"2023-09-14_Cov2_extrac…\n$ Sample          &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", …\n$ replicate       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ Sample_ID       &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", …\n$ dilution        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ quantity        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Extraction_kit  &lt;chr&gt; \"Zymo quick-RNA\", \"Zymo quick-RNA\", \"Zymo quick-RNA\", …\n$ Short_kit       &lt;chr&gt; \"1_ZR\", \"1_ZR\", \"1_ZR\", \"1_ZR\", \"1_ZR\", \"1_ZR\", \"1_ZR\"…\n$ `Kit Batch`     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Elution_volume  &lt;dbl&gt; 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30…\n$ Elution_format  &lt;chr&gt; \"15*2\", \"15*2\", \"15*2\", \"15*2\", \"15*2\", \"15*2\", \"15*2\"…\n$ Brand           &lt;chr&gt; \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\"…\n$ NA_Target       &lt;chr&gt; \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\"…\n$ FP_sampleID     &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…"
  },
  {
    "objectID": "notebooks/2023-09-28-qpcr_qc.html#amplification-curves",
    "href": "notebooks/2023-09-28-qpcr_qc.html#amplification-curves",
    "title": "2023-09-28 qPCR Quality Control",
    "section": "Amplification curves",
    "text": "Amplification curves\n\namp_data |&gt;\n  filter(!is.na(Extraction_kit)) |&gt;\n  ggplot(mapping = aes(\n    x = `Cycle Number`,\n    y = dRn,\n    color = as.factor(dilution),\n    group = Well\n  )) +\n  geom_line() +\n  facet_grid(\n    cols = vars(Extraction_kit), rows = vars(Target), scales = \"free_y\"\n  )\n\n\n\n\n\namp_data |&gt;\n  filter(!is.na(quantity)) |&gt;\n  ggplot(mapping = aes(\n    x = `Cycle Number`,\n    y = dRn,\n    color = as.factor(quantity),\n    group = interaction(plate, Well),\n  )) +\n  geom_line() +\n  facet_wrap(~Target, scales = \"free_y\")\n\n\n\n\n\nStandard ruler\nMaximum efficiency means doubling every cycle. The standard curve points here represent 10x dilutions. We can compare the amplification curves for the standard curve samples against a standard ruler: a set of idealized curves \\(y(c) = y_0 2^c\\) with \\(y_0\\) spaced \\(10\\times\\) apart.\n\nplot_amp &lt;- function(data, color) {\n  ggplot(data, aes(x = `Cycle Number`, y = dRn)) +\n    geom_line(mapping = aes(\n      color = as.factor({{ color }}),\n      group = interaction(plate, Well),\n    )) +\n    scale_y_log10(limits = c(1e-3, 1e1))\n}\n\nruler &lt;- function(y0_from, num_rules) {\n  y0 &lt;- 10^seq(from = y0_from, by = -1, length.out = num_rules)\n  rules &lt;- crossing(`Cycle Number` = amp_data$`Cycle Number`, y0 = y0) |&gt;\n    mutate(dRn = y0 * 2^`Cycle Number`)\n  geom_line(\n    data = rules,\n    mapping = aes(group = y0),\n    color = \"black\"\n  )\n}\n\nplot_amp_with_ruler &lt;- function(target, y0_from, num_rules) {\n  amp_data |&gt;\n    filter(!is.na(quantity), Target == target) |&gt;\n    plot_amp(quantity) +\n    ruler(y0_from, num_rules) +\n    labs(title = target)\n}\n\n\nplot_amp_with_ruler(\"16S\", -6, 5)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 11 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 133 rows containing missing values (`geom_line()`).\n\n\n\n\nplot_amp_with_ruler(\"CrA\", -5, 5)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 25 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 133 rows containing missing values (`geom_line()`).\n\n\n\n\nplot_amp_with_ruler(\"Noro\", -8, 5)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 53 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 136 rows containing missing values (`geom_line()`).\n\n\n\n\nplot_amp_with_ruler(\"Cov2\", -9, 5)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 93 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 142 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ndilution_kits &lt;- c(\n  \"Invitrogen PureLink RNA\",\n  \"QIAamp Viral RNA mini kit\",\n  \"Qiagen AllPrep PowerViral DNA/RNA\",\n  \"Zymo quick-RNA\"\n)\ntarget &lt;- \"Noro\"\namp_data |&gt;\n  filter(Target == target, is.element(Extraction_kit, dilution_kits)) |&gt;\n  plot_amp(dilution) +\n  ruler(-9, 3) +\n  facet_wrap(~Extraction_kit) +\n  labs(title = target)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 118 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 80 rows containing missing values (`geom_line()`).\n\n\n\n\ntarget &lt;- \"CrA\"\namp_data |&gt;\n  filter(Target == target, is.element(Extraction_kit, dilution_kits)) |&gt;\n  plot_amp(dilution) +\n  ruler(-7, 3) +\n  facet_wrap(~Extraction_kit) +\n  labs(title = target)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 76 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 80 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html",
    "href": "notebooks/2024-02-22_StochasticMVP.html",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "",
    "text": "Previously, we have worked out a deterministic model of the sequencing depth required to detect a novel pandemic virus by the time it reaches a fixed cumulative incidence (number of people ever infected). However, in the real world, there are a number of sources of noise that will affect our ability to detect a virus. In this post, we ask the question: How much sequencing is required to have an \\(x\\)% chance to detect a virus by the time it reaches a fixed cumulative incidence, given a particular model of the noise?"
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html#background",
    "href": "notebooks/2024-02-22_StochasticMVP.html#background",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "",
    "text": "Previously, we have worked out a deterministic model of the sequencing depth required to detect a novel pandemic virus by the time it reaches a fixed cumulative incidence (number of people ever infected). However, in the real world, there are a number of sources of noise that will affect our ability to detect a virus. In this post, we ask the question: How much sequencing is required to have an \\(x\\)% chance to detect a virus by the time it reaches a fixed cumulative incidence, given a particular model of the noise?"
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html#model",
    "href": "notebooks/2024-02-22_StochasticMVP.html#model",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Model",
    "text": "Model\nIt is useful to categorize noise sources by how they behave as various parameters like the sequencing depth change. For example, three types of noise that are relevant to our problem are:\n\nNoise whose coefficient of variation decreases as the sequencing depth increases. This includes poisson counting noise in the number of reads mapping to a sequence, due to finite sampling effects.\nNoise whose coefficient of variation goes to a constant value as the sequencing depth increases. For example, the relative abundances in a sequencing library depend on biases in enrichment efficiency. If there is a class of abundant sequences that are efficiently enriched by our lab protocol, random variation in the abundance of that class of sequences will generate noise in the counts of all the other sequences that is only weakly dependent on the total read depth.\nNoise that depends on the number of people contributing to a sample. For example, in the limit where each sample is taken from a single person, the noise in the counts of reads mapping to the pandemic virus will be dominated by whether that single person is infected or not.\n\nIn the following, we consider noise classes 1. and 2. We neglect 3 for the moment because in well-mixed municipal wastewater samples, we expect this to be a small effect. However, similar analysis to that presented here could be applied in that case as well.\nWe will consider a sequence of samples indexed by \\(i\\), where the random variable \\(Y_i\\) represents the number of reads corresponding to the pandemic virus in sample \\(i\\). We model \\(Y_i\\) as independent draws from a Poisson mixture distribution: \\[\nY_i \\sim \\text{Poisson}(X_i),\n\\] where \\(X_i\\) is a latent variable that represents excess noise not accounted for by the Poisson model. To connect this model to our previous deterministic model, we set the mean of \\(X_i\\) to the number of reads in the deterministic model:\n\\[\nE[X_i] = \\mu_i = \\frac{n b}{N} e^{r(t_0 + j \\delta t)}\n\\]\nwhere:\n\n\\(n\\) is the sequencing depth\n\\(b\\) is the P2RA factor\n\\(N\\) is the population size\n\\(r\\) is the growth rate of the virus\n\\(t_0\\) is the time of the first sample after the start of the pandemic\n\\(\\delta t\\) is the time between samples.\n\nNote that for simplicity this is assuming instantaneous grab sampling, which is a good approximation to 24-hr composite sampling.\nRecall that in our detection model, we declare a virus to be detected when the cumulative number of reads matching the virus cross a threshold. Thus, to calculate the probability of detection, we need to calculate the probability that the cumulative number of reads (\\(\\sum_{j=0}^{i} Y_j\\)) is greater than the threshold value \\(\\hat{K}\\). We will proceed in two steps:\n\nCalculate the cumulant generating function of the random variable \\(Y = \\sum_j Y_j\\). This is natural to calculate because the CGF of a sum of independent random variables is the sum of their individual CGFs.\nApproximate the cumulative distribution function (CDF) of \\(Y\\) from a finite set of cumulants using the Cornish-Fisher expansion. In this notebook, we will explore under what conditions we can truncate the Cornish-Fisher expansion at a certain number of terms."
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html#cumulant-generating-function-of-the-cumulative-read-count-y",
    "href": "notebooks/2024-02-22_StochasticMVP.html#cumulant-generating-function-of-the-cumulative-read-count-y",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Cumulant generating function of the cumulative read count, \\(Y\\)",
    "text": "Cumulant generating function of the cumulative read count, \\(Y\\)\nThe cumulant generating function \\(K_Y\\) of random variable \\(Y\\) is given by the log of its moment generating function:\n\\[\nK_Y(z) = \\log \\mathbb{E}[e^{zY}].\n\\]\nIf \\(Y_i\\) is Poisson distributed with random mean \\(X_i\\),\n\\[\n\\begin{align}\nK_{Y_i}(z) & = \\log \\mathbb{E}\\left[\\mathbb{E}[e^{zY_{i}} | X_i]\\right] \\\\\n       & = \\log \\mathbb{E}\\left[\\exp X_i (e^{z} - 1)\\right] \\\\\n       & = K_{X_i} \\left(e^{z} - 1\\right),\n\\end{align}\n\\] where the second line uses the moment-generating fuction of a Poisson random variable, and \\(K_{X_i}\\) is the CGF of \\(X_i\\).\nIf we assume that the \\(Y_i\\) are independent of one another, then we can add their CGFs to get the CGF of the cumulative read count: \\[\n\\begin{align}\nK_Y(z) & = K_{\\sum_i Y_i}(z) \\\\\n       & = \\sum_i K_{Y_i}(z) \\\\\n       & = \\sum_i K_{X_i}(e^z - 1) \\\\\n       & = K_{X}(e^z - 1),\n\\end{align}\n\\] where we define \\(X \\equiv \\sum_i X_i\\).\nThe last equation tells us how to combine the cumulants of \\(X\\) to get the cumulants of \\(Y\\). Let the cumulants of \\(Y\\) be denoted \\(\\kappa_1, \\kappa_2, \\ldots\\) and the cumulants of \\(X\\) by \\(\\chi_1, \\chi_2, \\ldots\\). Expanding the CGF gives: \\[\n\\begin{align}\nK_Y(z) & = K_X(e^z - 1) \\\\\n       & = \\chi_1 (e^z - 1) + \\chi_2 \\frac{{(e^z-1)}^2}{2} + \\chi_3 \\frac{{(e^z-1)}^3}{3!} + \\cdots \\\\\n       & = \\chi_1 \\sum_{j=1}^{\\infty} \\frac{z^j}{j!} + \\chi_2 \\frac{{\\left(\\sum_{j=1}^{\\infty} \\frac{z^j}{j!}\\right)}^2}{2} + \\chi_3 \\frac{{\\left(\\sum_{j=1}^{\\infty} \\frac{z^j}{j!}\\right)}^3}{3!} + \\cdots \\\\\n\\end{align}\n\\] Then, by equating powers of \\(z\\), we find \\[\n\\begin{align}\n\\kappa_1 & = \\chi_1 \\\\\n\\kappa_2 & = \\chi_1 + \\chi_2 \\\\\n\\kappa_3 & = \\chi_1 + 3 \\chi_2 + \\chi_3 \\\\\n\\kappa_4 & = \\chi_1 + 7 \\chi_2 + 6 \\chi_3 + \\chi_4 \\\\\n         & \\cdots\n\\end{align}\n\\] This cumulant series has the intuitive property that if \\(X \\to \\lambda\\) constant, \\(\\chi_\\alpha \\to \\lambda \\delta_{\\alpha, 1}\\) and \\(\\kappa_\\alpha \\to \\chi_1\\). That is, \\(Y \\to \\text{Poisson}(\\lambda)\\). For random \\(X\\), in constrast, all of the cumulants of \\(Y\\) are increased from their Poisson value of \\(\\chi_1\\) by the cumulants of \\(X\\). In particular, the variance of \\(Y\\), \\(\\kappa_2\\) is equal to the Poisson variance \\(\\chi_1\\) plus the variance of \\(X\\), \\(\\chi_2\\).\n\nCumulants of the latent variable \\(X\\)\nIt remains to find the cumulants of \\(X\\), \\(\\chi_\\alpha\\). For this, we need to specify a distribution for the latent variables at each sample, \\(X_i\\). For simplicity, we will choose the Gamma distribution, which allows us to vary the mean and variance independently. We will parameterize the distribution by its mean \\(\\mu\\), and inverse dispersion \\(\\phi\\). In standard shape-scale parameterization, we have: \\[\nX_i \\sim \\text{Gamma}(\\phi, \\mu_i / \\phi)\n\\] where we assume that the inverse dispersion is constant in time. Note that the coefficient of variation of \\(X_i\\) is \\(\\phi^{-1/2}\\), independent of \\(\\mu_i\\).\nThe gamma distribution has CGF: \\[\n\\begin{align}\nK_{X_i}(z) & = - \\phi \\log(1 - \\frac{\\mu_i}{\\phi} z) \\\\\n           & = \\phi \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha} {\\left(\\frac{\\mu_i}{\\phi}z\\right)}^{\\alpha}\n\\end{align}\n\\]\nBy the summation property of CGFs, we have the CGF of \\(X = \\sum_j X_j\\) at time \\(t_i\\): \\[\n\\begin{align}\nK_{X}(z) & = \\sum_{j=0}^i K_{X_j}(z) \\\\\n         & = \\phi \\sum_{j=0}^i \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha} {\\left(\\frac{\\mu_i}{\\phi}z\\right)}^{\\alpha} \\\\\n         & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} \\left( \\sum_{j=0}^{i} \\mu_j^\\alpha \\right) z^\\alpha.\n\\end{align}\n\\] Because the prevalence is growing exponentially in our model, \\(\\mu_j\\) is growing exponentially (see above). Letting \\(A \\equiv \\frac{nb}{N} e^{rt_0}\\), we have \\(\\mu_j = A e^{rj\\delta t}\\) and thus\n\\[\n\\begin{align}\nK_X(z) & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} \\left(\\sum_{j=0}^{i} A^\\alpha e^{\\alpha r j \\delta t} \\right) z^\\alpha \\\\\n       & = \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi^{1-\\alpha}}{\\alpha} A^\\alpha \\left(\\frac{e^{\\alpha r (i+1) \\delta t} - 1}{e^{\\alpha r \\delta t} - 1} \\right) z^\\alpha \\\\\n       & \\approx \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi}{\\alpha^2 r \\delta t} {\\left(A e^{r i \\delta t}\\right)}^\\alpha \\left(\\frac{\\alpha r \\delta t e^{\\alpha r \\delta t}}{e^{\\alpha r \\delta t} - 1} \\right) z^\\alpha \\\\\n       & \\approx \\sum_{\\alpha=1}^{\\infty} \\frac{\\phi}{\\alpha^2 r \\delta t} {\\left(A e^{r i \\delta t}\\right)}^\\alpha z^\\alpha\n\\end{align}\n\\] Where the first approximation requries the prevalence to be large compared to one, and the second approximation requires \\(\\alpha r \\delta t \\ll 1\\). (TODO: generalize the second approximation.)\nIt is illuminating to parameterize the distribution of \\(X\\) by its mean, \\(\\mu\\), and a shape parameter \\(\\nu\\): \\[\n\\begin{align}\n\\mu & \\equiv K_X'(0) \\\\\n    & = \\frac{A e^{r i \\delta t}}{r \\delta t}. \\\\\n\\nu & \\equiv \\frac{\\phi}{r \\delta t}.\n\\end{align}\n\\] Substituting these into the equation above gives \\[\nK_X(z) = \\nu \\sum_{\\alpha=1}^{\\infty} \\frac{1}{\\alpha^2} {\\left(\\frac{\\mu}{\\nu}\\right)}^\\alpha z^\\alpha.\n\\] (Note that this is similar to the CGF of the gamma distribution but with the logarithm replaced by a dilogarithm.)\nFinally, examination of the CGF yields the cumulants of \\(X\\): \\[\n\\chi_\\alpha = \\frac{(\\alpha - 1)!}{\\alpha} \\nu {\\left(\\frac{\\mu}{\\nu}\\right)}^{\\alpha}.\n\\]\nWe can say a few things about this result:\n\nBy construction, the mean of \\(X\\), \\(\\mu\\), is our previous deterministic prediction for the counts. (In the small \\(r \\delta t\\) limit).\nThe shape parameter \\(\\nu\\) controls the dispersion of \\(X\\). \\(\\text{Var}[X] = \\chi_2 = \\frac{\\mu^2}{2\\nu}\\). That is: larger \\(\\nu\\) means a smaller coefficient of variation.\n\\(\\nu\\) is controled by the latent inverse dispersion \\(\\phi\\) and the scaled sampling interval \\(r \\delta t\\). Smaller sampling inverval means we take more independent samples per unit time, which reduces the variance of the sum.\n\\(\\frac{\\mu}{\\nu}\\) is a pure scale parameter of the distribution of \\(X\\).\n\n\n\nCumulants of the cumulative counts \\(Y\\)\nSubstituting our equation for the cumulants of \\(X\\) \\(\\chi_\\alpha\\) into the equation for the cumulants of \\(Y\\) above gives \\[\n\\begin{align}\n\\kappa_1 & = \\mu \\\\\n\\kappa_2 & = \\mu + \\frac{1}{2} \\frac{\\mu^2}{\\nu} \\\\\n\\kappa_3 & = \\mu + \\frac{3}{2} \\frac{\\mu^2}{\\nu} + \\frac{2}{3} \\frac{\\mu^3}{\\nu^2} \\\\\n\\kappa_4 & = \\mu + \\frac{7}{2} \\frac{\\mu^2}{\\nu} + 4 \\frac{\\mu^3}{\\nu^2} + \\frac{3}{2} \\frac{\\mu^4}{\\nu^3}. \\\\\n\\end{align}\n\\]\nWe have two regimes, controled by the parameter \\(\\mu / \\nu\\):\n\nIf \\(\\frac{\\mu}{\\nu} \\ll 1\\), the Poisson noise dominates and \\(\\kappa_\\alpha \\approx \\mu\\).\nIf \\(\\frac{\\mu}{\\nu} \\gg 1\\), the latent noise dominates and \\(\\kappa_\\alpha \\approx \\chi_\\alpha\\).\n\nFor higher cumulants, the separation between the regimes becomes less clean (i.e. it takes a smaller/larger \\(\\mu/\\nu\\) for one term to dominate.)\nIn terms of model parameters:\n\nMore frequent samples (smaller \\(\\delta t\\), larger \\(\\nu\\)) pushes us toward the Poisson-dominant regime.\nMore variable latent abundances (smaller \\(\\phi\\), smaller \\(\\nu\\)) pushes us toward the latent-dominant regime.\nA higher threshold of detection (larger \\(\\mu\\)) pushes us toward the latent-dominant regime."
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html#the-cornish-fisher-expansion-of-the-quantiles-of-y",
    "href": "notebooks/2024-02-22_StochasticMVP.html#the-cornish-fisher-expansion-of-the-quantiles-of-y",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "The Cornish-Fisher expansion of the quantiles of \\(Y\\)",
    "text": "The Cornish-Fisher expansion of the quantiles of \\(Y\\)\nUltimately, our goal is to estimate the probability that \\(Y &gt; \\hat{K}\\), the detection threshold. Thus, we need to estimate the CDF of \\(Y\\) from its cumulants \\(\\kappa_\\alpha\\). For that we will use the Cornish-Fisher expansion. The idea behind the expansion is to start by approximating the CDF as that of a Gaussian random variable with the correct mean and variance, and then iteratively adjust it for higher-order cumulants (skew, kurtosis, etc). It is defined as follows:\nThe quantile function \\(y(p)\\) (i.e. the value for which \\(\\text{CDF}(y) = p\\)) is approximated by \\(y(p) \\approx \\kappa_1 + {\\kappa_2}^{1/2} w_p\\), where (TODO: fix formatting here) \\[\n\\begin{align}\nw_p & = x \\\\\n    & + \\gamma_1 h_1(x) \\\\\n    & + \\gamma_2 h_2(x) + {\\gamma_1}^2 h_{11}(x) \\\\\n    & + \\cdots \\\\\nx   & = \\Phi^{-1}(p) \\\\\n\\gamma_{\\alpha-2} & = \\frac{\\kappa_\\alpha}{{\\kappa_2}^{\\alpha/2}} \\\\\nh_1(x) & = \\frac{\\text{He}_2(x)}{6} \\\\\nh_2(x) & = \\frac{\\text{He}_3(x)}{24} \\\\\nh_{11}(x) & = - \\frac{2\\text{He}_3(x) + \\text{He}_1(x)}{36} \\\\\n\\end{align}\n\\] Where \\(\\Phi\\) is the CGF of the standard normal distribution and \\(\\text{He}\\) are the probabilists’ Hermite polynomials. Note that each line of the sum must be included as a whole for the approximation to be valid at that level.\nFor fixed \\(x\\) (and therefore fixed quantile), the relative sizes of the terms of the expansion are controlled by the coefficients \\(\\gamma\\). In the Poisson-dominated regime: \\[\n\\begin{align}\n\\gamma_1 & = \\mu^{-1/2} \\\\\n\\gamma_2 & = \\gamma_1^2 = \\mu^{-1} \\\\\n\\end{align}\n\\] so truncating the expansion at a few terms should work well when \\(\\mu &gt; 1\\). Since we’re interested in having a significant probability of detection (e.g. &gt;90%), and our threshold of detection is at least one read, this is not a very limiting requirement.\nIn the latent-noise-dominated regime: \\[\n\\begin{align}\n\\gamma_1 & = \\frac{2^{5/2}}{3} \\nu^{-1/2} \\\\\n\\gamma_2 & = 6 \\nu^{-1}\n\\end{align}\n\\] so truncating the series at a few terms should work well when \\(\\nu \\gg 1\\). This is also not very limiting, because \\(\\nu\\) can be large either by making \\(\\phi\\) large (which is likely unless our data source is extremely noisy) or by sampling frequently so that \\(r \\delta t\\) is small.\nNotice that in the latent regime, the only \\(\\nu\\) (which controls the latent coefficient of variation) matters for the validity of the expansion, not \\(\\mu\\). However, for this regime to hold \\(\\mu \\gg \\nu &gt; 1\\). If we put our requirements for the two regimes together, we see that the approximation is good if both \\(\\mu &gt; 1\\) and \\(\\nu \\gg 1\\). [TODO: write this more clearly and illustrate with figure.]"
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html#numerical-illustrations-to-do",
    "href": "notebooks/2024-02-22_StochasticMVP.html#numerical-illustrations-to-do",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Numerical illustrations (to-do)",
    "text": "Numerical illustrations (to-do)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial.polynomial import Polynomial\nfrom numpy.polynomial.hermite_e import HermiteE\nfrom scipy.special import factorial\nfrom scipy.stats import poisson, norm, gamma\n\n\ndef cgf_x_series(mu: float, nu: float, order: int = 4) -&gt; Polynomial:\n    coeffs = np.zeros(order + 1)\n    # No zeroth coefficient\n    k = np.arange(1, order + 1)\n    coeffs[1:] = nu * (mu / nu) ** k / k**2\n    return Polynomial(coeffs)\n\n\ndef expm1_series(order: int = 4) -&gt; Polynomial:\n    k = np.arange(order + 1)\n    return Polynomial(1.0 / factorial(k)) - 1\n\n\ndef cgf_y_series(mu: float, nu: float, order: int = 4):\n    return cgf_x_series(mu, nu, order)(expm1_series(order)).cutdeg(order)\n\n\ndef cumulant_from_cgf(cgf: Polynomial, order: int) -&gt; float:\n    return cgf.deriv(order)(0)\n\nSpot-check cumulants:\n\nmu = 2.0\nnu = 10.0\ncgf = cgf_x_series(mu, nu)(expm1_series()).cutdeg(4)\ncoeffs = [0, 1, (1, 1 / 2), (1, 3 / 2, 2 / 3), (1, 7 / 2, 4, 3 / 2)]\npredicted_cumulants = [mu * Polynomial(c)(mu / nu) for c in coeffs]\nfor k in range(5):\n    print(cumulant_from_cgf(cgf, k), predicted_cumulants[k])\n\n0.0 0.0\n2.0 2.0\n2.2 2.2\n2.6533333333333333 2.6533333333333333\n3.7439999999999998 3.744\n\n\nCheck variance:\n\nnu = 10.0\nmu = np.arange(1, 100)\nk2 = [cumulant_from_cgf(cgf_y_series(m, nu), 2) for m in mu]\nplt.plot(mu, k2)\nplt.plot(mu, mu + (1 / 2) * mu**2 / nu, \":\")\nplt.plot(mu, (1 / 2) * mu**2 / nu, \"--\")\n\n\n\n\nNote that it is growing quadratically, but the \\(\\mu\\) term is not negligible.\n\ndef cornish_fisher(*cumulants):\n    # cumulants = (k_1, k_2, ...)\n    order = len(cumulants)\n    if order &lt; 2:\n        raise ValueError(\"Order of approximation must be &gt;= 2\")\n    if order &gt; 4:\n        raise ValueError(\"Order of approximation must be &lt;= 4\")\n    sigma = np.sqrt(cumulants[1])\n    poly = HermiteE((0, 1))\n    if order &gt;= 3:\n        gamma_1 = cumulants[2] / sigma**3\n        h_1 = HermiteE((0, 0, 1)) / 6\n        poly += gamma_1 * h_1\n    if order &gt;= 4:\n        gamma_2 = cumulants[3] / sigma**4\n        h_2 = HermiteE((0, 0, 0, 1)) / 24\n        h_11 = -HermiteE((0, 1, 0, 2)) / 36\n        poly += gamma_2 * h_2 + gamma_1**2 * h_11\n    return cumulants[0] + sigma * poly\n\n\nChecking the Cornish-Fisher expansion against common distributions\nCheck against Poisson distribution:\n\norder = 4\np = np.arange(0.01, 1.0, 0.01)\nx = norm.ppf(p)\n\nfor lamb in [1, 2, 4, 8]:\n    poisson_cumulants = [lamb] * order\n    for o in range(2, order + 1):\n        cf_poisson = cornish_fisher(*poisson_cumulants[:o])\n        plt.plot(p, cf_poisson(x), label=o)\n    plt.plot(p, poisson(lamb).ppf(p), color=\"k\", linestyle=\"--\", label=\"exact\")\n    plt.legend()\n    plt.title(f\"$\\lambda$ = {lamb}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck against Gamma distribution:\n\nscale = 1\nk = np.arange(1, order + 1)\n\nfor shape in [1, 2, 4, 8]:\n    gamma_cumulants = factorial(k - 1) * shape * scale**k\n    for o in range(2, order + 1):\n        cf_gamma = cornish_fisher(*gamma_cumulants[:o])\n        plt.plot(p, cf_gamma(x), label=o)\n    plt.plot(\n        p, gamma(shape, scale=scale).ppf(p), color=\"k\", linestyle=\"--\", label=\"exact\"\n    )\n    plt.legend()\n    plt.title(f\"shape = {shape}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCornish-Fisher expansions for the cumulative count distribution\n\norder = 4\nnu = 2.0\nfor mu in [1, 2, 4, 8]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.legend()\n    plt.title(f\"$\\mu$ = {mu}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder = 4\nnu = 4.0\nfor mu in [1, 2, 4, 8]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.legend()\n    plt.title(f\"$\\mu$ = {mu}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder = 4\nnu = 10.0\nfor mu in [1, 10, 100, 1000]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.legend()\n    plt.title(f\"$\\mu$ = {mu}\")\n    plt.show()"
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html#implications-for-cost",
    "href": "notebooks/2024-02-22_StochasticMVP.html#implications-for-cost",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Implications for cost",
    "text": "Implications for cost\nSketch of implications:\nIn the Poisson-dominated regime, a Gaussian approximation is pretty good, so \\[\ny(p) \\approx \\mu + \\mu^{1/2} \\Phi^{-1}(p)\n\\] Set \\(p\\) to be one minus the target probability of detection, set \\(y(p) = \\hat{K}\\), the detection threshold. Solve for \\(\\mu\\). Then compare this \\(\\mu\\) to the deterministic model. Will predict that there’s a delay in detection due to having to wait for the mean to be larger than the threshold to have a high probability of detection.\nIn the Latent-dominated regime, the terms of the Cornish-Fisher expansion just depend on \\(\\nu\\), not on \\(\\mu\\), so: \\[\n\\begin{align}\ny(p) & \\approx \\mu + \\frac{\\mu}{{(2\\nu)}^{1/2}} w_p(\\nu) \\\\\n     & = \\mu \\left(1 +\\frac{1}{{(2\\nu)}^{1/2}} w_p(\\nu)\\right) \\\\\n\\end{align}\n\\] Can caluclate \\(w_p(\\nu)\\) to as high order as we need, then solve for \\(\\mu\\) as in the Poisson regime Because \\(w_p\\) will be negative (since \\(p\\) is small), this will inflate the required \\(\\mu\\) for by a factor."
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html#playground-ignore",
    "href": "notebooks/2024-02-22_StochasticMVP.html#playground-ignore",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Playground (ignore)",
    "text": "Playground (ignore)\n\nimport numpy as np\nfrom scipy.stats import poisson, norm, nbinom\nimport matplotlib.pyplot as plt\n\nk = np.arange(40)\nlamb = 4\n\nplt.plot(k, poisson.cdf(k, lamb))\nplt.plot(k, norm.cdf(k, loc=lamb, scale=np.sqrt(lamb)))\n\n\n\n\n\np = np.arange(0.01, 0.20, 0.001)\nx = norm.ppf(p)\nfor lamb in [1, 2, 4, 10, 20]:\n    plt.plot(p, poisson.ppf(p, lamb), label=\"Poisson\")\n    plt.plot(p, lamb + np.sqrt(lamb) * x, label=\"Normal\")\n    plt.plot(p, lamb + np.sqrt(lamb) * x + (x**2 - 1) / 6, label=\"Cornish\")\n    plt.legend()\n    plt.title(f\"$\\lambda = {lamb}$\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np = np.arange(0.01, 0.50, 0.001)\nx = norm.ppf(p)\nfor lamb in [1, 2, 4, 10, 20]:\n    plt.plot(p, poisson.ppf(p, lamb), label=\"Poisson\")\n    plt.plot(p, lamb + np.sqrt(lamb) * x, label=\"Normal\")\n    plt.plot(p, lamb + np.sqrt(lamb) * x + (x**2 - 1) / 6, label=\"Cornish\")\n    plt.legend()\n    plt.title(f\"$\\lambda = {lamb}$\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlamb = np.arange(1, 20, 1)\nfor p in [0.05, 0.1, 0.1587, 0.2]:\n    x = norm.ppf(p)\n    plt.plot(lamb, poisson.ppf(p, lamb), label=\"Poisson\")\n    plt.plot(lamb, lamb + np.sqrt(lamb) * x, label=\"Normal\")\n    plt.plot(lamb, lamb + np.sqrt(lamb) * x + (x**2 - 1) / 6, label=\"Cornish\")\n    plt.legend()\n    plt.xlabel(\"$\\lambda$\")\n    plt.ylabel(f\"$p = {p}$ quartile\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooks like the Cornish-Fisher adjustment doesn’t really matter for \\(p &gt; 0.05\\).\n\nr = np.log(2) / 7\nphi = 2\ndelta_t = 3\np = 0.1\nx = norm.ppf(p)\nm = np.arange(0, 100, 1)\nprint(r * delta_t / (2 * phi))\nplt.plot(m, m)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x)\n\n0.07426576934570842\n\n\n\n\n\n\nr = np.log(2) / 7\nphi = 2\ndelta_t = 6\np = 0.1\nx = norm.ppf(p)\nm = np.arange(0, 100, 1)\nprint(r * delta_t / (2 * phi))\nplt.plot(m, m)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x)\n\n0.14853153869141683\n\n\n\n\n\n\nr = np.log(2) / 7\nphi = 50\ndelta_t = 3\np = 0.1\nx = norm.ppf(p)\nm = np.arange(0, 100, 1)\nprint(r * delta_t / (2 * phi))\nplt.plot(m, m)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x)\n\n0.0029706307738283366\n\n\n\n\n\n\nr = np.log(2) / 7\nphi = 2\ndelta_t = 3\np = 0.1\nx = norm.ppf(p)\nm = np.arange(0, 100, 1)\nprint(x)\nprint(np.sqrt(r * delta_t / (2 * phi)))\nprint(r * delta_t / (2 * phi))\nprint((x**2 - 1) / 6)\nplt.plot(m, m)\nplt.plot(m, m + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x)\nplt.plot(\n    m,\n    m\n    + np.sqrt(m + m**2 * r * delta_t / (2 * phi)) * x\n    + m * r * delta_t / (3 * phi) * (x**2 - 1) / 6,\n)\n\n-1.2815515655446004\n0.2725174661296197\n0.07426576934570842\n0.10706240252496935\n\n\n\n\n\n\nGamma-Poisson mixture\nQuestion: When is the Gaussian approximation to the PPF good enough and when does third-order Cornish-Fisher help?\n\ndef cornish_fisher_ppf(p, mean, variance, skew=0.0):\n    x = norm.ppf(p)\n    return mean + np.sqrt(variance) * (x + skew * (x**2 - 1) / 6)\n\n\np = np.arange(0.001, 0.20, 0.001)\nfor lamb in [1.0, 2.0, 4.0, 8.0]:\n    dist = poisson(lamb)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.legend()\n    plt.title(f\"$\\lambda = {lamb}$\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef nbinom2(mean, phi):\n    n = phi\n    p = phi / (mean + phi)\n    return nbinom(n, p)\n\n\nphi = 4\nfor mean in [0.5, 1.0, 2.0, 4.0]:\n    dist = nbinom2(mean, phi)\n    m, v = (mean, mean + mean**2 / phi)\n    print(np.allclose(dist.stats(\"mv\"), (m, v)))\n\nTrue\nTrue\nTrue\nTrue\n\n\n\np = np.arange(0.001, 0.20, 0.001)\nmus = np.logspace(0, 8, 9, base=2)\n\nphi = 100\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.ylim([0, mu])\n    plt.legend()\n    plt.title(f\"$\\mu = {mu}$\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np = np.arange(0.001, 0.20, 0.001)\nphi = 4\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.ylim([0, mu])\n    plt.legend()\n    plt.title(f\"$\\mu = {mu}$\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np = np.arange(0.001, 0.20, 0.001)\nphi = 2\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.ylim([0, mu])\n    plt.legend()\n    plt.title(f\"$\\mu = {mu}$\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np = np.arange(0.001, 0.999, 0.001)\n\nphi = 1\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(p, dist.ppf(p), label=\"Poisson\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v), label=\"Normal\")\n    plt.plot(p, cornish_fisher_ppf(p, m, v, s), label=\"Cornish\")\n    plt.ylim([0, 3 * mu])\n    plt.legend()\n    plt.title(f\"$\\mu = {mu}$\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np = 0.1\nmus = np.logspace(0, 8, 9, base=2)\nphi = 100\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(mu, dist.ppf(p), \".\", color=\"C0\", label=\"Poisson\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v), \".\", color=\"C1\", label=\"Normal\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v, s), \".\", color=\"C2\", label=\"Cornish\")\nplt.ylim([0.1, mus[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n\n\n\n\n\np = 0.1\nmus = np.logspace(0, 8, 9, base=2)\nphi = 4\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(mu, dist.ppf(p), \".\", color=\"C0\", label=\"Poisson\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v), \".\", color=\"C1\", label=\"Normal\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v, s), \".\", color=\"C2\", label=\"Cornish\")\nplt.ylim([0.1, mus[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n\n\n\n\n\np = 0.1\nmus = np.logspace(0, 8, 9, base=2)\nphi = 2\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(mu, dist.ppf(p), \".\", color=\"C0\", label=\"Poisson\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v), \".\", color=\"C1\", label=\"Normal\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v, s), \".\", color=\"C2\", label=\"Cornish\")\nplt.ylim([0.1, mus[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n\n\n\n\n\np = 0.1\nmus = np.logspace(0, 8, 9, base=2)\nphi = 1\nfor mu in mus:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(mu, dist.ppf(p), \".\", color=\"C0\", label=\"Poisson\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v), \".\", color=\"C1\", label=\"Normal\")\n    plt.plot(mu, cornish_fisher_ppf(p, m, v, s), \".\", color=\"C2\", label=\"Cornish\")\nplt.ylim([0.1, mus[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n\n\n\n\n\nx = norm.ppf(0.1)\nprint(6 * x / (x**2 - 1))\n\n-11.970136437445571\n\n\n\np = 0.1\n# mus = np.logspace(0, 8, 9, base=2)\nmu = 40\nphis = np.logspace(0, 7, 8, base=2)\nfor phi in phis:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(phi, dist.ppf(p), \".\", color=\"C0\", label=\"Poisson\")\n    plt.plot(phi, cornish_fisher_ppf(p, m, v), \".\", color=\"C1\", label=\"Normal\")\n    plt.plot(phi, cornish_fisher_ppf(p, m, v, s), \".\", color=\"C2\", label=\"Cornish\")\n# plt.ylim([0.1, phis[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n\n\n\n\n\np = 0.1\n# mus = np.logspace(0, 8, 9, base=2)\nmu = 10\nphis = np.logspace(0, 7, 8, base=2)\nfor phi in phis:\n    dist = nbinom2(mu, phi)\n    m, v, s = dist.stats(moments=\"mvs\")\n    plt.plot(phi, dist.ppf(p), \".\", color=\"C0\", label=\"Poisson\")\n    plt.plot(phi, cornish_fisher_ppf(p, m, v), \".\", color=\"C1\", label=\"Normal\")\n    plt.plot(phi, cornish_fisher_ppf(p, m, v, s), \".\", color=\"C2\", label=\"Cornish\")\n# plt.ylim([0.1, phis[-1]])\nplt.yscale(\"log\")\nplt.xscale(\"log\")\n# plt.legend()\n# plt.title(f\"$\\mu = {mu}$\")\nplt.show()\n\n\n\n\n\nfrom numpy.polynomial.hermite_e import HermiteE\n\n\ndef cf_approx(shape, order: int = 3):\n    poly = HermiteE((0, 1))\n    if order &gt; 2:\n        poly += 1.9 * shape ** (-1 / 2) * HermiteE((0, 0, 1 / 6))\n    if order &gt; 3:\n        poly += 6 * shape ** (-1) * HermiteE((0, 0, 1 / 24))\n        poly += 1.9**2 * shape ** (-1) * HermiteE((0, -1 / 36, 0, -2 / 36))\n    if order &gt; 4:\n        raise ValueError\n    return poly\n\n\np = np.arange(0.01, 1.0, 0.01)\n# x = np.arange(-2, 2, 0.01)\nx = norm.ppf(p)\n\nshape = 8\nplt.plot(p, cf_approx(shape, order=2)(x))\nplt.plot(p, cf_approx(shape, order=3)(x))\nplt.plot(p, cf_approx(shape, order=4)(x))\nplt.title(shape)\nplt.show()\n\nshape = 4\nplt.plot(p, cf_approx(shape, order=2)(x))\nplt.plot(p, cf_approx(shape, order=3)(x))\nplt.plot(p, cf_approx(shape, order=4)(x))\nplt.title(shape)\nplt.show()\n\nshape = 1\nplt.plot(p, cf_approx(shape, order=2)(x))\nplt.plot(p, cf_approx(shape, order=3)(x))\nplt.plot(p, cf_approx(shape, order=4)(x))\nplt.title(shape)\nplt.show()"
  },
  {
    "objectID": "notebooks/2023-09-20-qpcr_analysis.html",
    "href": "notebooks/2023-09-20-qpcr_analysis.html",
    "title": "2023-07-18 Extraction Experiment 1 qPCR Analysis",
    "section": "",
    "text": "Testing the efficacy of RNA extraction kits in influent samples. See experiment google doc."
  },
  {
    "objectID": "notebooks/2023-09-20-qpcr_analysis.html#objectives",
    "href": "notebooks/2023-09-20-qpcr_analysis.html#objectives",
    "title": "2023-07-18 Extraction Experiment 1 qPCR Analysis",
    "section": "",
    "text": "Testing the efficacy of RNA extraction kits in influent samples. See experiment google doc."
  },
  {
    "objectID": "notebooks/2023-09-20-qpcr_analysis.html#preliminary-work",
    "href": "notebooks/2023-09-20-qpcr_analysis.html#preliminary-work",
    "title": "2023-07-18 Extraction Experiment 1 qPCR Analysis",
    "section": "Preliminary work",
    "text": "Preliminary work\n\nSomeone else already exported qPCR results as CSV\nI copied the file meta_samples.csv and manually added elution volumes that I found in this doc and saved the results in meta_samples_with_elution_volumes.csv."
  },
  {
    "objectID": "notebooks/2023-09-20-qpcr_analysis.html#data-import",
    "href": "notebooks/2023-09-20-qpcr_analysis.html#data-import",
    "title": "2023-07-18 Extraction Experiment 1 qPCR Analysis",
    "section": "Data import",
    "text": "Data import\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\n\n\ndata_dir &lt;-\n  \"~/airport/[2023-07-18] Extraction-kit comparison 1: Influent/qPCR results/\"\nfilename_pattern &lt;- \"_Results_\"\n\nNot sure why I had to change skip from 23 to 22.\n\ncol_types &lt;- list(\n  Target = col_character(),\n  Cq = col_double()\n)\nraw_data &lt;- list.files(\n  data_dir,\n  pattern = filename_pattern,\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 22,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind()\n\n[1] \"/Users/dan/airport/[2023-07-18] Extraction-kit comparison 1: Influent/qPCR results//Cov2/2023-07-26_Cov2-kits_Results_20230728 133121.csv\"        \n[2] \"/Users/dan/airport/[2023-07-18] Extraction-kit comparison 1: Influent/qPCR results//CrA/2023-07-26_CrA-kits_Results_20230728 135806.csv\"          \n[3] \"/Users/dan/airport/[2023-07-18] Extraction-kit comparison 1: Influent/qPCR results//Noro/2023-07-26_Noro_kits_Results_20230728 140336.csv\"        \n[4] \"/Users/dan/airport/[2023-07-18] Extraction-kit comparison 1: Influent/qPCR results//phagemid/2023-07-26_Phagemid-kits_Results_20230728 141050.csv\"\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\nprint(raw_data)\n\n# A tibble: 333 × 21\n    Well `Well Position` Omit  Sample  Target Task    Reporter Quencher\n   &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n 1     1 A1              FALSE 1_ZR_A  Cov2   UNKNOWN FAM      NFQ-MGB \n 2     2 A2              FALSE 1_ZR_B  Cov2   UNKNOWN FAM      NFQ-MGB \n 3     3 A3              FALSE 1_ZR_C  Cov2   UNKNOWN FAM      NFQ-MGB \n 4     4 A4              FALSE 2_ZDR_A Cov2   UNKNOWN FAM      NFQ-MGB \n 5     5 A5              FALSE 2_ZDR_B Cov2   UNKNOWN FAM      NFQ-MGB \n 6     6 A6              FALSE 2_ZDR_C Cov2   UNKNOWN FAM      NFQ-MGB \n 7     7 A7              FALSE 3_IP_A  Cov2   UNKNOWN FAM      NFQ-MGB \n 8     8 A8              FALSE 3_IP_B  Cov2   UNKNOWN FAM      NFQ-MGB \n 9     9 A9              FALSE 3_IP_C  Cov2   UNKNOWN FAM      NFQ-MGB \n10    10 A10             FALSE 4_NV_A  Cov2   UNKNOWN FAM      NFQ-MGB \n# ℹ 323 more rows\n# ℹ 13 more variables: `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;\n\n\n\nraw_data |&gt; count(Target)\n\n# A tibble: 4 × 2\n  Target     n\n  &lt;chr&gt;  &lt;int&gt;\n1 Cov2      71\n2 CrA       95\n3 Noro      71\n4 Phg       96\n\n\nTODO: use other metadata table\n\nmetadata_file &lt;- paste0(data_dir, \"meta_samples_with_elution_volumes.csv\")\nmetadata &lt;- read_csv(metadata_file)\n\nRows: 27 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): sample_qPCR, Kit, treatment_group, LPA, other_treatment, Virus_spec...\ndbl (2): group, Elution_volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(metadata)\n\nRows: 27\nColumns: 9\n$ sample_qPCR     &lt;chr&gt; \"1_ZR_A\", \"1_ZR_B\", \"1_ZR_C\", \"2_ZDR_A\", \"2_ZDR_B\", \"2…\n$ Kit             &lt;chr&gt; \"Zymo RNA\", \"Zymo RNA\", \"Zymo RNA\", \"Zymo RNA/DNA\", \"Z…\n$ treatment_group &lt;chr&gt; \"ZQ-RNA\", \"ZQ-RNA\", \"ZQ-RNA\", \"ZQ-RNADNA\", \"ZQ-RNADNA\"…\n$ LPA             &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", …\n$ group           &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ other_treatment &lt;chr&gt; \"Shield\", \"Shield\", \"Shield\", \"Shield\", \"Shield\", \"Shi…\n$ Virus_specific  &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", …\n$ FP_Data         &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No…\n$ Elution_volume  &lt;dbl&gt; 15, 15, 15, 50, 50, 50, 100, 100, 100, 30, 30, 30, 100…\n\n\n\ntidy_data &lt;- raw_data |&gt;\n  mutate(\n    replicate = str_extract(Sample, \"[A-Z]$\"),\n  ) |&gt;\n  left_join(\n    metadata,\n    by = join_by(Sample == sample_qPCR)\n  )\nglimpse(tidy_data)\n\nRows: 333\nColumns: 30\n$ Well                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Sample                  &lt;chr&gt; \"1_ZR_A\", \"1_ZR_B\", \"1_ZR_C\", \"2_ZDR_A\", \"2_ZD…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"U…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"Amp\", \"Amp\", \"Amp\", \"Amp\", \"Amp\", \"Amp\", \"Amp…\n$ `Amp Score`             &lt;dbl&gt; 1.1919015, 1.2241620, 1.3026766, 1.3064751, 1.…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.59224, 33.85453, 32.88852, 32.32325, 35.019…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9630252, 0.9623811, 0.9919308, 0.9907208, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 33.98352, 33.89186, 33.66943, 33.92028, 34.152…\n$ `Cq SD`                 &lt;dbl&gt; 0.55335254, 0.05279505, 1.10437933, 2.25853860…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.04098764, 0.04098764, 0.04098764, 0.04098764…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 31, 30, 30, 29, 32, 31, 31, 31, 30, 32, 33, 31…\n$ replicate               &lt;chr&gt; \"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"…\n$ Kit                     &lt;chr&gt; \"Zymo RNA\", \"Zymo RNA\", \"Zymo RNA\", \"Zymo RNA/…\n$ treatment_group         &lt;chr&gt; \"ZQ-RNA\", \"ZQ-RNA\", \"ZQ-RNA\", \"ZQ-RNADNA\", \"ZQ…\n$ LPA                     &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ group                   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3…\n$ other_treatment         &lt;chr&gt; \"Shield\", \"Shield\", \"Shield\", \"Shield\", \"Shiel…\n$ Virus_specific          &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\"…\n$ FP_Data                 &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Y…\n$ Elution_volume          &lt;dbl&gt; 15, 15, 15, 50, 50, 50, 100, 100, 100, 30, 30,…\n\n\n\ntidy_data |&gt;\n  count(Sample, Kit, LPA, Target, replicate) |&gt;\n  print(n = Inf)\n\n# A tibble: 126 × 6\n    Sample      Kit                 LPA   Target replicate     n\n    &lt;chr&gt;       &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;\n  1 1.0         &lt;NA&gt;                &lt;NA&gt;  Cov2   &lt;NA&gt;          3\n  2 1.0         &lt;NA&gt;                &lt;NA&gt;  Noro   &lt;NA&gt;          3\n  3 10.0        &lt;NA&gt;                &lt;NA&gt;  Cov2   &lt;NA&gt;          3\n  4 10.0        &lt;NA&gt;                &lt;NA&gt;  Noro   &lt;NA&gt;          3\n  5 100.0       &lt;NA&gt;                &lt;NA&gt;  Cov2   &lt;NA&gt;          3\n  6 100.0       &lt;NA&gt;                &lt;NA&gt;  Noro   &lt;NA&gt;          3\n  7 1000.0      &lt;NA&gt;                &lt;NA&gt;  Cov2   &lt;NA&gt;          3\n  8 1000.0      &lt;NA&gt;                &lt;NA&gt;  Noro   &lt;NA&gt;          3\n  9 10000.0     &lt;NA&gt;                &lt;NA&gt;  Cov2   &lt;NA&gt;          2\n 10 10000.0     &lt;NA&gt;                &lt;NA&gt;  Noro   &lt;NA&gt;          2\n 11 1_ZR_A      Zymo RNA            No    Cov2   A             2\n 12 1_ZR_A      Zymo RNA            No    CrA    A             3\n 13 1_ZR_A      Zymo RNA            No    Noro   A             2\n 14 1_ZR_A      Zymo RNA            No    Phg    A             3\n 15 1_ZR_B      Zymo RNA            No    Cov2   B             2\n 16 1_ZR_B      Zymo RNA            No    CrA    B             3\n 17 1_ZR_B      Zymo RNA            No    Noro   B             2\n 18 1_ZR_B      Zymo RNA            No    Phg    B             3\n 19 1_ZR_C      Zymo RNA            No    Cov2   C             2\n 20 1_ZR_c      &lt;NA&gt;                &lt;NA&gt;  CrA    &lt;NA&gt;          3\n 21 1_ZR_c      &lt;NA&gt;                &lt;NA&gt;  Noro   &lt;NA&gt;          2\n 22 1_ZR_c      &lt;NA&gt;                &lt;NA&gt;  Phg    &lt;NA&gt;          3\n 23 20          &lt;NA&gt;                &lt;NA&gt;  CrA    &lt;NA&gt;          3\n 24 200         &lt;NA&gt;                &lt;NA&gt;  CrA    &lt;NA&gt;          3\n 25 2000        &lt;NA&gt;                &lt;NA&gt;  CrA    &lt;NA&gt;          3\n 26 20000       &lt;NA&gt;                &lt;NA&gt;  CrA    &lt;NA&gt;          3\n 27 2_ZDR_A     Zymo RNA/DNA        No    Cov2   A             2\n 28 2_ZDR_A     Zymo RNA/DNA        No    CrA    A             3\n 29 2_ZDR_A     Zymo RNA/DNA        No    Noro   A             2\n 30 2_ZDR_A     Zymo RNA/DNA        No    Phg    A             3\n 31 2_ZDR_B     Zymo RNA/DNA        No    Cov2   B             2\n 32 2_ZDR_B     Zymo RNA/DNA        No    CrA    B             2\n 33 2_ZDR_B     Zymo RNA/DNA        No    Noro   B             2\n 34 2_ZDR_B     Zymo RNA/DNA        No    Phg    B             3\n 35 2_ZDR_C     Zymo RNA/DNA        No    Cov2   C             2\n 36 2_ZDR_C     Zymo RNA/DNA        No    CrA    C             3\n 37 2_ZDR_C     Zymo RNA/DNA        No    Noro   C             2\n 38 2_ZDR_C     Zymo RNA/DNA        No    Phg    C             3\n 39 3_IP_A      Invitrogen Purelink No    Cov2   A             2\n 40 3_IP_A      Invitrogen Purelink No    CrA    A             3\n 41 3_IP_A      Invitrogen Purelink No    Noro   A             2\n 42 3_IP_A      Invitrogen Purelink No    Phg    A             3\n 43 3_IP_B      Invitrogen Purelink No    Cov2   B             2\n 44 3_IP_B      Invitrogen Purelink No    CrA    B             3\n 45 3_IP_B      Invitrogen Purelink No    Noro   B             2\n 46 3_IP_B      Invitrogen Purelink No    Phg    B             3\n 47 3_IP_C      Invitrogen Purelink No    Cov2   C             2\n 48 3_IP_C      Invitrogen Purelink No    CrA    C             3\n 49 3_IP_C      Invitrogen Purelink No    Noro   C             2\n 50 3_IP_C      Invitrogen Purelink No    Phg    C             3\n 51 4_NV_A      Nucleospin Virus    No    Cov2   A             2\n 52 4_NV_A      Nucleospin Virus    No    CrA    A             3\n 53 4_NV_A      Nucleospin Virus    No    Noro   A             2\n 54 4_NV_A      Nucleospin Virus    No    Phg    A             3\n 55 4_NV_B      Nucleospin Virus    No    Cov2   B             2\n 56 4_NV_B      Nucleospin Virus    No    CrA    B             3\n 57 4_NV_B      Nucleospin Virus    No    Noro   B             2\n 58 4_NV_B      Nucleospin Virus    No    Phg    B             3\n 59 4_NV_C      Nucleospin Virus    No    Cov2   C             2\n 60 4_NV_C      Nucleospin Virus    No    CrA    C             3\n 61 4_NV_C      Nucleospin Virus    No    Noro   C             2\n 62 4_NV_C      Nucleospin Virus    No    Phg    C             3\n 63 5_QM_A      QIAamp MinElute     No    Cov2   A             2\n 64 5_QM_A      QIAamp MinElute     No    CrA    A             3\n 65 5_QM_A      QIAamp MinElute     No    Noro   A             2\n 66 5_QM_A      QIAamp MinElute     No    Phg    A             3\n 67 5_QM_B      QIAamp MinElute     No    Cov2   B             2\n 68 5_QM_B      QIAamp MinElute     No    CrA    B             3\n 69 5_QM_B      QIAamp MinElute     No    Noro   B             2\n 70 5_QM_B      QIAamp MinElute     No    Phg    B             3\n 71 5_QM_C      QIAamp MinElute     No    Cov2   C             2\n 72 5_QM_C      QIAamp MinElute     No    CrA    C             3\n 73 5_QM_C      QIAamp MinElute     No    Noro   C             2\n 74 5_QM_C      QIAamp MinElute     No    Phg    C             3\n 75 6_QVR_A     QIAamp Viral RNA    No    Cov2   A             2\n 76 6_QVR_A     QIAamp Viral RNA    No    CrA    A             3\n 77 6_QVR_A     QIAamp Viral RNA    No    Noro   A             2\n 78 6_QVR_A     QIAamp Viral RNA    No    Phg    A             3\n 79 6_QVR_B     QIAamp Viral RNA    No    Cov2   B             2\n 80 6_QVR_B     QIAamp Viral RNA    No    CrA    B             3\n 81 6_QVR_B     QIAamp Viral RNA    No    Noro   B             2\n 82 6_QVR_B     QIAamp Viral RNA    No    Phg    B             3\n 83 6_QVR_C     QIAamp Viral RNA    No    Cov2   C             2\n 84 6_QVR_C     QIAamp Viral RNA    No    CrA    C             3\n 85 6_QVR_C     QIAamp Viral RNA    No    Noro   C             2\n 86 6_QVR_C     QIAamp Viral RNA    No    Phg    C             3\n 87 7_NV-car_A  Nucleospin Virus    Yes   Cov2   A             2\n 88 7_NV-car_A  Nucleospin Virus    Yes   CrA    A             3\n 89 7_NV-car_A  Nucleospin Virus    Yes   Noro   A             2\n 90 7_NV-car_A  Nucleospin Virus    Yes   Phg    A             3\n 91 7_NV-car_B  Nucleospin Virus    Yes   Cov2   B             2\n 92 7_NV-car_B  Nucleospin Virus    Yes   CrA    B             3\n 93 7_NV-car_B  Nucleospin Virus    Yes   Noro   B             2\n 94 7_NV-car_B  Nucleospin Virus    Yes   Phg    B             3\n 95 7_NV-car_C  Nucleospin Virus    Yes   Cov2   C             2\n 96 7_NV-car_C  Nucleospin Virus    Yes   CrA    C             3\n 97 7_NV-car_C  Nucleospin Virus    Yes   Noro   C             2\n 98 7_NV-car_C  Nucleospin Virus    Yes   Phg    C             3\n 99 8_QM-car_A  QIAamp MinElute     Yes   Cov2   A             2\n100 8_QM-car_A  QIAamp MinElute     Yes   CrA    A             3\n101 8_QM-car_A  QIAamp MinElute     Yes   Noro   A             2\n102 8_QM-car_A  QIAamp MinElute     Yes   Phg    A             3\n103 8_QM-car_B  QIAamp MinElute     Yes   Cov2   B             2\n104 8_QM-car_B  QIAamp MinElute     Yes   CrA    B             3\n105 8_QM-car_B  QIAamp MinElute     Yes   Noro   B             2\n106 8_QM-car_B  QIAamp MinElute     Yes   Phg    B             3\n107 8_QM-car_C  QIAamp MinElute     Yes   Cov2   C             2\n108 8_QM-car_C  QIAamp MinElute     Yes   CrA    C             3\n109 8_QM-car_C  QIAamp MinElute     Yes   Noro   C             2\n110 8_QM-car_C  QIAamp MinElute     Yes   Phg    C             3\n111 9_QVR-car_A QIAamp Viral RNA    Yes   Cov2   A             2\n112 9_QVR-car_A QIAamp Viral RNA    Yes   CrA    A             3\n113 9_QVR-car_A QIAamp Viral RNA    Yes   Noro   A             2\n114 9_QVR-car_A QIAamp Viral RNA    Yes   Phg    A             3\n115 9_QVR-car_B QIAamp Viral RNA    Yes   Cov2   B             2\n116 9_QVR-car_B QIAamp Viral RNA    Yes   CrA    B             3\n117 9_QVR-car_B QIAamp Viral RNA    Yes   Noro   B             2\n118 9_QVR-car_B QIAamp Viral RNA    Yes   Phg    B             3\n119 9_QVR-car_C QIAamp Viral RNA    Yes   Cov2   C             2\n120 9_QVR-car_C QIAamp Viral RNA    Yes   CrA    C             3\n121 9_QVR-car_C QIAamp Viral RNA    Yes   Noro   C             2\n122 9_QVR-car_C QIAamp Viral RNA    Yes   Phg    C             3\n123 &lt;NA&gt;        &lt;NA&gt;                &lt;NA&gt;  Cov2   &lt;NA&gt;          3\n124 &lt;NA&gt;        &lt;NA&gt;                &lt;NA&gt;  CrA    &lt;NA&gt;          3\n125 &lt;NA&gt;        &lt;NA&gt;                &lt;NA&gt;  Noro   &lt;NA&gt;          3\n126 &lt;NA&gt;        &lt;NA&gt;                &lt;NA&gt;  Phg    &lt;NA&gt;         15"
  },
  {
    "objectID": "notebooks/2023-09-20-qpcr_analysis.html#kit-comparison",
    "href": "notebooks/2023-09-20-qpcr_analysis.html#kit-comparison",
    "title": "2023-07-18 Extraction Experiment 1 qPCR Analysis",
    "section": "Kit comparison",
    "text": "Kit comparison\n\ntidy_data |&gt;\n  filter(!is.na(Kit)) |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = Kit,\n    color = interaction(replicate, LPA),\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\")\n\nWarning: Removed 15 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\n\nAdjusting for elution volume\nAssume the amplification efficiency is 100%, so that an increase in initial concentration by a factor of \\(x\\) decreases \\(C_q\\) by \\(\\log_{2}(x)\\).\nIf a method has elution volume \\(v\\) and we dilute it to total volume \\(V\\), this reduces its final concentration by a factor \\(v / V\\). We can put different methods on the same footing by adding \\(\\log_{2}(v/V)\\) to \\(C_q\\) (so that large elution volumes are penalized with a higher adjusted \\(C_q\\)).\n\nfinal_volume &lt;- 100\ntidy_data |&gt;\n  mutate(elution_adjusted_Cq = Cq + log2(Elution_volume / final_volume)) |&gt;\n  filter(!is.na(Kit)) |&gt;\n  ggplot(mapping = aes(\n    x = elution_adjusted_Cq,\n    y = Kit,\n    color = interaction(replicate, LPA),\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\")\n\nWarning: Removed 15 rows containing non-finite values (`stat_summary()`)."
  },
  {
    "objectID": "notebooks/2023-07-18_ExtractionKitEvaluation.html",
    "href": "notebooks/2023-07-18_ExtractionKitEvaluation.html",
    "title": "Examine extraction-kit comparison experiment",
    "section": "",
    "text": "Drive folder"
  },
  {
    "objectID": "notebooks/2023-07-18_ExtractionKitEvaluation.html#metadata",
    "href": "notebooks/2023-07-18_ExtractionKitEvaluation.html#metadata",
    "title": "Examine extraction-kit comparison experiment",
    "section": "metadata",
    "text": "metadata\n\nmeta_samples &lt;- path(data_path, \"qpcr\", \"meta_samples.csv\") %&gt;%\n  read_csv() %&gt;%\n  rename(sample_qpcr = sample_qPCR) %&gt;%\n  janitor::clean_names() %&gt;%\n  glimpse()\n\nRows: 27 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): sample_qPCR, Kit, treatment_group, LPA, other_treatment, Virus_spec...\ndbl (1): group\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRows: 27\nColumns: 8\n$ sample_qpcr     &lt;chr&gt; \"1_ZR_A\", \"1_ZR_B\", \"1_ZR_C\", \"2_ZDR_A\", \"2_ZDR_B\", \"2…\n$ kit             &lt;chr&gt; \"Zymo RNA\", \"Zymo RNA\", \"Zymo RNA\", \"Zymo RNA/DNA\", \"Z…\n$ treatment_group &lt;chr&gt; \"ZQ-RNA\", \"ZQ-RNA\", \"ZQ-RNA\", \"ZQ-RNADNA\", \"ZQ-RNADNA\"…\n$ lpa             &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", …\n$ group           &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ other_treatment &lt;chr&gt; \"Shield\", \"Shield\", \"Shield\", \"Shield\", \"Shield\", \"Shi…\n$ virus_specific  &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", …\n$ fp_data         &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No…\n\nmeta_targets &lt;- path(data_path, \"qpcr\", \"meta_target.csv\") %&gt;%\n  read_csv() %&gt;%\n  rename(target_qpcr = target_qPCR) %&gt;%\n  janitor::clean_names() %&gt;%\n  glimpse()\n\nRows: 5 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): target_qPCR, target\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRows: 5\nColumns: 2\n$ target_qpcr &lt;chr&gt; \"CrA\", \"Cov2\", \"Noro\", \"16S\", \"Phg\"\n$ target      &lt;chr&gt; \"Crassphage\", \"SARS-CoV-2\", \"Norovirus\", \"bacteria\", \"phag…"
  },
  {
    "objectID": "notebooks/2023-07-18_ExtractionKitEvaluation.html#qpcr",
    "href": "notebooks/2023-07-18_ExtractionKitEvaluation.html#qpcr",
    "title": "Examine extraction-kit comparison experiment",
    "section": "qPCR",
    "text": "qPCR\n\nfns &lt;- data_path %&gt;%\n  dir_ls(recurse = TRUE, glob = \"*_Standard Curve Result_*.csv\")\nfns %&gt;% path_file()\n\n[1] \"2023-07-26_Cov2-kits_Standard Curve Result_20230728 133121.csv\"    \n[2] \"2023-07-26_CrA-kits_Standard Curve Result_20230728 135806.csv\"     \n[3] \"2023-07-26_Noro_kits_Standard Curve Result_20230728 140336.csv\"    \n[4] \"2023-07-26_Phagemid-kits_Standard Curve Result_20230728 141050.csv\"\n\n\n\nresults_raw &lt;- tibble(file = fns) %&gt;%\n  mutate(\n    .keep = \"unused\",\n    data = map(file, read_qpcr_results_csv)\n  ) %&gt;%\n  unnest(data)\n\n\nresults &lt;- results_raw %&gt;%\n  rename(target_qpcr = target) %&gt;%\n  left_join(meta_samples, by = c(\"sample\" = \"sample_qpcr\")) %&gt;%\n  left_join(meta_targets, by = \"target_qpcr\") %&gt;%\n  mutate()\n\n\namp curves\n\nfns_amp &lt;- data_path %&gt;%\n  dir_ls(recurse = TRUE, glob = \"*_Amplification Data_*.csv\") %&gt;%\n  str_subset(negate = TRUE, \"Raw\")\n\namp1 &lt;- tibble(file = fns_amp) %&gt;%\n  mutate(\n    .keep = \"unused\",\n    data = map(file, read_qpcr_amplification_csv)\n  ) %&gt;%\n  unnest(data) %&gt;%\n  rename(target_qpcr = target) %&gt;%\n  left_join(results)\n\nJoining with `by = join_by(well, well_position, row, column, target_qpcr,\nsample, omit)`\n\n\n\n\nSecond experiment\n\ndata_path2 &lt;- here(\"_data/2023-07-13-volume-and-dilution\")\n\nmeta_samples2 &lt;- path(data_path2, \"qpcr\", \"meta_samples.csv\") %&gt;%\n  read_csv() %&gt;%\n  rename(sample_qpcr = sample_qPCR) %&gt;%\n  janitor::clean_names()\n\nRows: 6 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): sample_qPCR, treatment_group\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmeta_targets2 &lt;- path(data_path2, \"qpcr\", \"meta_target.csv\") %&gt;%\n  read_csv() %&gt;%\n  rename(target_qpcr = target_qPCR) %&gt;%\n  janitor::clean_names()\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): target_qPCR, target\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfns2 &lt;- data_path2 %&gt;%\n  dir_ls(recurse = TRUE, glob = \"*_Standard Curve Result_*.csv\")\nfns2 %&gt;% path_file()\n\n[1] \"2023-07-20_SSTweenCovNoro_Standard Curve Result_20230724 143043.csv\"\n[2] \"2023-07-20_TweenCrA16S_Standard Curve Result_20230724 142856.csv\"   \n\nresults_raw2 &lt;- tibble(file = fns2) %&gt;%\n  mutate(\n    .keep = \"unused\",\n    data = map(file, read_qpcr_results_csv)\n  ) %&gt;%\n  unnest(data)\n\nresults2 &lt;- results_raw2 %&gt;%\n  rename(target_qpcr = target) %&gt;%\n  left_join(meta_samples2, by = c(\"sample\" = \"sample_qpcr\")) %&gt;%\n  left_join(meta_targets2, by = \"target_qpcr\") %&gt;%\n  mutate()\n\nfns_amp2 &lt;- data_path2 %&gt;%\n  dir_ls(recurse = TRUE, glob = \"*_Amplification Data_*.csv\") %&gt;%\n  str_subset(negate = TRUE, \"Raw\")\n\namp2 &lt;- tibble(file = fns_amp2) %&gt;%\n  mutate(\n    .keep = \"unused\",\n    data = map(file, read_qpcr_amplification_csv)\n  ) %&gt;%\n  unnest(data) %&gt;%\n  rename(target_qpcr = target) %&gt;%\n  left_join(results2)\n\nJoining with `by = join_by(well, well_position, row, column, target_qpcr,\nsample, omit)`\n\n\n\ndata_path3 &lt;- here(\"_data/2023-06-13-concentration\")\n\nmeta_samples3 &lt;- path(data_path3, \"qpcr\", \"meta_samples.csv\") %&gt;%\n  read_csv() %&gt;%\n  mutate(treatment_group = as.character(treatment_group)) %&gt;%\n  # rename(sample_qpcr = sample_qPCR) %&gt;%\n  janitor::clean_names()\n\nRows: 12 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): sample_qpcr, filter, sewer_system, method, method_short\ndbl (4): treatment_group, sample_number, volume, amicon_mwco\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmeta_targets3 &lt;- path(data_path3, \"qpcr\", \"meta_target.csv\") %&gt;%\n  read_csv() %&gt;%\n  rename(target_qpcr = target_qPCR) %&gt;%\n  janitor::clean_names()\n\nRows: 5 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): target_qPCR, target\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfns3 &lt;- data_path3 %&gt;%\n  dir_ls(recurse = TRUE, glob = \"*_Standard Curve Result_*.csv\")\nfns3 %&gt;% path_file()\n\n[1] \"2023-06-23_CPAmicon_16S_Standard Curve Result_20230625 111656.csv\"    \n[2] \"2023-06-23_AmiconCP-COVNORO_Standard Curve Result_20230623 171728.csv\"\n[3] \"2023-06-23_CrAPhg_Standard Curve Result_20230623 171458.csv\"          \n\nresults_raw3 &lt;- tibble(file = fns3) %&gt;%\n  mutate(\n    .keep = \"unused\",\n    data = map(file, read_qpcr_results_csv)\n  ) %&gt;%\n  unnest(data)\nprint(unique(results_raw3$sample))\n\n [1] \"N_30\"               \"N_0.05-80\"          \"6840.0\"            \n [4] \"N_100\"              \"N_Ultra-80\"         \"684.0\"             \n [7] \"N_0.05\"             \"S_0.05-80\"          \"68.4\"              \n[10] \"N_Ultra\"            \"S_Ultra-80\"         \"6.840000000000001\" \n[13] \"S_30\"               NA                   \"0.6840000000000002\"\n[16] \"S_100\"              \"S_0.05\"             \"S_Ultra\"           \n[19] \"10.0\"               \"100000.0\"           \"10000.0\"           \n[22] \"1000.0\"             \"100.0\"             \n\nprint(unique(results_raw3$target))\n\n[1] \"16S\"  \"Cov2\" \"Noro\" \"CrA\"  \"phg\" \n\nresults3 &lt;- results_raw3 %&gt;%\n  rename(target_qpcr = target) %&gt;%\n  left_join(meta_samples3, by = c(\"sample\" = \"sample_qpcr\")) %&gt;%\n  left_join(meta_targets3, by = \"target_qpcr\") %&gt;%\n  mutate()\n\nfns_amp3 &lt;- data_path3 %&gt;%\n  dir_ls(recurse = TRUE, glob = \"*_Amplification Data_*.csv\") %&gt;%\n  str_subset(negate = TRUE, \"Raw\")\n\n\namp3 &lt;- tibble(file = fns_amp3) %&gt;%\n  mutate(\n    .keep = \"unused\",\n    data = map(file, read_qpcr_amplification_csv)\n  ) %&gt;%\n  unnest(data) %&gt;%\n  rename(target_qpcr = target) %&gt;%\n  left_join(results3)\n\nJoining with `by = join_by(well, well_position, row, column, target_qpcr,\nsample, omit)`\n\n\n\namp1$experiment &lt;- 1\namp2$experiment &lt;- 2\namp3$experiment &lt;- 3\namp &lt;- bind_rows(amp1, amp2, amp3)\n# amp &lt;- bind_rows(amp1, amp2)"
  },
  {
    "objectID": "notebooks/2023-07-18_ExtractionKitEvaluation.html#inspect-sars2-amplification-curves",
    "href": "notebooks/2023-07-18_ExtractionKitEvaluation.html#inspect-sars2-amplification-curves",
    "title": "Examine extraction-kit comparison experiment",
    "section": "Inspect SARS2 amplification curves",
    "text": "Inspect SARS2 amplification curves\n\ndelta_rn_min &lt;- 1e-3\nct_threshold &lt;- results %&gt;%\n  filter(target == \"SARS-CoV-2\") %&gt;%\n  pull(threshold) %&gt;%\n  unique()\nstopifnot(length(ct_threshold) == 1)\n\namp %&gt;%\n  filter(\n    target == \"SARS-CoV-2\",\n    !is.na(treatment_group)\n  ) %&gt;%\n  ggplot(aes(cycle_number, pmax(d_rn, delta_rn_min), color = treatment_group)) +\n  # scale_color_manual(values = colors_oi %&gt;% unname) +\n  scale_y_log10() +\n  geom_line(aes(group = well)) +\n  geom_hline(yintercept = ct_threshold, alpha = 0.3) +\n  facet_wrap(~experiment) +\n  # scale_color_brewer(type = 'qual') +\n  # geom_point(data = baselines, aes(shape = baseline_boundary), size = 3) +\n  # scale_shape_manual(values = c(1, 4)) +\n  labs(y = \"Delta Rn\", x = \"Cycle\", color = \"Target\")\n\n\n\n\n\namp %&gt;%\n  filter(\n    target == \"SARS-CoV-2\",\n    task %in% c(\"STANDARD\", \"UNKNOWN\")\n  ) %&gt;%\n  ggplot(aes(cycle_number, pmax(d_rn, delta_rn_min), color = treatment_group)) +\n  facet_wrap(~treatment_group) +\n  # scale_color_manual(values = colors_oi %&gt;% unname) +\n  scale_x_continuous(limits = c(25, 40)) +\n  scale_y_log10() +\n  geom_line(aes(group = well)) +\n  geom_hline(yintercept = ct_threshold, alpha = 0.3) +\n  labs(y = \"Delta Rn\", x = \"Cycle\", color = \"Target\")\n\nWarning: Removed 2064 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\namp %&gt;%\n  filter(\n    target == \"SARS-CoV-2\",\n    task == \"STANDARD\"\n  ) %&gt;%\n  ggplot(aes(cycle_number, pmax(d_rn, delta_rn_min), color = sample)) +\n  # scale_color_manual(values = colors_oi %&gt;% unname) +\n  # scale_x_continuous(limits = c(25, 40)) +\n  scale_y_log10() +\n  geom_line(aes(group = well)) +\n  geom_hline(yintercept = ct_threshold, alpha = 0.3) +\n  labs(y = \"Delta Rn\", x = \"Cycle\", color = \"Target\")\n\n\n\n\n\nCheck SC versus target samples\n\namp %&gt;%\n  filter(\n    target == \"SARS-CoV-2\",\n    task %in% c(\"STANDARD\", \"UNKNOWN\")\n  ) %&gt;%\n  ggplot(aes(cycle_number, pmax(d_rn, delta_rn_min), color = interaction(task, experiment))) +\n  scale_color_manual(values = colors_oi %&gt;% unname()) +\n  scale_x_continuous(limits = c(15, 40)) +\n  scale_y_log10() +\n  geom_line(aes(group = interaction(well, experiment))) +\n  geom_hline(yintercept = ct_threshold, alpha = 0.3) +\n  labs(y = \"Delta Rn\", x = \"Cycle\", color = \"Target\")\n\nWarning: Removed 1960 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\namp %&gt;%\n  filter(\n    # target == 'SARS-CoV-2',\n    task %in% c(\"STANDARD\", \"UNKNOWN\")\n  ) %&gt;%\n  ggplot(aes(cycle_number, pmax(d_rn, delta_rn_min), color = interaction(task, experiment))) +\n  facet_wrap(~target) +\n  scale_color_manual(values = colors_oi %&gt;% unname()) +\n  scale_x_continuous(limits = c(15, 40)) +\n  scale_y_log10() +\n  geom_line(aes(group = interaction(well, experiment))) +\n  geom_hline(yintercept = ct_threshold, alpha = 0.3) +\n  labs(y = \"Delta Rn\", x = \"Cycle\", color = \"Target\")\n\nWarning: Removed 3220 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "notebooks/2023-09-23-pmmov_qpcr.html",
    "href": "notebooks/2023-09-23-pmmov_qpcr.html",
    "title": "2023-09-13 PMMoV qPCR Analysis",
    "section": "",
    "text": "NOTE: Need sample metadata. I don’t know what the samples are.\n\nTest qPCR of PMMoV\nTest duplexed qPCR\nSee Google Doc"
  },
  {
    "objectID": "notebooks/2023-09-23-pmmov_qpcr.html#objectives",
    "href": "notebooks/2023-09-23-pmmov_qpcr.html#objectives",
    "title": "2023-09-13 PMMoV qPCR Analysis",
    "section": "",
    "text": "NOTE: Need sample metadata. I don’t know what the samples are.\n\nTest qPCR of PMMoV\nTest duplexed qPCR\nSee Google Doc"
  },
  {
    "objectID": "notebooks/2023-09-23-pmmov_qpcr.html#preliminary-work",
    "href": "notebooks/2023-09-23-pmmov_qpcr.html#preliminary-work",
    "title": "2023-09-13 PMMoV qPCR Analysis",
    "section": "Preliminary work",
    "text": "Preliminary work\n\nDan copied the .eds file from the NAO qPCR data directory to the experiment directory and exported .csv files."
  },
  {
    "objectID": "notebooks/2023-09-23-pmmov_qpcr.html#data-import",
    "href": "notebooks/2023-09-23-pmmov_qpcr.html#data-import",
    "title": "2023-09-13 PMMoV qPCR Analysis",
    "section": "Data import",
    "text": "Data import\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\n\n\ndata_dir &lt;-\n  \"~/airport/[2023-09-13] PMMoV and Multiplex qPCR/\"\nfilename_pattern &lt;- \"Results\"\n\n\ncol_types &lt;- list(\n  Target = col_character(),\n  Cq = col_double()\n)\nraw_data &lt;- list.files(\n  paste0(data_dir, \"qpcr\"),\n  pattern = filename_pattern,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind()\n\n[1] \"/Users/dan/airport/[2023-09-13] PMMoV and Multiplex qPCR/qpcr/Ari PMMoV_Results_20230925_115306.csv\"\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\nprint(raw_data)\n\n# A tibble: 63 × 21\n    Well `Well Position` Omit  Sample Target Task    Reporter Quencher\n   &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n 1     1 A1              FALSE 1      PPMoV  UNKNOWN VIC      NFQ-MGB \n 2     2 A2              FALSE 1      PPMoV  UNKNOWN VIC      NFQ-MGB \n 3     3 A3              FALSE 1      PPMoV  UNKNOWN VIC      NFQ-MGB \n 4     5 A5              FALSE 1      N2     UNKNOWN FAM      NFQ-MGB \n 5     6 A6              FALSE 1      N2     UNKNOWN FAM      NFQ-MGB \n 6     8 A8              FALSE NTC    N2     UNKNOWN FAM      NFQ-MGB \n 7    10 A10             FALSE 1      N2     UNKNOWN FAM      NFQ-MGB \n 8    10 A10             FALSE 1      PPMoV  UNKNOWN VIC      NFQ-MGB \n 9    11 A11             FALSE 1      N2     UNKNOWN FAM      NFQ-MGB \n10    11 A11             FALSE 1      PPMoV  UNKNOWN VIC      NFQ-MGB \n# ℹ 53 more rows\n# ℹ 13 more variables: `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;\n\n\nIt looks like the software splits duplexed targets into two rows, but doesn’t save the information about multiplexing.\n\nraw_data |&gt; count(Target, Reporter, Sample, Quencher)\n\n# A tibble: 19 × 5\n   Target Reporter Sample Quencher     n\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;int&gt;\n 1 N2     FAM      1      NFQ-MGB      5\n 2 N2     FAM      2      NFQ-MGB      5\n 3 N2     FAM      NTC    NFQ-MGB      2\n 4 N2     FAM      SC1    NFQ-MGB      3\n 5 N2     FAM      SC2    NFQ-MGB      3\n 6 N2     FAM      SC3    NFQ-MGB      3\n 7 N2     FAM      SC4    NFQ-MGB      3\n 8 N2     FAM      SC5    NFQ-MGB      3\n 9 N2     FAM      SC6    NFQ-MGB      3\n10 N2     FAM      &lt;NA&gt;   NFQ-MGB      2\n11 PPMoV  VIC      1      NFQ-MGB      6\n12 PPMoV  VIC      2      NFQ-MGB      6\n13 PPMoV  VIC      NTC    NFQ-MGB      4\n14 PPMoV  VIC      SC1    NFQ-MGB      1\n15 PPMoV  VIC      SC2    NFQ-MGB      3\n16 PPMoV  VIC      SC3    NFQ-MGB      3\n17 PPMoV  VIC      SC4    NFQ-MGB      3\n18 PPMoV  VIC      SC5    NFQ-MGB      3\n19 PPMoV  VIC      &lt;NA&gt;   NFQ-MGB      2\n\n\n\nraw_data |&gt;\n  count(`Well Position`) |&gt;\n  print(n = Inf)\n\n# A tibble: 54 × 2\n   `Well Position`     n\n   &lt;chr&gt;           &lt;int&gt;\n 1 A1                  1\n 2 A10                 2\n 3 A11                 2\n 4 A12                 2\n 5 A2                  1\n 6 A3                  1\n 7 A5                  1\n 8 A6                  1\n 9 A8                  1\n10 B1                  1\n11 B10                 2\n12 B11                 2\n13 B12                 2\n14 B2                  1\n15 B3                  1\n16 B5                  1\n17 B6                  1\n18 C1                  1\n19 C10                 2\n20 C11                 2\n21 C12                 2\n22 C2                  1\n23 C3                  1\n24 C5                  1\n25 C6                  1\n26 C7                  1\n27 D3                  1\n28 D5                  1\n29 D6                  1\n30 D7                  1\n31 E1                  1\n32 E2                  1\n33 E3                  1\n34 E5                  1\n35 E6                  1\n36 E7                  1\n37 F1                  1\n38 F2                  1\n39 F3                  1\n40 F5                  1\n41 F6                  1\n42 F7                  1\n43 G1                  1\n44 G2                  1\n45 G3                  1\n46 G5                  1\n47 G6                  1\n48 G7                  1\n49 H1                  1\n50 H2                  1\n51 H3                  1\n52 H5                  1\n53 H6                  1\n54 H7                  1\n\n\nWe can count the multiplexing and save it as a column:\n\nwith_counts &lt;- raw_data |&gt;\n  left_join(count(raw_data, Well, name = \"n_multiplex\"), by = join_by(Well))\n\nIt looks like the multiplexed wells didn’t amplifiy well. No N2 wells amplified. It also looks like there was poor amp in general.\n\nwith_counts |&gt;\n  count(Target, Reporter, Sample, n_multiplex, `Amp Status`) |&gt;\n  print(n = Inf)\n\n# A tibble: 32 × 6\n   Target Reporter Sample n_multiplex `Amp Status`     n\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        &lt;int&gt;\n 1 N2     FAM      1                1 AMP              1\n 2 N2     FAM      1                1 NO_AMP           1\n 3 N2     FAM      1                2 NO_AMP           3\n 4 N2     FAM      2                1 NO_AMP           2\n 5 N2     FAM      2                2 NO_AMP           3\n 6 N2     FAM      NTC              1 NO_AMP           1\n 7 N2     FAM      NTC              2 NO_AMP           1\n 8 N2     FAM      SC1              1 AMP              1\n 9 N2     FAM      SC1              1 NO_AMP           2\n10 N2     FAM      SC2              1 AMP              2\n11 N2     FAM      SC2              1 NO_AMP           1\n12 N2     FAM      SC3              1 AMP              2\n13 N2     FAM      SC3              1 NO_AMP           1\n14 N2     FAM      SC4              1 AMP              2\n15 N2     FAM      SC4              1 NO_AMP           1\n16 N2     FAM      SC5              1 AMP              2\n17 N2     FAM      SC5              1 NO_AMP           1\n18 N2     FAM      SC6              1 AMP              1\n19 N2     FAM      SC6              1 NO_AMP           2\n20 N2     FAM      &lt;NA&gt;             2 NO_AMP           2\n21 PPMoV  VIC      1                1 AMP              3\n22 PPMoV  VIC      1                2 AMP              3\n23 PPMoV  VIC      2                1 AMP              3\n24 PPMoV  VIC      2                2 AMP              3\n25 PPMoV  VIC      NTC              1 NO_AMP           3\n26 PPMoV  VIC      NTC              2 NO_AMP           1\n27 PPMoV  VIC      SC1              1 AMP              1\n28 PPMoV  VIC      SC2              1 AMP              3\n29 PPMoV  VIC      SC3              1 AMP              3\n30 PPMoV  VIC      SC4              1 AMP              3\n31 PPMoV  VIC      SC5              1 AMP              3\n32 PPMoV  VIC      &lt;NA&gt;             2 NO_AMP           2\n\n\n\nwith_counts |&gt;\n  count(Target, n_multiplex, `Amp Status`) |&gt;\n  print(n = Inf)\n\n# A tibble: 7 × 4\n  Target n_multiplex `Amp Status`     n\n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        &lt;int&gt;\n1 N2               1 AMP             11\n2 N2               1 NO_AMP          12\n3 N2               2 NO_AMP           9\n4 PPMoV            1 AMP             19\n5 PPMoV            1 NO_AMP           3\n6 PPMoV            2 AMP              6\n7 PPMoV            2 NO_AMP           3\n\n\nCheck that the thresholds are the same for every well with the same target\n\nwith_counts |&gt;\n  count(Target, Threshold) |&gt;\n  print(n = Inf)\n\n# A tibble: 2 × 3\n  Target Threshold     n\n  &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;\n1 N2          5.82    32\n2 PPMoV       1.53    31"
  },
  {
    "objectID": "notebooks/2023-09-23-pmmov_qpcr.html#amplification-curves",
    "href": "notebooks/2023-09-23-pmmov_qpcr.html#amplification-curves",
    "title": "2023-09-13 PMMoV qPCR Analysis",
    "section": "Amplification curves",
    "text": "Amplification curves\n\namp_data &lt;- list.files(\n  paste0(data_dir, \"qpcr\"),\n  pattern = \"Amplification Data\",\n  full.names = TRUE,\n) |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind() |&gt;\n  left_join(with_counts,\n    by = join_by(Well, `Well Position`, Sample, Target)\n  ) |&gt;\n  print()\n\nWarning: The following named parsers don't match the column names: Cq\n\n\n# A tibble: 2,520 × 26\n    Well `Well Position` `Cycle Number` Target    Rn     dRn Sample Omit.x\n   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;lgl&gt; \n 1     1 A1                           1 PPMoV   13.5 -1.41   1      FALSE \n 2     1 A1                           2 PPMoV   13.8 -1.23   1      FALSE \n 3     1 A1                           3 PPMoV   14.4 -0.752  1      FALSE \n 4     1 A1                           4 PPMoV   14.8 -0.474  1      FALSE \n 5     1 A1                           5 PPMoV   15.2 -0.290  1      FALSE \n 6     1 A1                           6 PPMoV   15.5 -0.0864 1      FALSE \n 7     1 A1                           7 PPMoV   15.8  0.0333 1      FALSE \n 8     1 A1                           8 PPMoV   16.0  0.138  1      FALSE \n 9     1 A1                           9 PPMoV   16.3  0.272  1      FALSE \n10     1 A1                          10 PPMoV   16.5  0.302  1      FALSE \n# ℹ 2,510 more rows\n# ℹ 18 more variables: Omit.y &lt;lgl&gt;, Task &lt;chr&gt;, Reporter &lt;chr&gt;,\n#   Quencher &lt;chr&gt;, `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;, n_multiplex &lt;int&gt;\n\n\n\nAmplification curves by sample\nThe one that amplified:\n\namp_data |&gt;\n  filter(`Amp Status` == \"AMP\") |&gt;\n  ggplot(mapping = aes(\n    x = `Cycle Number`,\n    y = dRn,\n    group = Well,\n    color = as.factor(n_multiplex),\n  )) +\n  geom_line() +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold,\n  )) +\n  scale_y_log10() +\n  facet_grid(\n    rows = vars(Target), cols = vars(Sample), scales = \"free_y\"\n  )\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 169 rows containing missing values (`geom_line()`).\n\n\n\n\n\nThe ones that didn’t:\n\namp_data |&gt;\n  filter(`Amp Status` == \"NO_AMP\") |&gt;\n  ggplot(mapping = aes(\n    x = `Cycle Number`,\n    y = dRn,\n    group = Well,\n    color = as.factor(n_multiplex),\n  )) +\n  geom_line() +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold,\n  )) +\n  scale_y_log10() +\n  facet_grid(\n    rows = vars(Target), cols = vars(Sample), scales = \"free_y\"\n  )\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 187 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\nAmplification curves by well\n\namp_data |&gt;\n  ggplot(mapping = aes(\n    x = `Cycle Number`,\n    y = dRn,\n    color = as.factor(Target),\n  )) +\n  geom_line() +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  )) +\n  geom_vline(aes(xintercept = `Cq`, color = as.factor(Target)),\n    linetype = \"dashed\"\n  ) +\n  scale_y_log10() +\n  facet_wrap(~`Well Position`, scales = \"free\")\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 15 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 960 rows containing missing values (`geom_vline()`)."
  },
  {
    "objectID": "notebooks/2023-10-24-setled-solids-dissociation-test.html",
    "href": "notebooks/2023-10-24-setled-solids-dissociation-test.html",
    "title": "2023-10-24 Analysis of sludge dissociation test qPCR",
    "section": "",
    "text": "See Google Doc"
  },
  {
    "objectID": "notebooks/2023-10-24-setled-solids-dissociation-test.html#objectives",
    "href": "notebooks/2023-10-24-setled-solids-dissociation-test.html#objectives",
    "title": "2023-10-24 Analysis of sludge dissociation test qPCR",
    "section": "",
    "text": "See Google Doc"
  },
  {
    "objectID": "notebooks/2023-10-24-setled-solids-dissociation-test.html#preliminary-work",
    "href": "notebooks/2023-10-24-setled-solids-dissociation-test.html#preliminary-work",
    "title": "2023-10-24 Analysis of sludge dissociation test qPCR",
    "section": "Preliminary work",
    "text": "Preliminary work\nExported csv files from Olivia’s eds file uploads. Also exported metadata google sheets as CSV"
  },
  {
    "objectID": "notebooks/2023-10-24-setled-solids-dissociation-test.html#data-import",
    "href": "notebooks/2023-10-24-setled-solids-dissociation-test.html#data-import",
    "title": "2023-10-24 Analysis of sludge dissociation test qPCR",
    "section": "Data import",
    "text": "Data import\n\nlibrary(here)\n\nhere() starts at /Users/dan/notebook\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(ggh4x)\n\n\nget_plate &lt;- function(f) {\n  str_extract(basename(f),\n    \"(.*)_(.*)_[0-9]{8}_[0-9]{6}\\\\.csv\",\n    group = 1\n  )\n}\n\n\ndata_dir &lt;- here(\"~\", \"airport\")\nexperiments &lt;- c(\"[2023-10-18] New Combined Protocol run-through\")\n\n\nfilename_pattern &lt;- \"_Results_\"\ncol_types &lt;- list(\n  Target = col_character(),\n  Cq = col_double(),\n  `Treatment Group` = col_character()\n)\nraw_data &lt;- list.files(\n  map_chr(experiments, function(exp) {\n    here(data_dir, exp, \"qpcr\")\n  }),\n  pattern = filename_pattern,\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f, skip = 23, col_types = col_types) |&gt;\n      mutate(plate = get_plate(f))\n  }) |&gt;\n  list_rbind() |&gt;\n  glimpse()\n\n[1] \"/Users/dan/airport/[2023-10-18] New Combined Protocol run-through/qpcr/2023-10-24_Cov2Noro_Results_20231024_143610.csv\"\n[2] \"/Users/dan/airport/[2023-10-18] New Combined Protocol run-through/qpcr/2023-10-24_CrA16S_Results_20231024_143657.csv\"  \n[3] \"/Users/dan/airport/[2023-10-18] New Combined Protocol run-through/qpcr/2023-10-24_PMMoV_Results_20231024_160308.csv\"   \n\n\nWarning: The following named parsers don't match the column names: Treatment\nGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: The following named parsers don't match the column names: Treatment\nGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: The following named parsers don't match the column names: Treatment\nGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 223\nColumns: 22\n$ Well                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"3C\", \"3C\", \"3C\", \"1A\", \"1A\"…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"U…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.285299, 1.261658, 1.291325, 1.307973, 1.3246…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.47677, 33.83380, 33.69172, 33.08026, 32.584…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9706938, 0.9862888, 0.9894744, 0.9870891, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 33.66743, 33.66743, 33.66743, 32.88368, 32.883…\n$ `Cq SD`                 &lt;dbl&gt; 0.17974764, 0.17974764, 0.17974764, 0.26309028…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.09301463, 0.09301463, 0.09301463, 0.09301463…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 29, 29, 29, 29, 28, 28, 24, 25, 24, 24, 24, 24…\n$ plate                   &lt;chr&gt; \"2023-10-24_Cov2Noro\", \"2023-10-24_Cov2Noro\", …\n\n\n\nmetadata_file &lt;- here(\n  data_dir,\n  experiments[1],\n  \"metadata.csv\"\n)\nmetadata &lt;- experiments |&gt;\n  map(function(exp) {\n    read_csv(here(data_dir, exp, \"metadata.csv\"), col_types = col_types)\n  }) |&gt;\n  list_rbind() |&gt;\n  glimpse()\n\nWarning: The following named parsers don't match the column names: Target, Cq\n\n\nRows: 9\nColumns: 12\n$ Sample_ID               &lt;chr&gt; \"1A\", \"1B\", \"1C\", \"2A\", \"2B\", \"2C\", \"3A\", \"3B\"…\n$ `Treatment Group`       &lt;chr&gt; \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"3\", \"3\", \"3\"\n$ Dissocation             &lt;chr&gt; \"Vortex 5m\", \"Vortex 5m\", \"Vortex 5m\", \"Sonica…\n$ `Vortex time (min)`     &lt;dbl&gt; 5, 5, 5, 0, 0, 0, 1, 1, 1\n$ `Sonication time (min)` &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1\n$ CollectionDate          &lt;date&gt; 2023-10-17, 2023-10-17, 2023-10-17, 2023-10-17…\n$ Source                  &lt;chr&gt; \"Sludge\", \"Sludge\", \"Sludge\", \"Sludge\", \"Sludg…\n$ `Filtration time (sec)` &lt;dbl&gt; 10, 10, 10, 10, 10, 60, 15, 10, 10\n$ `CP Time`               &lt;time&gt; 03:37:00, 04:03:00, 04:00:00, 04:27:00, 04:00:…\n$ `Shield Added (uL)`     &lt;dbl&gt; 400, 400, 400, 400, 400, 400, 400, 400, 400\n$ `Qubit RNA`             &lt;dbl&gt; 24.2, 24.2, 18.5, 27.1, 27.7, 98.0, 100.0, 32…\n$ `Qubit DNA`             &lt;chr&gt; \"40\", \"36.4\", \"29.9\", \"59.2\", \"41.9\", \"97.2\", …\n\n\n\ntidy_data &lt;- raw_data |&gt;\n  separate_wider_regex(\n    `Well Position`,\n    c(well_row = \"[A-Z]+\", well_col = \"[0-9]+\"),\n    cols_remove = FALSE,\n  ) |&gt;\n  left_join(metadata, by = join_by(Sample == Sample_ID)) |&gt;\n  mutate(Target = if_else(Target == \"PMMV\", \"PMMoV\", Target)) |&gt;\n  glimpse()\n\nRows: 223\nColumns: 35\n$ Well                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ well_row                &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ well_col                &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"3C\", \"3C\", \"3C\", \"1A\", \"1A\"…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"U…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.285299, 1.261658, 1.291325, 1.307973, 1.3246…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.47677, 33.83380, 33.69172, 33.08026, 32.584…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9706938, 0.9862888, 0.9894744, 0.9870891, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 33.66743, 33.66743, 33.66743, 32.88368, 32.883…\n$ `Cq SD`                 &lt;dbl&gt; 0.17974764, 0.17974764, 0.17974764, 0.26309028…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.09301463, 0.09301463, 0.09301463, 0.09301463…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 29, 29, 29, 29, 28, 28, 24, 25, 24, 24, 24, 24…\n$ plate                   &lt;chr&gt; \"2023-10-24_Cov2Noro\", \"2023-10-24_Cov2Noro\", …\n$ `Treatment Group`       &lt;chr&gt; \"1\", \"1\", \"1\", \"3\", \"3\", \"3\", \"1\", \"1\", \"1\", \"…\n$ Dissocation             &lt;chr&gt; \"Vortex 5m\", \"Vortex 5m\", \"Vortex 5m\", \"Vortex…\n$ `Vortex time (min)`     &lt;dbl&gt; 5, 5, 5, 1, 1, 1, 5, 5, 5, 1, 1, 1, 5, 5, 5, N…\n$ `Sonication time (min)` &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, N…\n$ CollectionDate          &lt;date&gt; 2023-10-17, 2023-10-17, 2023-10-17, 2023-10-1…\n$ Source                  &lt;chr&gt; \"Sludge\", \"Sludge\", \"Sludge\", \"Sludge\", \"Sludg…\n$ `Filtration time (sec)` &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10…\n$ `CP Time`               &lt;time&gt; 03:37:00, 03:37:00, 03:37:00, 04:13:00, 04:13…\n$ `Shield Added (uL)`     &lt;dbl&gt; 400, 400, 400, 400, 400, 400, 400, 400, 400, 4…\n$ `Qubit RNA`             &lt;dbl&gt; 24.2, 24.2, 24.2, 25.7, 25.7, 25.7, 24.2, 24.2…\n$ `Qubit DNA`             &lt;chr&gt; \"40\", \"40\", \"40\", \"29.2\", \"29.2\", \"29.2\", \"40\"…\n\n\n\namp_data &lt;- list.files(\n  map_chr(experiments, function(exp) {\n    here(data_dir, exp, \"qpcr\")\n  }),\n  pattern = \"Amplification Data\",\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    ) |&gt;\n      mutate(plate = get_plate(f))\n  }) |&gt;\n  list_rbind() |&gt;\n  mutate(Target = if_else(Target == \"PMMV\", \"PMMoV\", Target)) |&gt;\n  left_join(tidy_data,\n    by = join_by(plate, Well, `Well Position`, Sample, Omit, Target)\n  ) |&gt;\n  glimpse()\n\n[1] \"/Users/dan/airport/[2023-10-18] New Combined Protocol run-through/qpcr/2023-10-24_Cov2Noro_Amplification Data_20231024_143610.csv\"\n[2] \"/Users/dan/airport/[2023-10-18] New Combined Protocol run-through/qpcr/2023-10-24_CrA16S_Amplification Data_20231024_143657.csv\"  \n[3] \"/Users/dan/airport/[2023-10-18] New Combined Protocol run-through/qpcr/2023-10-24_PMMoV_Amplification Data_20231024_160308.csv\"   \n\n\nWarning: The following named parsers don't match the column names: Cq, Treatment Group\nThe following named parsers don't match the column names: Cq, Treatment Group\nThe following named parsers don't match the column names: Cq, Treatment Group\n\n\nRows: 8,920\nColumns: 38\n$ Well                    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\"…\n$ `Cycle Number`          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Rn                      &lt;dbl&gt; 0.4810576, 0.4760514, 0.4685638, 0.4619383, 0.…\n$ dRn                     &lt;dbl&gt; 0.0255262889, 0.0208546813, 0.0137017026, 0.00…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ plate                   &lt;chr&gt; \"2023-10-24_Cov2Noro\", \"2023-10-24_Cov2Noro\", …\n$ well_row                &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ well_col                &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"U…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.285299, 1.285299, 1.285299, 1.285299, 1.2852…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.47677, 33.47677, 33.47677, 33.47677, 33.476…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9706938, 0.9706938, 0.9706938, 0.9706938, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 33.66743, 33.66743, 33.66743, 33.66743, 33.667…\n$ `Cq SD`                 &lt;dbl&gt; 0.1797476, 0.1797476, 0.1797476, 0.1797476, 0.…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.09301463, 0.09301463, 0.09301463, 0.09301463…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29…\n$ `Treatment Group`       &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ Dissocation             &lt;chr&gt; \"Vortex 5m\", \"Vortex 5m\", \"Vortex 5m\", \"Vortex…\n$ `Vortex time (min)`     &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5…\n$ `Sonication time (min)` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ CollectionDate          &lt;date&gt; 2023-10-17, 2023-10-17, 2023-10-17, 2023-10-1…\n$ Source                  &lt;chr&gt; \"Sludge\", \"Sludge\", \"Sludge\", \"Sludge\", \"Sludg…\n$ `Filtration time (sec)` &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10…\n$ `CP Time`               &lt;time&gt; 03:37:00, 03:37:00, 03:37:00, 03:37:00, 03:37…\n$ `Shield Added (uL)`     &lt;dbl&gt; 400, 400, 400, 400, 400, 400, 400, 400, 400, 4…\n$ `Qubit RNA`             &lt;dbl&gt; 24.2, 24.2, 24.2, 24.2, 24.2, 24.2, 24.2, 24.2…\n$ `Qubit DNA`             &lt;chr&gt; \"40\", \"40\", \"40\", \"40\", \"40\", \"40\", \"40\", \"40\"…"
  },
  {
    "objectID": "notebooks/2023-10-24-setled-solids-dissociation-test.html#quality-control",
    "href": "notebooks/2023-10-24-setled-solids-dissociation-test.html#quality-control",
    "title": "2023-10-24 Analysis of sludge dissociation test qPCR",
    "section": "Quality control",
    "text": "Quality control\n\ntidy_data |&gt; count(Task, is.na(Cq))\n\n# A tibble: 4 × 3\n  Task     `is.na(Cq)`     n\n  &lt;chr&gt;    &lt;lgl&gt;       &lt;int&gt;\n1 NTC      FALSE           2\n2 NTC      TRUE           11\n3 STANDARD FALSE          75\n4 UNKNOWN  FALSE         135\n\ntidy_data |&gt;\n  filter(Task == \"NTC\", !is.na(Cq)) |&gt;\n  glimpse()\n\nRows: 2\nColumns: 35\n$ Well                    &lt;dbl&gt; 82, 83\n$ well_row                &lt;chr&gt; \"G\", \"G\"\n$ well_col                &lt;chr&gt; \"10\", \"11\"\n$ `Well Position`         &lt;chr&gt; \"G10\", \"G11\"\n$ Omit                    &lt;lgl&gt; FALSE, FALSE\n$ Sample                  &lt;chr&gt; NA, NA\n$ Target                  &lt;chr&gt; \"16S\", \"16S\"\n$ Task                    &lt;chr&gt; \"NTC\", \"NTC\"\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\"\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\"\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\"\n$ `Amp Score`             &lt;dbl&gt; 1.322672, 1.322650\n$ `Curve Quality`         &lt;lgl&gt; NA, NA\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA\n$ Cq                      &lt;dbl&gt; 30.11587, 30.00533\n$ `Cq Confidence`         &lt;dbl&gt; 0.9847199, 0.9853924\n$ `Cq Mean`               &lt;dbl&gt; 30.0606, 30.0606\n$ `Cq SD`                 &lt;dbl&gt; 0.07816425, 0.07816425\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE\n$ Threshold               &lt;dbl&gt; 0.2087637, 0.2087637\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE\n$ `Baseline Start`        &lt;dbl&gt; 3, 3\n$ `Baseline End`          &lt;dbl&gt; 24, 24\n$ plate                   &lt;chr&gt; \"2023-10-24_CrA16S\", \"2023-10-24_CrA16S\"\n$ `Treatment Group`       &lt;chr&gt; NA, NA\n$ Dissocation             &lt;chr&gt; NA, NA\n$ `Vortex time (min)`     &lt;dbl&gt; NA, NA\n$ `Sonication time (min)` &lt;dbl&gt; NA, NA\n$ CollectionDate          &lt;date&gt; NA, NA\n$ Source                  &lt;chr&gt; NA, NA\n$ `Filtration time (sec)` &lt;dbl&gt; NA, NA\n$ `CP Time`               &lt;time&gt; NA, NA\n$ `Shield Added (uL)`     &lt;dbl&gt; NA, NA\n$ `Qubit RNA`             &lt;dbl&gt; NA, NA\n$ `Qubit DNA`             &lt;chr&gt; NA, NA\n\n\n\namp_data |&gt;\n  filter(Task == \"NTC\") |&gt;\n  ggplot(aes(x = `Cycle Number`, y = dRn)) +\n  geom_line(mapping = aes(\n    group = Well,\n  )) +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10(limits = c(1e-3, 1e1)) +\n  facet_wrap(~ interaction(plate, Target))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 95 rows containing missing values (`geom_line()`).\n\n\n\n\n\nThere is the usual amplification of 16S from contamination.\n\nAll the amplification curves\n\namp_data |&gt;\n  ggplot(aes(x = `Cycle Number`, y = dRn)) +\n  geom_line(mapping = aes(\n    color = Task,\n    group = Well,\n  )) +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10(limits = c(1e-3, 1e1)) +\n  facet_wrap(vars(Target))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 139 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\n16S unknowns only\nFigure out the one outlier curve.\n\ntidy_data |&gt;\n  filter(Target == \"16S\", Task == \"UNKNOWN\") |&gt;\n  filter(Cq == max(Cq))\n\n# A tibble: 1 × 35\n   Well well_row well_col `Well Position` Omit  Sample Target Task    Reporter\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;           &lt;lgl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   \n1     7 A        7        A7              FALSE 1A     16S    UNKNOWN FAM     \n# ℹ 26 more variables: Quencher &lt;chr&gt;, `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;, plate &lt;chr&gt;,\n#   `Treatment Group` &lt;chr&gt;, Dissocation &lt;chr&gt;, `Vortex time (min)` &lt;dbl&gt;,\n#   `Sonication time (min)` &lt;dbl&gt;, CollectionDate &lt;date&gt;, Source &lt;chr&gt;, …\n\n\n\namp_data |&gt;\n  filter(Target == \"16S\", Task == \"UNKNOWN\") |&gt;\n  ggplot(aes(x = `Cycle Number`, y = dRn)) +\n  geom_line(mapping = aes(\n    color = Well == 7,\n    group = Well,\n  )) +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10(limits = c(1e-3, 1e1))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 31 rows containing missing values (`geom_line()`).\n\n\n\n\n\nThis one well looks weird:\n\namp_data |&gt;\n  filter(Target == \"16S\", Task == \"UNKNOWN\") |&gt;\n  ggplot(aes(x = `Cycle Number`, y = Rn)) +\n  geom_line(mapping = aes(\n    color = Well == 7,\n    group = Well,\n  )) +\n  scale_y_log10() # limits = c(1e-3, 1e1))\n\n\n\n\n\n\nPMMoV only\nOne technical replicate is an outlier for low Cq:\n\namp_data |&gt;\n  filter(Target == \"PMMoV\", Task == \"UNKNOWN\", `Treatment Group` == 3) |&gt;\n  ggplot(aes(x = `Cycle Number`, y = dRn)) +\n  geom_line(mapping = aes(\n    color = Sample,\n    group = Well,\n  )) +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10(limits = c(1e-3, 1e1))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 45 rows containing missing values (`geom_line()`).\n\n\n\n\n\nAmp curve looks ok"
  },
  {
    "objectID": "notebooks/2023-10-24-setled-solids-dissociation-test.html#compare-methods",
    "href": "notebooks/2023-10-24-setled-solids-dissociation-test.html#compare-methods",
    "title": "2023-10-24 Analysis of sludge dissociation test qPCR",
    "section": "Compare methods",
    "text": "Compare methods\nUsing ggh4x to fix the scales to have the same spacing. Currently doing manually, but should automate.\n\nscales &lt;- list(\n  scale_x_continuous(limits = c(21, 24)),\n  scale_x_continuous(limits = c(32, 35)),\n  scale_x_continuous(limits = c(21, 24)),\n  scale_x_continuous(limits = c(27, 30)),\n  scale_x_continuous(limits = c(22, 25))\n)\ntidy_data |&gt;\n  filter(Task == \"UNKNOWN\") |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = Dissocation,\n    color = Sample,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\") +\n  facetted_pos_scales(x = scales)"
  },
  {
    "objectID": "notebooks/2024-01-19_SimpleSensitivity.html",
    "href": "notebooks/2024-01-19_SimpleSensitivity.html",
    "title": "Detection sensitivity to read count noise",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nfrom scipy.special import digamma"
  },
  {
    "objectID": "notebooks/2024-01-19_SimpleSensitivity.html#background",
    "href": "notebooks/2024-01-19_SimpleSensitivity.html#background",
    "title": "Detection sensitivity to read count noise",
    "section": "Background",
    "text": "Background\nSee all-hands memo."
  },
  {
    "objectID": "notebooks/2024-01-19_SimpleSensitivity.html#theory",
    "href": "notebooks/2024-01-19_SimpleSensitivity.html#theory",
    "title": "Detection sensitivity to read count noise",
    "section": "Theory",
    "text": "Theory\n\nRead count model\nPoisson counting noise mixed with a latent distribution. For viral read counts \\(C\\) and total per-sample read count \\(n\\):\n\\(C \\sim Poisson(n X)\\),\nwhere \\(X\\) follows a latent distribution (specified below). This latent distribution should increase in expectation with the prevalence and also capture non-Poisson noise.\nWe can show that the coefficient of variation obeys:\n\\(CV[C]^2 = \\frac{1}{n E[X]} + CV[X]\\).\nThat is, once we expect to see more than one read, the CV of the latent distribution will start to dominate the variation in counts.\n\n\nProperties of latent distributions\nWe need to specify and parameterize the latent distribution. Each distribution will have its own usual parameters, but we want to put them on common footing. This means specifying:\n\nA central value (mean, median, etc)\nA measure of spread (stdev, etc).\n\nFor spread, we will use the coefficient of variation. Caveat: it’s not clear that this should be constant as the the mean grows. Need to think about this mechanistically.\nFor central value, we’ll try specifying two different ways:\n\nArithmetic mean = prevalence X P2RA factor\nGeometric mean = prevalence X P2RA factor\n\nThe former is more natural for the gamma distribution, the latter for the lognormal, but we’ll try each both ways for comparison.\n\nGamma distribution\n\nMaxEnt distribution fixing \\(E[X]\\) and \\(E[\\log X]\\)\nUsually specified by shape parameter \\(k\\) and scale parameter \\(\\theta\\)\n\\(AM = E[X] = k\\theta\\)\n\\(GM = \\exp E[\\log X] = e^{\\psi(k)} \\theta\\), where \\(\\psi\\) is the digamma function.\n\\(Var[X] = k \\theta^2\\)\n\\(CV[X]^2 = 1/k\\)\n\\(AM / GM = k e^{-\\psi(k)} \\sim k e^{1/k}, k \\to 0\\). This is exponentially big in \\(CV^2\\).\nOn linear scale, density has an interior mode when \\(k &gt; 1\\), a mode at zero when \\(k = 1\\) and a power-law sigularity at zero when \\(k &lt; 1\\).\nOn a log scale, the density of \\(Y = \\log X\\) is: \\(f(y) \\propto \\exp[ky - e^y / \\theta]\\). Has a peak at \\(\\hat{y} = \\log \\theta k\\), slow decay to the left, fast decay to the right.\n\n\n\nLog-normal distribution\n\nMaxEnt distrib fixing geometric mean and variance\nSpecified by mean and variance of \\(\\log X\\)\n\\(AM = e^{\\mu + \\sigma^2 / 2}\\)\n\\(GM = e^{\\mu}\\)\n\\(Var[X] = [\\exp(\\sigma^2) - 1] \\exp(2 \\mu + \\sigma^2)\\)\n\\(CV[X]^2 = e^{\\sigma^2} - 1\\)\n\\(AM / GM = e^{\\sigma^2 / 2}\\), linear in CV for large CV.\n\nBoth distributions have \\(AM &gt; GM\\). But it grows much faster with CV for Gamma.\n\n# Test\nx = 1 * 2"
  },
  {
    "objectID": "notebooks/2024-01-19_SimpleSensitivity.html#parameters",
    "href": "notebooks/2024-01-19_SimpleSensitivity.html#parameters",
    "title": "Detection sensitivity to read count noise",
    "section": "Parameters",
    "text": "Parameters\n\nsampling_period = 7\ndaily_depth = 1e8\nsampling_depth = sampling_period * daily_depth\n\ndoubling_time = 7\ngrowth_rate = np.log(2) / doubling_time\n\n# Boston metro area\npopulation_size = 5e6\n\n# P2RA factor (roughly covid in Rothman)\n# normalize by the fact that we estimate per 1% incidence/prevalence there\np2ra = 1e-7 / 1e-2\n\n\nmax_prevalence = 0.2\nmax_time = np.ceil(np.log(population_size * max_prevalence) / growth_rate)\n\ntime = np.arange(0, int(max_time) + sampling_period, sampling_period)\nprevalence = np.exp(growth_rate * time) / population_size\n\nrng = np.random.default_rng(seed=10343)"
  },
  {
    "objectID": "notebooks/2024-01-19_SimpleSensitivity.html#simulation",
    "href": "notebooks/2024-01-19_SimpleSensitivity.html#simulation",
    "title": "Detection sensitivity to read count noise",
    "section": "Simulation",
    "text": "Simulation\n\nThreshold detection\n\ndef get_detection_times(time, counts, threshold: float):\n    indices = np.argmax(np.cumsum(counts, axis=1) &gt;= threshold, axis=1)\n    # FIXME: if never detected, indices will be 0, replace with -1 so that we get the largest time\n    # indices[indices == 0] = -1\n    return time[indices]\n\n\n\nParameterization\n\ndef params_lognormal(mean, cv, mean_type):\n    sigma_2 = np.log(1 + cv**2)\n    if mean_type == \"geom\":\n        mu = np.log(mean)\n    elif mean_type == \"arith\":\n        mu = np.log(mean) - sigma_2 / 2\n    else:\n        raise ValueError(\"mean_type must be geom|arith\")\n    return mu, np.sqrt(sigma_2)\n\n\ndef params_gamma(mean, cv, mean_type):\n    shape = cv ** (-2)\n    if mean_type == \"geom\":\n        scale = mean * np.exp(-digamma(shape))\n    elif mean_type == \"arith\":\n        scale = mean / shape\n    else:\n        raise ValueError(\"mean_type must be geom|arith\")\n    return shape, scale\n\n\n\nCount simulation\n\ndef simulate_latent(\n    mean,\n    cv: float,  # coefficient of variation\n    mean_type: str,  # geom | arith\n    distribution: str,  # gamma | lognormal\n    num_reps: int = 1,\n    rng: np.random.Generator = np.random.default_rng(),  # CHECK\n):\n    size = (num_reps, len(mean))\n    if distribution == \"gamma\":\n        shape, scale = params_gamma(mean, cv, mean_type)\n        return rng.gamma(shape, scale, size)\n    elif distribution == \"lognormal\":\n        mu, sigma = params_lognormal(mean, cv, mean_type)\n        return rng.lognormal(mu, sigma, size)\n    else:\n        raise ValueError(\"distribution must be gamma|lognormal\")\n\n\ndef simulate_counts(\n    prevalence,\n    p2ra: float,\n    sampling_depth: float,\n    cv: float,  # coefficient of variation\n    mean_type: str,  # geom | arith\n    latent_dist: str,  # gamma | lognormal\n    num_reps: int = 1,\n    rng: np.random.Generator = np.random.default_rng(),  # CHECK\n):\n    relative_abundance = p2ra * prevalence\n    lamb = simulate_latent(\n        relative_abundance, cv, mean_type, latent_dist, num_reps, rng\n    )\n    counts = rng.poisson(sampling_depth * lamb)\n    return counts\n\n\n\nTest latent params\n\nt = np.arange(100)\nmean = 0.01 * np.exp(t / 7)\ncvs = [0.5, 1.0, 2, 4]\nnum_reps = 1000\n\nfor cv in cvs:\n    latent = simulate_latent(mean, cv, \"arith\", \"gamma\", num_reps, rng)\n    plt.semilogy(t, np.mean(latent, axis=0))\n    plt.semilogy(t, np.std(latent, axis=0))\n    plt.semilogy(t, mean, \"--k\")\n    plt.semilogy(t, mean * cv, \":k\")\n    plt.title(f\"CV = {cv}\")\n    plt.show()\n\nfor cv in cvs:\n    latent = simulate_latent(mean, cv, \"arith\", \"lognormal\", num_reps, rng)\n    plt.semilogy(t, np.mean(latent, axis=0))\n    plt.semilogy(t, np.std(latent, axis=0))\n    plt.semilogy(t, mean, \"--k\")\n    plt.semilogy(t, mean * cv, \":k\")\n    plt.title(f\"CV = {cv}\")\n    plt.show()\n\nfor cv in cvs:\n    latent = simulate_latent(mean, cv, \"geom\", \"gamma\", num_reps, rng)\n    plt.semilogy(t, np.exp(np.mean(np.log(latent), axis=0)))\n    plt.semilogy(t, np.std(latent, axis=0))\n    plt.semilogy(t, np.mean(latent, axis=0))\n    print(np.std(latent, axis=0) / np.mean(latent, axis=0))\n    plt.semilogy(t, mean, \"--k\")\n    plt.title(f\"CV = {cv}\")\n    plt.show()\n\nfor cv in cvs:\n    latent = simulate_latent(mean, cv, \"geom\", \"lognormal\", num_reps, rng)\n    plt.semilogy(t, np.exp(np.mean(np.log(latent), axis=0)))\n    plt.semilogy(t, np.std(latent, axis=0))\n    plt.semilogy(t, np.mean(latent, axis=0))\n    plt.semilogy(t, mean, \"--k\")\n    plt.title(f\"CV = {cv}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[0.50348026 0.49182015 0.50282823 0.50870395 0.49265948 0.50533411\n 0.52211055 0.50050186 0.49085442 0.48295105 0.51659485 0.47924307\n 0.51916236 0.49989031 0.47911488 0.50655809 0.5040264  0.50973589\n 0.5259777  0.521355   0.51196868 0.50614298 0.5106999  0.52373657\n 0.47330863 0.50842704 0.51643281 0.47957447 0.50206644 0.50377273\n 0.49816772 0.51913592 0.52782476 0.49194066 0.51983427 0.52527224\n 0.49243682 0.49117164 0.5037379  0.48340348 0.4940295  0.49636163\n 0.52741949 0.49278562 0.50890312 0.51281443 0.47388754 0.51449679\n 0.49937058 0.48876435 0.50115377 0.52301337 0.50080361 0.50482404\n 0.50357979 0.50982194 0.48190781 0.49510878 0.48948755 0.48957791\n 0.49904929 0.49695378 0.4944266  0.49329813 0.51841087 0.49790695\n 0.49476528 0.48351475 0.51986067 0.48381848 0.49908738 0.49081166\n 0.50835463 0.49487533 0.49129641 0.51527339 0.50583592 0.49600538\n 0.49727427 0.50392921 0.49140025 0.50015994 0.49576992 0.50345842\n 0.49471129 0.51298593 0.47710101 0.47296923 0.4792705  0.51883331\n 0.49841553 0.51106549 0.51211019 0.48844829 0.50808292 0.51041238\n 0.49871134 0.49191981 0.47683314 0.51868499]\n[0.98393079 1.02228662 1.02012703 0.98510473 0.99054394 1.05067537\n 0.98531434 1.01228511 0.98334931 1.01326723 0.97723524 0.98954736\n 1.03536915 1.01491756 1.01645102 0.97640239 0.97622787 0.98144578\n 0.97274661 1.01520805 0.95254337 1.00135342 0.97954303 1.00187655\n 1.06584758 1.00567692 1.00412554 0.97124493 0.9312591  0.99823349\n 0.963092   1.00199325 1.03445693 1.06515596 1.06077553 0.97742492\n 1.08236563 0.90886172 0.99237003 1.01603279 1.03544314 0.94547342\n 0.99914824 1.02062342 0.99032014 0.97735808 1.00704571 0.99753384\n 1.04174574 0.99912939 0.97997322 0.98636165 0.98046552 0.96440814\n 0.98053455 1.00069747 1.02727122 0.95428542 0.97861231 0.96537031\n 0.99165826 0.98666603 0.98276165 0.95895163 0.97315295 0.96513772\n 1.01242235 1.02112884 1.0552492  0.97147578 0.98236027 1.01597814\n 1.00647088 1.00451165 1.01165352 1.02227556 0.98477906 1.08072714\n 0.99728964 1.01029694 1.00368864 0.98316007 0.97854387 0.97504436\n 0.97201678 1.05501862 1.03827394 1.00967839 1.02932978 1.00375372\n 1.02989618 0.94712783 0.97704349 1.03746789 1.00793446 0.99466767\n 0.97315889 1.03402584 0.99440691 0.99421547]\n[1.93348289 1.94075628 1.9371521  2.0596737  2.01922064 1.77050199\n 1.95359177 2.21491995 1.76238332 2.01896386 1.8359474  1.93522955\n 1.92304791 2.07436114 2.00112383 1.92611896 1.91318319 1.89199837\n 1.95239552 1.94197283 1.96052082 2.04660114 2.11607495 1.91440194\n 1.87350164 1.87829558 1.96142867 2.18442557 1.88364222 1.87840667\n 2.10924738 1.94424085 2.01537714 1.99424229 1.84297648 1.8983749\n 2.05230387 2.0670051  1.96440085 1.98658515 1.94209637 1.8520699\n 2.08009187 2.06838783 1.79127993 1.87616927 1.97531894 1.985033\n 2.11656599 1.89554444 1.95982964 2.12635557 2.03685668 1.90855538\n 2.019133   1.93651675 1.8725479  1.99982613 1.99215463 2.10543879\n 1.95080877 1.90585074 1.9536033  2.02331806 1.90023477 2.03508627\n 1.93722256 1.89147195 1.95247127 1.91292066 1.96191177 2.04529256\n 1.90620202 2.05602709 2.04635604 1.9964848  2.06337469 1.91445034\n 1.98958428 1.90751535 1.87740487 2.12006546 1.80767642 1.91483707\n 1.89387481 2.03026639 1.98127219 1.96318601 2.01879857 1.9100694\n 1.98198117 2.11591477 2.03354811 2.20515637 1.86657211 1.9549832\n 2.03214746 2.01828408 2.01163885 2.19198731]\n[3.85885115 4.15112238 3.92872672 3.60005621 4.11774644 4.04874281\n 3.76652958 3.56179868 3.75387821 4.72929878 4.51766138 4.09093613\n 3.4065016  4.29871855 3.66379723 4.05704088 3.50161145 4.0989186\n 3.93898714 3.76942373 3.82458744 4.49414312 4.08017117 3.79421135\n 4.19901864 3.55467022 3.71045934 3.91660997 4.27304547 4.20155975\n 3.78045465 4.09401223 4.13905594 3.78531078 3.90054913 4.04997723\n 3.7416698  3.61782184 4.01523185 3.61314766 4.57979094 3.80476479\n 4.2427299  4.53425588 3.90922339 4.62143615 3.48617567 3.87590464\n 3.84146648 3.7905067  3.57560193 3.93392831 3.31075887 4.12729374\n 4.29561327 4.20645896 3.6806276  4.23596369 4.04506462 4.67984352\n 4.23507313 4.34065718 4.28223705 3.67078099 4.04157157 4.68677653\n 4.2690662  3.57520629 4.05247374 4.02668955 4.41856066 3.62780225\n 3.64177031 4.27659845 3.80032842 3.78244861 3.56100671 4.10671816\n 3.55325258 4.55545487 4.29575367 4.3634054  3.74042498 3.99472887\n 3.3625012  3.86508634 3.74088776 4.77547092 3.77768133 3.95763258\n 3.27596278 4.1696738  4.1919582  3.87427013 3.6489727  4.12122796\n 3.91741195 3.4060893  3.70814923 4.08789586]"
  },
  {
    "objectID": "notebooks/2024-01-19_SimpleSensitivity.html#results",
    "href": "notebooks/2024-01-19_SimpleSensitivity.html#results",
    "title": "Detection sensitivity to read count noise",
    "section": "Results",
    "text": "Results\n\nCounts\n\nnum_reps = 1000\ncvs = [0.25, 0.5, 1.0, 2, 4]\n\ncounts_ga = [\n    simulate_counts(\n        prevalence,\n        p2ra,\n        sampling_depth,\n        cv,\n        mean_type=\"arith\",\n        latent_dist=\"gamma\",\n        num_reps=num_reps,\n        rng=rng,\n    )\n    for cv in cvs\n]\ncounts_la = [\n    simulate_counts(\n        prevalence,\n        p2ra,\n        sampling_depth,\n        cv,\n        mean_type=\"arith\",\n        latent_dist=\"lognormal\",\n        num_reps=num_reps,\n        rng=rng,\n    )\n    for cv in cvs\n]\ncounts_gg = [\n    simulate_counts(\n        prevalence,\n        p2ra,\n        sampling_depth,\n        cv,\n        mean_type=\"geom\",\n        latent_dist=\"gamma\",\n        num_reps=num_reps,\n        rng=rng,\n    )\n    for cv in cvs\n]\ncounts_lg = [\n    simulate_counts(\n        prevalence,\n        p2ra,\n        sampling_depth,\n        cv,\n        mean_type=\"geom\",\n        latent_dist=\"lognormal\",\n        num_reps=num_reps,\n        rng=rng,\n    )\n    for cv in cvs\n]\n\n\nArithmetic mean\n\nto_plot = 100\nplt.figure(figsize=(8, 4))\nfor i, cv in enumerate(cvs):\n    ax = plt.subplot(2, len(cvs), i + 1)\n    ax.semilogy(time, counts_ga[i][:to_plot].T, \".\", color=\"C0\", alpha=0.1)\n    ax.semilogy(time, sampling_depth * p2ra * prevalence, \"k--\")\n    ax.set_title(r\"$CV = $\" + f\"{cv}\")\n    ax.set_ylim([1, 1e5])\n    if i == 0:\n        ax.set_ylabel(\"Count\")\n    else:\n        ax.set_yticklabels([])\n    ax = plt.subplot(2, len(cvs), len(cvs) + i + 1)\n    ax.semilogy(time, counts_la[i][:to_plot].T, \".\", color=\"C0\", alpha=0.1)\n    ax.semilogy(time, sampling_depth * p2ra * prevalence, \"k--\")\n    ax.set_ylim([1, 1e5])\n    if i == 0:\n        ax.set_ylabel(\"Count\")\n    else:\n        ax.set_yticklabels([])\n    ax.set_xlabel(\"Day\")\nplt.show()\n\n\n\n\n\n\nGeometric mean\n\nplt.figure(figsize=(8, 4))\nfor i, cv in enumerate(cvs):\n    ax = plt.subplot(2, len(cvs), i + 1)\n    ax.semilogy(time, counts_gg[i][:to_plot].T, \".\", color=\"C0\", alpha=0.1)\n    ax.semilogy(time, sampling_depth * p2ra * prevalence, \"k--\")\n    ax.set_title(r\"$CV = $\" + f\"{cv}\")\n    ax.set_ylim([1, 1e5])\n    if i == 0:\n        ax.set_ylabel(\"Count\")\n    else:\n        ax.set_yticklabels([])\n    ax = plt.subplot(2, len(cvs), len(cvs) + i + 1)\n    ax.semilogy(time, counts_lg[i][:to_plot].T, \".\", color=\"C0\", alpha=0.1)\n    ax.semilogy(time, sampling_depth * p2ra * prevalence, \"k--\")\n    ax.set_ylim([1, 1e5])\n    if i == 0:\n        ax.set_ylabel(\"Count\")\n    else:\n        ax.set_yticklabels([])\n    ax.set_xlabel(\"Day\")\nplt.show()\n\n\n\n\n\n\n\nCumulative counts\n\nplt.figure(figsize=(8, 8))\nfor i, cv in enumerate(cvs):\n    ax = plt.subplot(4, len(cvs), i + 1)\n    ax.semilogy(\n        time, np.cumsum(counts_ga[i][:to_plot], axis=1).T, \"-\", color=\"C0\", alpha=0.1\n    )\n    ax.semilogy(time, np.cumsum(sampling_depth * p2ra * prevalence), \"k--\")\n    ax.set_title(r\"$CV = $\" + f\"{cv}\")\n    ax.set_ylim([1, 1e5])\n    if i == 0:\n        ax.set_ylabel(\"Cumulative count\")\n        ax.text(0, 1e4, \"Arithmetic\\nGamma\")\n    else:\n        ax.set_yticklabels([])\n    ax = plt.subplot(4, len(cvs), len(cvs) + i + 1)\n    ax.semilogy(\n        time, np.cumsum(counts_la[i][:to_plot], axis=1).T, \"-\", color=\"C1\", alpha=0.1\n    )\n    ax.semilogy(time, np.cumsum(sampling_depth * p2ra * prevalence), \"k--\")\n    ax.set_ylim([1, 1e5])\n    if i == 0:\n        ax.set_ylabel(\"Cumulative count\")\n        ax.text(0, 1e4, \"Arithmetic\\nLognormal\")\n    else:\n        ax.set_yticklabels([])\n\n    ax = plt.subplot(4, len(cvs), 2 * len(cvs) + i + 1)\n    ax.semilogy(\n        time, np.cumsum(counts_gg[i][:to_plot], axis=1).T, \"-\", color=\"C0\", alpha=0.1\n    )\n    ax.semilogy(time, np.cumsum(sampling_depth * p2ra * prevalence), \"k--\")\n    ax.set_ylim([1, 1e5])\n    if i == 0:\n        ax.set_ylabel(\"Cumulative count\")\n        ax.text(0, 1e4, \"Geometric\\nGamma\")\n    else:\n        ax.set_yticklabels([])\n    ax = plt.subplot(4, len(cvs), 3 * len(cvs) + i + 1)\n    ax.semilogy(\n        time, np.cumsum(counts_lg[i][:to_plot], axis=1).T, \"-\", color=\"C1\", alpha=0.1\n    )\n    ax.semilogy(time, np.cumsum(sampling_depth * p2ra * prevalence), \"k--\")\n    ax.set_ylim([1, 1e5])\n    if i == 0:\n        ax.set_ylabel(\"Cumulative count\")\n        ax.text(0, 1e4, \"Geometric\\nLognormal\")\n    else:\n        ax.set_yticklabels([])\n    ax.set_xlabel(\"Day\")\nplt.show()\n\n\n\n\n\n\nDetection times\n\nthresholds = [2, 100]\ndetection_times_ga = [\n    [get_detection_times(time, counts, threshold) for counts in counts_ga]\n    for threshold in thresholds\n]\ndetection_times_la = [\n    [get_detection_times(time, counts, threshold) for counts in counts_la]\n    for threshold in thresholds\n]\ndetection_times_gg = [\n    [get_detection_times(time, counts, threshold) for counts in counts_gg]\n    for threshold in thresholds\n]\ndetection_times_lg = [\n    [get_detection_times(time, counts, threshold) for counts in counts_lg]\n    for threshold in thresholds\n]\n\n\nq = 0.9\n\nax = plt.subplot(111)\n\nplt.semilogx(\n    cvs, [np.quantile(dt, q) for dt in detection_times_ga[0]], \"o-\", color=\"C0\"\n)\nplt.semilogx(\n    cvs, [np.quantile(dt, q) for dt in detection_times_la[0]], \"o-\", color=\"C1\"\n)\nplt.semilogx(\n    cvs, [np.quantile(dt, q) for dt in detection_times_gg[0]], \"o:\", color=\"C0\"\n)\nplt.semilogx(\n    cvs, [np.quantile(dt, q) for dt in detection_times_lg[0]], \"o:\", color=\"C1\"\n)\n\nplt.semilogx(\n    cvs,\n    [np.quantile(dt, q) for dt in detection_times_ga[1]],\n    \"s-\",\n    color=\"C0\",\n    label=\"Gamma-Arith\",\n)\nplt.semilogx(\n    cvs,\n    [np.quantile(dt, q) for dt in detection_times_la[1]],\n    \"s-\",\n    color=\"C1\",\n    label=\"LN-Arith\",\n)\nplt.semilogx(\n    cvs,\n    [np.quantile(dt, q) for dt in detection_times_gg[1]],\n    \"s:\",\n    color=\"C0\",\n    label=\"Gamma-Geom\",\n)\nplt.semilogx(\n    cvs,\n    [np.quantile(dt, q) for dt in detection_times_lg[1]],\n    \"s:\",\n    color=\"C1\",\n    label=\"LN-Geom\",\n)\n\nax.set_ylabel(\"Detection day (90th percentile)\")\nax.set_xscale(\"log\", base=2)\nax.set_xlabel(\"Coefficient of variation\")\nplt.legend(\n    handles=[\n        mlines.Line2D([], [], color=\"C0\", marker=\"o\", label=\"Gamma\"),\n        mlines.Line2D([], [], color=\"C1\", marker=\"o\", label=\"Lognormal\"),\n        mlines.Line2D([], [], color=\"0.5\", linestyle=\"-\", label=\"Arithmetic mean\"),\n        mlines.Line2D([], [], color=\"0.5\", linestyle=\":\", label=\"Geometric mean\"),\n        mlines.Line2D([], [], color=\"0.5\", marker=\"s\", label=\"Threshold = 100\"),\n        mlines.Line2D([], [], color=\"0.5\", marker=\"o\", label=\"Threshold = 2\"),\n    ]\n)\n\n&lt;matplotlib.legend.Legend at 0x16ccc1690&gt;"
  },
  {
    "objectID": "CombiningData-2023-07-27.html",
    "href": "CombiningData-2023-07-27.html",
    "title": "The part we’re using",
    "section": "",
    "text": "def posterior_seq_only(p_eg, p_neg, p_eng, p_neng, p_s_e, p_s_ne):\n    return (p_s_e * p_eg) / (\n        p_s_e * p_eg + \n        p_s_ne * p_neg +\n        p_s_e * p_eng +\n        p_s_ne * p_neng\n    )\n\ndef posterior_growth_only(p_eg, p_neg, p_eng, p_neng, p_c_g, p_c_ng):\n    return (p_c_g * p_eg) / (\n        p_c_g * p_eg + \n        p_c_g * p_neg +\n        p_c_ng * p_eng +\n        p_c_ng * p_neng\n    )\n\ndef posterior_combined(p_eg, p_neg, p_eng, p_neng, p_s_e, p_s_ne, p_c_g, p_c_ng):\n    return (p_s_e * p_c_g * p_eg) / (\n        p_s_e * p_c_g * p_eg + \n        p_s_ne * p_c_g * p_neg +\n        p_s_e * p_c_ng * p_eng +\n        p_s_ne * p_c_ng * p_neng\n    )\n\nE = engineered NE = not engineered G = growing NG = not growing\nData: S = sequence is flagged C = count data is flagged\nWant posterior Pr{E, G| S} or Pr{E, G | S, C}.\nLikelihood Pr{S, C | E, G} = Pr{S | E, G} * Pr{C | E, G} = Pr{S | E} * Pr{C | G}\nLikelihood ratios: Pr{S | E} / Pr{S | NE} Pr{C | G} / Pr{C | NG}\nPrior\n\nsequences = 1e5\nthreats = 0.1\ngrowing = 1e4\n\np_eg = threats / sequences\n# Assumes anything engineered is growing\np_eng = 0.0\np_neg = (growing - threats) / sequences\np_neng = 1 - p_eg - p_eng - p_neg\n\nLikelihood\n\np_s_e = 0.9\np_s_ne = 0.2\nprint(p_s_e / p_s_ne)\np_c_g = 0.90\np_c_ng = 0.10\nprint(p_c_g / p_c_ng)\np_seq_only = posterior_seq_only(p_eg, p_neg, p_eng, p_neng, p_s_e, p_s_ne)\np_growth_only = posterior_growth_only(p_eg, p_neg, p_eng, p_neng, p_c_g, p_c_ng)\np_combined = posterior_combined(p_eg, p_neg, p_eng, p_neng, p_s_e, p_s_ne, p_c_g, p_c_ng)\nprint(p_seq_only)\nprint(p_growth_only)\nprint(p_combined)\n\n4.5\n9.0\n4.499984250055124e-06\n4.9999999999999996e-06\n2.24996062568905e-05\n\n\n\np_s_e = 0.9\np_s_ne = 0.05\nposterior_seq_only(p_eg, p_neg, p_eng, p_neng, p_s_e, p_s_ne)\n\n1.7999694005201907e-05\n\n\nBase rates (assume the threat is growing):\n\nsequences = 1e6\nthreats = 1\ngrowing = 1e4\n\nLikelihoods of the Sequence-based test:\n\n# Pr{S|E}\np_s_e = 0.99\n# Pr{S|NE}\np_s_ne = 0.1\n\nFalse and true positives of Sequence-based test alone:\n\n# Number flagged not threats\nprint(p_s_ne * (sequences - threats))\n# Number flagged that are threats\nprint(p_s_e * (threats))\n\n99999.90000000001\n0.99\n\n\nLikelihoods of the Count-based test:\n\n# Pr{C|G}\np_c_g = 0.95\n# Pr{C|NG}\np_c_ng = 0.05\n\n\n# Number flagged not threats\nprint(p_c_g * (growing - threats))\nprint(p_c_ng * (sequences - growing))\n# Number flagged that are threats\nprint(p_c_g * threats)\n\n9499.05\n49500.0\n0.95\n\n\nCombined test (assumes flagging by the two tests is independent conditional on (E, G))\n\n# Number flagged not threats\nprint(p_s_ne * p_c_g * (growing - threats) + p_s_ne * p_c_ng * (sequences - growing))\n# Number flagged threats\nprint(p_s_e * p_c_g * threats)\n\n5899.905000000001\n0.9405"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dan’s NAO Notebook",
    "section": "",
    "text": "NAO Cost Estimate MVP – Adding noise\n\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\nNAO Cost Estimate MVP – Optimizing the sampling interval\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\nNAO Cost Estimate MVP\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\nDetection sensitivity to read count noise\n\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-10-24 Analysis of sludge dissociation test qPCR\n\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-10-16 Analysis of recent protocol-comparison experiments\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-10-11 Analyze qPCR standard curves\n\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-10-10 Analyze prefilter experiment qPCR\n\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-10-05 Qubit data exploration\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-09-28 qPCR Quality Control\n\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-09-13 PMMoV qPCR Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-18 Extraction Experiment 1 qPCR Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\nExamine extraction-kit comparison experiment\n\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-09-13 Extraction Experiment 2 qPCR Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\nAirport experiment sequencing cost estimate\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\n2023-08-29 qPCR Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nDan Rice\n\n\n\n\n\n\n  \n\n\n\n\nSimple deterministic model of local and global prevalence\n\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nDan Rice\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "decision-theory/numerical_fredholm_integral.html",
    "href": "decision-theory/numerical_fredholm_integral.html",
    "title": "Introduction",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial.laguerre import laggauss\nfrom numpy import linalg\nfrom scipy.stats import norm\nWe want to solve a Fredholm integral equation of the second kind, of the form:\n\\(u(x) = \\int_{0}^{\\infty} K(x, t) u(t) dt + f(x)\\)\nWe’ll approach it by using Gauss-Laguerre quadrature to approximate the integral (note that we have to insert \\(\\exp(t)\\) to cancel the implicit \\(\\exp(-t)\\) weighting function of the G-L quadrature.):\n\\(u(x) \\approx \\sum_{j=1}^{n} w_j \\exp(t_j) K(x, t_j) u(t_j) + f(x)\\)\nIf we evaluate \\(u\\) at the quadrature points (\\(u_i = u(x_i)\\)), we get a linear system:\n\\(u_i \\approx \\sum_{j=1}^{n} w_j \\exp(x_j) K(x_i, x_j) u_j + f(x_i)\\)\nIn matrix form, we have:\n\\(A \\vec{u} = \\vec{b}\\)\n\\(A_{i,j} = \\delta_{i,j} - w_j exp(x_j) K(x_i, x_j)\\)\n\\(b_i = f(x_i)\\)\nWe can solve this for \\(\\vec{u}\\). Then to get the continuous function \\(u(x)\\), we can substitute \\(\\vec{u}\\) into the right-hand side of the approximation above:\n\\(u(x) \\approx \\sum_{j=1}^{n} w_j \\exp(t_j) K(x, t_j) u_j + f(x)\\)\ndef solve_fredholm(k, f, n):\n    x, w = laggauss(n)\n    A = np.eye(n) - w * np.exp(x) * k(x[:,None], x)\n    b = f(x)\n    u = linalg.solve(A, b)\n    def soln(y):\n        return np.dot(w * np.exp(x) * k(y[:,None], x), u) + f(y)\n    return soln\ndef k(mu, sigma):\n    return lambda x, t: norm(loc=mu, scale=sigma).pdf(t - x)\n\ndef f_nt(mu, sigma):\n    return norm(loc=-mu, scale=sigma).sf\n\ndef f_t():\n    return np.ones_like\nAs we increase the number of nodes, we approach the asymptotic behavior (in \\(x\\)) that we expect from theory.\nmu = 1\nsigma = 1\n\nxmax = 5\nx = np.arange(0, xmax, 0.01)\n\nfor n in [5, 10, 20]:\n    plt.plot(x, solve_fredholm(k(-mu, sigma), f_t(), n)(x), label = f\"n = {n}\")\nplt.plot(x, x / mu + 1, 'k--', label=\"asymptotic theory\")\nplt.legend(frameon=False)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$u_T(x)$\")\nplt.show()\n\nfor n in [5, 10, 20]:\n    plt.semilogy(x, solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)(x), label = f\"n = {n}\")\nplt.plot(x, norm.cdf(-mu/sigma) * np.exp(-2 * mu * x / sigma**2), 'k--', label=\"asymptotic theory\")\nplt.legend(frameon=False)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$u_{NT}(x)$\")\nplt.show()"
  },
  {
    "objectID": "decision-theory/numerical_fredholm_integral.html#scaling-diagrams",
    "href": "decision-theory/numerical_fredholm_integral.html#scaling-diagrams",
    "title": "Introduction",
    "section": "Scaling diagrams",
    "text": "Scaling diagrams\n\nn = 185\nsigma = 1\n\n\nThreat, \\(|\\mu / \\sigma| \\ge 1\\)\n\nxmax = 32\nx = np.arange(0, xmax, 0.01)\n\nfor mu in [-1, -2, -4, -8]:\n    soln = solve_fredholm(k(mu, sigma), f_t(), n)\n    plt.plot(x / (-mu), soln(x), label=mu)\nplt.plot(x, x + 0.5, 'k')\nplt.xlim([0, 4])\nplt.ylim([0, 5])\nplt.xlabel(r\"$x / (-\\mu)$\")\nplt.ylabel(r\"$u(x)$\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x126fa1f90&gt;\n\n\n\n\n\n\nxmax = 16\nx = np.arange(0, xmax, 0.01)\n\nplt.figure(figsize=(3,2))\nfor mu in [-4, -8]:\n    soln = solve_fredholm(k(mu, sigma), f_t(), n)\n    plt.plot(x, soln(x), label=r\"$\\tau = $\" + f\"{-2 * mu}\")\nplt.xlim([0, xmax])\nplt.ylim([0, 5])\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$u_T(x)$\")\nplt.legend(frameon=False)\nplt.savefig(\"large_updates.png\", bbox_inches=\"tight\")\n\n\n\n\n\n\nNon-threat, \\(|\\mu / \\sigma| \\ge 1\\)\nOne term from the Liouville-Neumann series:\n\nfor mu in [1, 2, 3, 4]:\n    xmax = 2*mu\n    x = np.arange(0, xmax, 0.01)\n\n    soln = solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)\n    plt.semilogy(x / mu, soln(x), label=mu)\n    plt.semilogy(x / mu, norm.cdf(-(x+mu)/sigma))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple terms from the Liouville-Neumann series:\n\nfor mu in [1, 2, 3, 4]:\n    xmax = 2*mu\n    x = np.arange(0, xmax, 0.01)\n\n    soln = solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)\n    approx = np.zeros_like(x)\n    for j in range(1, 4):\n        approx += norm.cdf(-(x + j * mu)/(np.sqrt(j) * sigma))\n    plt.semilogy(x / mu, soln(x), label=mu)\n    plt.semilogy(x / mu, approx)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-threat, \\(|\\mu / \\sigma| \\leq 1\\)\n\nxmax = 32\nx = np.arange(0, xmax, 0.01)\n\nfor mu in [1/8, 1/4, 1/2, 1]:\n    soln = solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)\n    plt.semilogy(2 * mu * x / sigma**2, soln(x), label=mu)\nplt.semilogy(x, np.exp(- x), 'k')\nplt.xlim([0, 8])\nplt.ylim([1e-4,1])\nplt.xlabel(r\"$2 \\mu x / \\sigma^2$\")\nplt.ylabel(r\"$u(x)$\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x126fbac10&gt;\n\n\n\n\n\nTesting the hypothesis that \\(u_{NT}(0) \\sim \\frac{1}{1 + \\sqrt{\\tau/2}}\\) for \\(\\tau \\to 0\\).\n\nplt.subplot(211)\nfor mu in np.logspace(-1.5, 0, 20):\n    soln = solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)\n    plt.loglog(mu, 1 - soln(np.zeros(1)), '.k')\n    plt.loglog(mu, 1 - 1 / (1 + np.sqrt(2)*mu / sigma), 'xk')\nplt.ylabel(r\"$1 - u_{NT}(0)$\")\n\nplt.subplot(212)\nfor mu in np.logspace(-1.5, 0, 20):\n    soln = solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)\n    plt.semilogx(mu, soln(np.zeros(1)), '.k')\n    plt.semilogx(mu, 1 / (1 + np.sqrt(2)*mu / sigma), 'xk')\n\nplt.xlabel(r\"$\\mu / \\sigma = \\sqrt{\\tau}/2$\")\nplt.ylabel(r\"$u_{NT}(0)$\")\nplt.ylim([0,1])\n\n(0.0, 1.0)\n\n\n\n\n\n\n\nThreat, \\(|\\mu / \\sigma| \\le 1\\)\n\nxmax = 32\nx = np.arange(0, xmax, 0.01)\n\nfor mu in [-1/16, -1/8, -1/4, -1/2, -1]:\n    soln = solve_fredholm(k(mu, sigma), f_t(), n)\n    plt.plot(x / sigma, soln(x) * (-mu / sigma), label=r\"$\\tau = $\" + f\"{-2*mu}\")\nplt.plot(x, x + 0.5, 'k')\nplt.xlim([0, 4])\nplt.ylim([0, 5])\nplt.xlabel(r\"$x / \\sqrt{\\tau}$\")\nplt.ylabel(r\"$\\frac{\\sqrt{\\tau}}{2} u(x)$\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x130102910&gt;\n\n\n\n\n\n\nxmax = 32\nx = np.arange(0, xmax, 0.01)\n\nfig = plt.figure(figsize=(5.5,2.5))\n\nax = plt.subplot(122)\nfor mu in [-1/8, -1/4, -1/2, -1]:\n    soln = solve_fredholm(k(mu, sigma), f_t(), n)\n    plt.plot(x / sigma, soln(x) * (-mu / sigma), label=f\"{-2*mu}\")\nplt.plot(x, x + 0.5, 'k')\nplt.xlim([0, 4])\nplt.ylim([0, 5])\nplt.xlabel(r\"$x / \\sqrt{\\tau}$\")\nplt.ylabel(r\"$\\frac{\\sqrt{\\tau}}{2} u_{T}(x)$\")\nplt.legend(frameon=False, title=r\"$\\tau$\")\n\n\nxmax = 32\nx = np.arange(0, xmax, 0.01)\n\nax = plt.subplot(121)\nfor mu in [1/8, 1/4, 1/2, 1]:\n    soln = solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)\n    plt.semilogy(2 * mu * x / sigma**2, soln(x), label=mu)\nplt.semilogy(x, np.exp(- x), 'k')\nplt.xlim([0, 8])\nplt.ylim([1e-4,1])\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$u_{NT}(x)$\")\n\nfig.tight_layout()\nplt.savefig(\"small_updates.png\", bbox_inches=\"tight\")\n# plt.legend()"
  },
  {
    "objectID": "decision-theory/numerical_fredholm_integral.html#scratch",
    "href": "decision-theory/numerical_fredholm_integral.html#scratch",
    "title": "Introduction",
    "section": "Scratch",
    "text": "Scratch\nAttempts at higher-order approximations, fitting the constants, etc.\n\ndef theory_nt(x, mu, sigma):\n    return (norm.cdf(-mu/sigma) / norm.cdf(mu/sigma)) * np.exp(-2 * mu * x / sigma**2)\n\ndef theory_t(x, mu, sigma):\n    a = 1 + np.abs(sigma/mu) * norm.pdf(np.abs(mu/sigma)) / norm.cdf(np.abs(mu/sigma))\n    return - (1/mu) * x + a\n\n\nmu = -2\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 6\nx = np.arange(0, xmax, 0.1)\n\nplt.plot(x, soln(x))\nplt.plot(x, theory_t(x, mu, sigma))\n# a = 1 + np.abs(sigma/mu) * norm.pdf(np.abs(mu/sigma)) / norm.cdf(np.abs(mu/sigma))\na = 1/2\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.ylim([0,1 - xmax/mu])\n\n(0.0, 4.0)\n\n\n\n\n\n\ndef a_opt(x):\n    return 1/2 + 1/(2*x) - (1/x)*norm.pdf(1/2 + x/2) / norm.cdf(-(1/2 + x/2))\n\n\nmu = -1/2\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 8\nx = np.arange(0, xmax, 0.1)\n\na = 1/4\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = a_opt(mu/sigma)\nprint(a)\nprint(-sigma / mu * (norm.pdf(1/2) / norm.cdf(-1/2) - 1/2))\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 1\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\n\n\n\n\n1.4271079588328082\n1.2821555407361296\n\n\n\n\n\n\n\n\n\nmu = -8\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 32\nx = np.arange(0, xmax, 0.1)\n\na = 1/16\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.vlines((-mu) / 2, 0, 1 - xmax/mu, 'k', linestyle='dashed')\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 1/2\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.vlines((-mu) / 2, 0, 1 - xmax/mu, 'k', linestyle='dashed')\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 2\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.vlines((-mu) / 2, 0, 1 - xmax/mu, 'k', linestyle='dashed')\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nmu = -4\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 12\nx = np.arange(0, xmax, 0.1)\n\na = 1/16\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.vlines((-mu) / 2, 0, 1 - xmax/mu, 'k', linestyle='dashed')\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 1/2\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.vlines((-mu) / 2, 0, 1 - xmax/mu, 'k', linestyle='dashed')\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 2\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma), ':', lw=4)\nplt.vlines((-mu) / 2, 0, 1 - xmax/mu, 'k', linestyle='dashed')\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nmu = -1/4\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 4\nx = np.arange(0, xmax, 0.01)\n\na = 1/2\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 0.641 * sigma / (-mu)\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 5\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nmu = -1\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 4\nx = np.arange(0, xmax, 0.01)\n\na = 1/2\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 0.9\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\na = 2\nplt.plot(x, soln(x))\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nmu = -4\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 12\nx = np.arange(0, xmax, 0.1)\n\nplt.plot(x, soln(x))\na = 1/2\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\n\n(0.0, 4.0)\n\n\n\n\n\n\nxmax = 32\nx = np.arange(0, xmax, 0.01)\n\nfor mu in [1/8, 1/4, 1/2, 1]:\n    a = norm.cdf(-mu / sigma)\n    soln = solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)\n    plt.semilogy(x, soln(x), label=mu)\n    plt.semilogy(x, a * np.exp(-2 * mu * x / sigma**2))\n    plt.semilogy(x, a * norm.sf(-(x+mu)/sigma) * np.exp(-2 * mu * x / sigma**2) + norm.cdf(-(x+mu)/sigma))\n    # plt.semilogy(x, np.exp(-2 * x), 'k')\n    # plt.xlim([0,4])\n    # plt.ylim([3e-3,1.1])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu = -1/8\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 4\nx = np.arange(0, xmax, 0.1)\n\nplt.plot(x, soln(x))\na = 2 * np.sqrt(2*np.pi)\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\n\n(0.0, 33.0)\n\n\n\n\n\n\nnp.sqrt(2*np.pi)\n\n2.5066282746310002\n\n\n\nmu = -1/4\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 4\nx = np.arange(0, xmax, 0.1)\n\nplt.plot(x, soln(x))\na = np.sqrt(2*np.pi)\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\n\n(0.0, 17.0)\n\n\n\n\n\n\nmu = -1/2\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 4\nx = np.arange(0, xmax, 0.1)\n\nplt.plot(x, soln(x))\na = np.sqrt(2*np.pi) / 2\nplt.plot(x, -(x/mu) + a)\nplt.plot(x, 1 - (sigma/mu)*norm.pdf((x+mu)/sigma) + (- (x/mu) + a - 1)*norm.cdf((x+mu)/sigma))\nplt.ylim([0,1 - xmax/mu])\n\n(0.0, 9.0)\n\n\n\n\n\n\nplt.loglog([1/2, 1/4, 1/8], [1, 2, 4], '.')\n\n\n\n\n\nmu = -1/4\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_t(), n)\n\nxmax = 16\nx = np.arange(0, xmax, 0.1)\n\nplt.plot(x, soln(x))\nplt.plot(x, theory_t(x, mu, sigma))\nplt.plot(x[x&lt;=-mu], 1 + norm.cdf((x[x&lt;=-mu]+mu)/sigma))\nplt.plot(x[x&gt;=-mu], -x[x&gt;=-mu]/mu + 1/2)\nplt.ylim([0,1 - xmax/mu])\n\n(0.0, 65.0)\n\n\n\n\n\n\nsigma = 1\nn = 185\nxmax = 4\nx = np.arange(0, xmax, 0.1)\n\nfor mu in [-1/2, -1/3, -1/4, -1/5]:\n    soln = solve_fredholm(k(mu, sigma), f_t(), n)\n    mod = soln(x) * -mu - x\n    plt.plot(x, mod - mod[-1])\n    # plt.plot(x, theory_t(x, mu, sigma) * -mu)\n\n\n\n\n\nmu = 2\nsigma = 1\nn = 185\n\nsoln = solve_fredholm(k(mu, sigma), f_nt(mu, sigma), n)\n\nxmax = 10\nx = np.arange(0, xmax, 0.1)\n\nplt.semilogy(x, soln(x))\n# plt.plot(x, 1 + norm.cdf((x+mu)/sigma))\n# plt.plot(x, 1 - x/mu)\nplt.semilogy(x, theory_nt(x, mu, sigma))\nplt.semilogy(x[x&lt;=mu], norm.cdf(-(x[x&lt;=mu]+mu)/sigma))\nplt.semilogy(x[x&gt;mu], np.exp(2*(mu/sigma)**2) * norm.cdf(-2*mu/sigma) * np.exp(-2*mu * x[x&gt;mu] / sigma**2))\n# plt.plot(x, -x/mu + norm.cdf(0))\n# plt.ylim([0,1 - xmax/mu])"
  },
  {
    "objectID": "notebooks/2023-08-29-qpcr_analysis.html",
    "href": "notebooks/2023-08-29-qpcr_analysis.html",
    "title": "2023-08-29 qPCR Analysis",
    "section": "",
    "text": "Compare several wastewater filtering options by measuring the nucleic acid content by qPCR. Experimental design here.\nGet Dan up to speed with working with this data.\nExplore options for data analysis workflows."
  },
  {
    "objectID": "notebooks/2023-08-29-qpcr_analysis.html#objectives",
    "href": "notebooks/2023-08-29-qpcr_analysis.html#objectives",
    "title": "2023-08-29 qPCR Analysis",
    "section": "",
    "text": "Compare several wastewater filtering options by measuring the nucleic acid content by qPCR. Experimental design here.\nGet Dan up to speed with working with this data.\nExplore options for data analysis workflows."
  },
  {
    "objectID": "notebooks/2023-08-29-qpcr_analysis.html#preliminary-work",
    "href": "notebooks/2023-08-29-qpcr_analysis.html#preliminary-work",
    "title": "2023-08-29 qPCR Analysis",
    "section": "Preliminary work",
    "text": "Preliminary work\n\nAri put the .eds files into Google Drive from the lab computer. (He also exported some Excel files but we’re not using those.)\nDan installed the Google Drive desktop app and Design and Analysis on his Mac, opened the .eds files, and fixed some missing data in the plate layout\nDan used the “Analyze” in Design and Analysis to automatically calculate thresholds and compute c_q values.\nDan exported the data to .csv.\nSymlinked the google drive folder for the airport experiment to ~/airport/ on his computer so I don’t have to refer to the whole filepath here."
  },
  {
    "objectID": "notebooks/2023-08-29-qpcr_analysis.html#data-import",
    "href": "notebooks/2023-08-29-qpcr_analysis.html#data-import",
    "title": "2023-08-29 qPCR Analysis",
    "section": "Data import",
    "text": "Data import\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\n\ndata_dir &lt;- \"~/airport/[2023-08-29] CP Prefilters vs Vacuum Filters/Test 1 qPCR Results/csv/\"\nfilename_pattern &lt;- \"Results\"\n\n\nraw_data &lt;- list.files(\n  data_dir,\n  pattern = filename_pattern,\n  full.names = TRUE\n) |&gt;\n  map(function(f) read_csv(f, skip = 23)) |&gt;\n  list_rbind()\n\nRows: 36 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Well Position, Sample, Target, Task, Reporter, Quencher, Amp Status...\ndbl (8): Well, Amp Score, Cq Confidence, Cq Mean, Cq SD, Threshold, Baseline...\nlgl (5): Omit, Curve Quality, Result Quality Issues, Auto Threshold, Auto Ba...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 29 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Well Position, Sample, Target, Task, Reporter, Quencher, Amp Status...\ndbl (8): Well, Amp Score, Cq Confidence, Cq Mean, Cq SD, Threshold, Baseline...\nlgl (5): Omit, Curve Quality, Result Quality Issues, Auto Threshold, Auto Ba...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprint(raw_data)\n\n# A tibble: 65 × 21\n    Well `Well Position` Omit  Sample Target Task    Reporter Quencher\n   &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n 1     1 A1              FALSE 1      CrA    UNKNOWN FAM      NFQ-MGB \n 2     2 A2              FALSE 1      CrA    UNKNOWN FAM      NFQ-MGB \n 3     3 A3              FALSE 1      CrA    UNKNOWN FAM      NFQ-MGB \n 4     5 A5              FALSE 1      16S    UNKNOWN FAM      NFQ-MGB \n 5     6 A6              FALSE 1      16S    UNKNOWN FAM      NFQ-MGB \n 6     7 A7              FALSE 1      16S    UNKNOWN FAM      NFQ-MGB \n 7    13 B1              FALSE 2      CrA    UNKNOWN FAM      NFQ-MGB \n 8    14 B2              FALSE 2      CrA    UNKNOWN FAM      NFQ-MGB \n 9    15 B3              FALSE 2      CrA    UNKNOWN FAM      NFQ-MGB \n10    17 B5              FALSE 2      16S    UNKNOWN FAM      NFQ-MGB \n# ℹ 55 more rows\n# ℹ 13 more variables: `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;chr&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;\n\n\n\ncoding &lt;- list(\n  \"1\" = \"Normal centrifugation/filtration, regular CP\",\n  \"2\" = \"Normal centrifugation/filtration, regular CP\",\n  \"3\" = \"Normal centrifugation, no filtration, prefilter CP\",\n  \"4\" = \"Normal centrifugation, no filtration, prefilter CP\",\n  \"5\" = \"No centrifugation/filtration, prefilter CP\",\n  \"6\" = \"No centrifugation/filtration, prefilter CP\"\n)\ntidy_data &lt;- raw_data |&gt;\n  mutate(\n    Cq = as.double(Cq),\n    Treatment = recode(Sample, !!!coding)\n  )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Cq = as.double(Cq)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n\nlibrary(ggplot2)\ntidy_data |&gt;\n  ggplot(mapping = aes(x = Cq, y = Treatment, color = Sample)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(facets = ~Target)\n\nWarning: Removed 11 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "notebooks/2023-08-29-qpcr_analysis.html#todo",
    "href": "notebooks/2023-08-29-qpcr_analysis.html#todo",
    "title": "2023-08-29 qPCR Analysis",
    "section": "TODO",
    "text": "TODO\n\nFigure out what the Sample numbers mean with respect to the different treatments\nThere are a few NaNs that show up as missing points and aren’t evident in the plot. Will investigate later."
  },
  {
    "objectID": "notebooks/2023-10-16-combined_protocol_experiments.html",
    "href": "notebooks/2023-10-16-combined_protocol_experiments.html",
    "title": "2023-10-16 Analysis of recent protocol-comparison experiments",
    "section": "",
    "text": "See Twist"
  },
  {
    "objectID": "notebooks/2023-10-16-combined_protocol_experiments.html#objectives",
    "href": "notebooks/2023-10-16-combined_protocol_experiments.html#objectives",
    "title": "2023-10-16 Analysis of recent protocol-comparison experiments",
    "section": "",
    "text": "See Twist"
  },
  {
    "objectID": "notebooks/2023-10-16-combined_protocol_experiments.html#preliminary-work",
    "href": "notebooks/2023-10-16-combined_protocol_experiments.html#preliminary-work",
    "title": "2023-10-16 Analysis of recent protocol-comparison experiments",
    "section": "Preliminary work",
    "text": "Preliminary work\nExported csv files from Olivia’s eds file uploads. Also exported metadata google sheets as CSV"
  },
  {
    "objectID": "notebooks/2023-10-16-combined_protocol_experiments.html#data-import",
    "href": "notebooks/2023-10-16-combined_protocol_experiments.html#data-import",
    "title": "2023-10-16 Analysis of recent protocol-comparison experiments",
    "section": "Data import",
    "text": "Data import\n\nlibrary(here)\n\nhere() starts at /Users/dan/notebook\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\n\n\nget_plate &lt;- function(f) {\n  str_extract(basename(f),\n    \"(.*)_(.*)_[0-9]{8}_[0-9]{6}\\\\.csv\",\n    group = 1\n  )\n}\n\n\ndata_dir &lt;- here(\"~\", \"airport\")\nexperiments &lt;- c(\n  paste(\n    \"[2023-10-12] Settled Solids Protocol Development,\",\n    \"Vortex Time and Centrifuge Settings\"\n  ),\n  \"[2023-10-10] Daily Processing Protocol Testing\",\n  \"[2023-09-22] New Processing Tests\"\n)\n\n\nfilename_pattern &lt;- \"_Results_\"\ncol_types &lt;- list(\n  Target = col_character(),\n  Cq = col_double(),\n  TreatmentGroup = col_character()\n)\nraw_data &lt;- list.files(\n  map_chr(experiments, function(exp) {\n    here(data_dir, exp, \"qpcr\")\n  }),\n  pattern = filename_pattern,\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f, skip = 23, col_types = col_types) |&gt;\n      mutate(plate = get_plate(f))\n  }) |&gt;\n  list_rbind() |&gt;\n  glimpse()\n\n[1] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_Cov2_PMMV_Results_20231010_125053.csv\"                                                         \n[2] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_CrA_16S_Results_20231010_125152.csv\"                                                           \n[3] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_Noro_Results_20231010_125241.csv\"                                                              \n[4] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_16S_Results_20231016_105057.csv\"           \n[5] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_Cov2_CORRECTED_Results_20231016_133517.csv\"\n[6] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_CrA_Results_20231016_104600.csv\"           \n[7] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_Noro_Results_20231016_130005.csv\"          \n[8] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_PMMoV_Results_20231016_104527.csv\"         \n\n\nWarning: The following named parsers don't match the column names:\nTreatmentGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: The following named parsers don't match the column names:\nTreatmentGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: The following named parsers don't match the column names:\nTreatmentGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: The following named parsers don't match the column names: TreatmentGroup\nThe following named parsers don't match the column names: TreatmentGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: The following named parsers don't match the column names:\nTreatmentGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: The following named parsers don't match the column names:\nTreatmentGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nWarning: The following named parsers don't match the column names:\nTreatmentGroup\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 481\nColumns: 22\n$ Well                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"10000.0\", \"10000.0\", \"10000…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"STANDARD\", \"…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.3915582, 1.4014582, 1.4073581, 1.4090574, 1.…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.15919, 32.98389, 32.66178, 22.39386, 22.220…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9759085, 0.9891340, 0.9883204, 0.9891666, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 32.93495, 32.93495, 32.93495, 22.31821, 22.318…\n$ `Cq SD`                 &lt;dbl&gt; 0.25229116, 0.25229116, 0.25229116, 0.08875503…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.2999157, 0.2999157, 0.2999157, 0.2999157, 0.…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 27, 27, 26, 16, 16, 16, 24, 23, 23, 20, 20, 19…\n$ plate                   &lt;chr&gt; \"2023-10-09_Cov2_PMMV\", \"2023-10-09_Cov2_PMMV\"…\n\n\n\nmetadata_file &lt;- here(\n  data_dir,\n  experiments[1],\n  \"metadata.csv\"\n)\nmetadata &lt;- experiments |&gt;\n  map(function(exp) {\n    read_csv(here(data_dir, exp, \"metadata.csv\"), col_types = col_types)\n  }) |&gt;\n  list_rbind() |&gt;\n  glimpse()\n\nWarning: The following named parsers don't match the column names: Target, Cq\nThe following named parsers don't match the column names: Target, Cq\nThe following named parsers don't match the column names: Target, Cq\n\n\nRows: 21\nColumns: 13\n$ Sample_ID         &lt;chr&gt; \"1-1\", \"1-2\", \"2-1\", \"2-2\", \"3-1\", \"3-2\", \"4-1\", \"4-…\n$ TreatmentGroup    &lt;chr&gt; \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"Centrifuge …\n$ VortexMin         &lt;dbl&gt; 20, 20, 20, 20, 5, 5, 5, 5, NA, NA, NA, NA, NA, NA, …\n$ CFSpeed           &lt;dbl&gt; 10000, 10000, 3500, 3500, 10000, 10000, 3500, 3500, …\n$ CollectionDate    &lt;date&gt; 2023-10-12, 2023-10-12, 2023-10-12, 2023-10-12, 202…\n$ Source            &lt;chr&gt; \"Solids\", \"Solids\", \"Solids\", \"Solids\", \"Solids\", \"S…\n$ Volume            &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 200, 200, 20, 200, 2…\n$ Qubit_ID          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ ProcessingHandler &lt;chr&gt; \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ar…\n$ ExtractionHandler &lt;chr&gt; \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ar…\n$ `qPCR date`       &lt;date&gt; 2023-10-14, 2023-10-14, 2023-10-14, 2023-10-14, 202…\n$ qPCRHandler       &lt;chr&gt; \"Olivia\", \"Olivia\", \"Olivia\", \"Olivia\", \"Olivia\", \"O…\n$ qPCR_dilution     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\n\ntidy_data &lt;- raw_data |&gt;\n  separate_wider_regex(\n    `Well Position`,\n    c(well_row = \"[A-Z]+\", well_col = \"[0-9]+\"),\n    cols_remove = FALSE,\n  ) |&gt;\n  left_join(metadata, by = join_by(Sample == Sample_ID)) |&gt;\n  mutate(Target = if_else(Target == \"PMMV\", \"PMMoV\", Target)) |&gt;\n  glimpse()\n\nRows: 481\nColumns: 36\n$ Well                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ well_row                &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ well_col                &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"10000.0\", \"10000.0\", \"10000…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"STANDARD\", \"…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.3915582, 1.4014582, 1.4073581, 1.4090574, 1.…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.15919, 32.98389, 32.66178, 22.39386, 22.220…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9759085, 0.9891340, 0.9883204, 0.9891666, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 32.93495, 32.93495, 32.93495, 22.31821, 22.318…\n$ `Cq SD`                 &lt;dbl&gt; 0.25229116, 0.25229116, 0.25229116, 0.08875503…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.2999157, 0.2999157, 0.2999157, 0.2999157, 0.…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 27, 27, 26, 16, 16, 16, 24, 23, 23, 20, 20, 19…\n$ plate                   &lt;chr&gt; \"2023-10-09_Cov2_PMMV\", \"2023-10-09_Cov2_PMMV\"…\n$ TreatmentGroup          &lt;chr&gt; \"Centrifuge + 0.45 filter\", \"Centrifuge + 0.45…\n$ VortexMin               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ CFSpeed                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ CollectionDate          &lt;date&gt; 2023-10-02, 2023-10-02, 2023-10-02, NA, NA, N…\n$ Source                  &lt;chr&gt; \"N-S mix\", \"N-S mix\", \"N-S mix\", NA, NA, NA, \"…\n$ Volume                  &lt;dbl&gt; 280, 280, 280, NA, NA, NA, 280, 280, 280, NA, …\n$ Qubit_ID                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ProcessingHandler       &lt;chr&gt; \"Ari\", \"Ari\", \"Ari\", NA, NA, NA, \"Ari\", \"Ari\",…\n$ ExtractionHandler       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `qPCR date`             &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ qPCRHandler             &lt;chr&gt; \"Olivia\", \"Olivia\", \"Olivia\", NA, NA, NA, \"Oli…\n$ qPCR_dilution           &lt;chr&gt; \"1:5\", \"1:5\", \"1:5\", NA, NA, NA, \"1:5\", \"1:5\",…\n\n\n\namp_data &lt;- list.files(\n  map_chr(experiments, function(exp) {\n    here(data_dir, exp, \"qpcr\")\n  }),\n  pattern = \"Amplification Data\",\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    ) |&gt;\n      mutate(plate = get_plate(f))\n  }) |&gt;\n  list_rbind() |&gt;\n  mutate(Target = if_else(Target == \"PMMV\", \"PMMoV\", Target)) |&gt;\n  left_join(tidy_data,\n    by = join_by(plate, Well, `Well Position`, Sample, Omit, Target)\n  ) |&gt;\n  glimpse()\n\n[1] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_Cov2_PMMV_Amplification Data_20231010_125053.csv\"                                                         \n[2] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_CrA_16S_Amplification Data_20231010_125152.csv\"                                                           \n[3] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_Noro_Amplification Data_20231010_125241.csv\"                                                              \n[4] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_16S_Amplification Data_20231016_105058.csv\"           \n[5] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_Cov2_CORRECTED_Amplification Data_20231016_133517.csv\"\n[6] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_CrA_Amplification Data_20231016_104600.csv\"           \n[7] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_Noro_Amplification Data_20231016_130005.csv\"          \n[8] \"/Users/dan/airport/[2023-10-12] Settled Solids Protocol Development, Vortex Time and Centrifuge Settings/qpcr/2023-10-14_PMMoV_Amplification Data_20231016_104527.csv\"         \n\n\nWarning: The following named parsers don't match the column names: Cq, TreatmentGroup\nThe following named parsers don't match the column names: Cq, TreatmentGroup\nThe following named parsers don't match the column names: Cq, TreatmentGroup\nThe following named parsers don't match the column names: Cq, TreatmentGroup\nThe following named parsers don't match the column names: Cq, TreatmentGroup\nThe following named parsers don't match the column names: Cq, TreatmentGroup\nThe following named parsers don't match the column names: Cq, TreatmentGroup\nThe following named parsers don't match the column names: Cq, TreatmentGroup\n\n\nRows: 19,240\nColumns: 39\n$ Well                    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\"…\n$ `Cycle Number`          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Rn                      &lt;dbl&gt; 0.6443956, 0.6382574, 0.6284555, 0.6179680, 0.…\n$ dRn                     &lt;dbl&gt; 2.836028e-02, 2.374099e-02, 1.545804e-02, 6.48…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ plate                   &lt;chr&gt; \"2023-10-09_Cov2_PMMV\", \"2023-10-09_Cov2_PMMV\"…\n$ well_row                &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ well_col                &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"U…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.391558, 1.391558, 1.391558, 1.391558, 1.3915…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.15919, 33.15919, 33.15919, 33.15919, 33.159…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9759085, 0.9759085, 0.9759085, 0.9759085, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 32.93495, 32.93495, 32.93495, 32.93495, 32.934…\n$ `Cq SD`                 &lt;dbl&gt; 0.2522912, 0.2522912, 0.2522912, 0.2522912, 0.…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.2999157, 0.2999157, 0.2999157, 0.2999157, 0.…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27…\n$ TreatmentGroup          &lt;chr&gt; \"Centrifuge + 0.45 filter\", \"Centrifuge + 0.45…\n$ VortexMin               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ CFSpeed                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ CollectionDate          &lt;date&gt; 2023-10-02, 2023-10-02, 2023-10-02, 2023-10-0…\n$ Source                  &lt;chr&gt; \"N-S mix\", \"N-S mix\", \"N-S mix\", \"N-S mix\", \"N…\n$ Volume                  &lt;dbl&gt; 280, 280, 280, 280, 280, 280, 280, 280, 280, 2…\n$ Qubit_ID                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ProcessingHandler       &lt;chr&gt; \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari\", \"Ari…\n$ ExtractionHandler       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `qPCR date`             &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ qPCRHandler             &lt;chr&gt; \"Olivia\", \"Olivia\", \"Olivia\", \"Olivia\", \"Olivi…\n$ qPCR_dilution           &lt;chr&gt; \"1:5\", \"1:5\", \"1:5\", \"1:5\", \"1:5\", \"1:5\", \"1:5…"
  },
  {
    "objectID": "notebooks/2023-10-16-combined_protocol_experiments.html#quality-control",
    "href": "notebooks/2023-10-16-combined_protocol_experiments.html#quality-control",
    "title": "2023-10-16 Analysis of recent protocol-comparison experiments",
    "section": "Quality control",
    "text": "Quality control\n\ntidy_data |&gt; count(Task, is.na(Cq))\n\n# A tibble: 6 × 3\n  Task     `is.na(Cq)`     n\n  &lt;chr&gt;    &lt;lgl&gt;       &lt;int&gt;\n1 NTC      FALSE           3\n2 NTC      TRUE           28\n3 STANDARD FALSE         133\n4 STANDARD TRUE            2\n5 UNKNOWN  FALSE         314\n6 UNKNOWN  TRUE            1\n\ntidy_data |&gt;\n  filter(Task == \"NTC\", !is.na(Cq)) |&gt;\n  glimpse()\n\nRows: 3\nColumns: 36\n$ Well                    &lt;dbl&gt; 85, 86, 87\n$ well_row                &lt;chr&gt; \"H\", \"H\", \"H\"\n$ well_col                &lt;chr&gt; \"1\", \"2\", \"3\"\n$ `Well Position`         &lt;chr&gt; \"H1\", \"H2\", \"H3\"\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE\n$ Sample                  &lt;chr&gt; NA, NA, NA\n$ Target                  &lt;chr&gt; \"16S\", \"16S\", \"16S\"\n$ Task                    &lt;chr&gt; \"NTC\", \"NTC\", \"NTC\"\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\"\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\"\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\"\n$ `Amp Score`             &lt;dbl&gt; 1.418582, 1.403374, 1.409229\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA\n$ Cq                      &lt;dbl&gt; 29.98365, 30.00206, 29.95623\n$ `Cq Confidence`         &lt;dbl&gt; 0.9892989, 0.9831535, 0.9896414\n$ `Cq Mean`               &lt;dbl&gt; 29.98065, 29.98065, 29.98065\n$ `Cq SD`                 &lt;dbl&gt; 0.02306268, 0.02306268, 0.02306268\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE\n$ Threshold               &lt;dbl&gt; 0.2693924, 0.2693924, 0.2693924\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3\n$ `Baseline End`          &lt;dbl&gt; 23, 21, 23\n$ plate                   &lt;chr&gt; \"2023-10-14_16S\", \"2023-10-14_16S\", \"2023-10-1…\n$ TreatmentGroup          &lt;chr&gt; NA, NA, NA\n$ VortexMin               &lt;dbl&gt; NA, NA, NA\n$ CFSpeed                 &lt;dbl&gt; NA, NA, NA\n$ CollectionDate          &lt;date&gt; NA, NA, NA\n$ Source                  &lt;chr&gt; NA, NA, NA\n$ Volume                  &lt;dbl&gt; NA, NA, NA\n$ Qubit_ID                &lt;lgl&gt; NA, NA, NA\n$ ProcessingHandler       &lt;chr&gt; NA, NA, NA\n$ ExtractionHandler       &lt;chr&gt; NA, NA, NA\n$ `qPCR date`             &lt;date&gt; NA, NA, NA\n$ qPCRHandler             &lt;chr&gt; NA, NA, NA\n$ qPCR_dilution           &lt;chr&gt; NA, NA, NA\n\n\n\namp_data |&gt;\n  filter(Task == \"NTC\") |&gt;\n  ggplot(aes(x = `Cycle Number`, y = dRn)) +\n  geom_line(mapping = aes(\n    group = Well,\n  )) +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10(limits = c(1e-3, 1e1)) +\n  facet_wrap(~ interaction(plate, Target))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 234 rows containing missing values (`geom_line()`).\n\n\n\n\n\nThere is amplification of the NTC for the 2023-10-14_16S plate. Olivia says:\n\nThose are not an error. We should discuss this- the plates we are using for qPCR are not sterile. That’s not a problem for most assays, but so far we’ve almost always had amplification in the 16S negative controls. It’s usually much lower than the samples and lowest standards, though.\n\nVerify:\n\namp_data |&gt;\n  filter(plate == \"2023-10-14_16S\" & (Task == \"NTC\" | Task == \"STANDARD\")) |&gt;\n  ggplot(aes(x = `Cycle Number`, y = dRn)) +\n  geom_line(mapping = aes(\n    color = Task,\n    group = Well,\n  )) +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10(limits = c(1e-3, 1e1))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 29 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\namp_data |&gt;\n  filter(Task == \"UNKNOWN\" & is.na(Cq)) |&gt;\n  print() |&gt;\n  ggplot(aes(x = `Cycle Number`, y = dRn)) +\n  geom_line(mapping = aes(\n    color = Task,\n    group = Well,\n  )) +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10(limits = c(1e-3, 1e1))\n\n# A tibble: 40 × 39\n    Well `Well Position` `Cycle Number` Target    Rn      dRn Sample Omit  plate\n   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;lgl&gt; &lt;chr&gt;\n 1    90 H6                           1 PMMoV  0.350 -0.0310  4-2    FALSE 2023…\n 2    90 H6                           2 PMMoV  0.353 -0.0290  4-2    FALSE 2023…\n 3    90 H6                           3 PMMoV  0.363 -0.0211  4-2    FALSE 2023…\n 4    90 H6                           4 PMMoV  0.371 -0.0143  4-2    FALSE 2023…\n 5    90 H6                           5 PMMoV  0.378 -0.00919 4-2    FALSE 2023…\n 6    90 H6                           6 PMMoV  0.383 -0.00520 4-2    FALSE 2023…\n 7    90 H6                           7 PMMoV  0.389 -0.00109 4-2    FALSE 2023…\n 8    90 H6                           8 PMMoV  0.393  0.00154 4-2    FALSE 2023…\n 9    90 H6                           9 PMMoV  0.395  0.00290 4-2    FALSE 2023…\n10    90 H6                          10 PMMoV  0.398  0.00399 4-2    FALSE 2023…\n# ℹ 30 more rows\n# ℹ 30 more variables: well_row &lt;chr&gt;, well_col &lt;chr&gt;, Task &lt;chr&gt;,\n#   Reporter &lt;chr&gt;, Quencher &lt;chr&gt;, `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;, TreatmentGroup &lt;chr&gt;, …\n\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 19 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nAll the amplification curves\n\namp_data |&gt;\n  ggplot(aes(x = `Cycle Number`, y = dRn)) +\n  geom_line(mapping = aes(\n    color = Task,\n    group = Well,\n  )) +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10(limits = c(1e-3, 1e1)) +\n  facet_grid(rows = vars(plate), cols = vars(Target))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 295 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "notebooks/2023-10-16-combined_protocol_experiments.html#compare-methods",
    "href": "notebooks/2023-10-16-combined_protocol_experiments.html#compare-methods",
    "title": "2023-10-16 Analysis of recent protocol-comparison experiments",
    "section": "Compare methods",
    "text": "Compare methods\n\ntidy_data |&gt;\n  filter(Task == \"UNKNOWN\") |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = TreatmentGroup,\n    color = Source,\n    shape = as.factor(CollectionDate),\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\")\n\nWarning: Removed 1 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\nMike: &gt; @Dan R for the second experiment ([2023-10-10] Daily Processing Protocol Testing) can you create a figure where they y-axis is the wastewater sample (N Inf, S Inf, or SS), and the color is the treatment?\n\ntidy_data |&gt;\n  filter(Task == \"UNKNOWN\", CollectionDate == \"2023-10-11\") |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = Source,\n    color = TreatmentGroup,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\") +\n  theme(legend.position = \"bottom\") +\n  theme(panel.spacing.x = unit(6, \"mm\"))\n\n\n\n\n\nThen for the 3rd experiment (2×2 design), maybe set color to centrifuge and shape to vortex treatment, and make one plot where y = centrifuge and a second plot where y = vortex\n\n\ntidy_data |&gt;\n  filter(Task == \"UNKNOWN\", CollectionDate == \"2023-10-12\") |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = as.factor(CFSpeed),\n    color = as.factor(VortexMin),\n    group = Sample\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\") +\n  theme(legend.position = \"bottom\") +\n  theme(panel.spacing.x = unit(6, \"mm\"))\n\nWarning: Removed 1 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\n\ntidy_data |&gt;\n  filter(Task == \"UNKNOWN\", CollectionDate == \"2023-10-12\") |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = as.factor(VortexMin),\n    color = as.factor(CFSpeed),\n    group = Sample\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\") +\n  theme(legend.position = \"bottom\") +\n  theme(panel.spacing.x = unit(6, \"mm\"))\n\nWarning: Removed 1 rows containing non-finite values (`stat_summary()`)."
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html",
    "href": "notebooks/2023-09-15-qpcr_analysis.html",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "",
    "text": "Testing the efficacy of RNA extraction kits in settled solid samples. See experiment google doc."
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#objectives",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#objectives",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "",
    "text": "Testing the efficacy of RNA extraction kits in settled solid samples. See experiment google doc."
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#preliminary-work",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#preliminary-work",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "Preliminary work",
    "text": "Preliminary work\n\nOlivia put the .eds files in NAO qPCR data/Olivia on Google Drive and shared the folder with Dan.\nGoogle Drive for desktop only syncs shared drives, not shared folders in other drives, so Dan figured out a work around. He made a shortcut to the shared folder in his own google drive so it would sync locally.\nOpened up the .eds files in Design and Analysis locally and exported to .csv and saved those in the airport experiment folder on the main google drive.\nFound a Google Sheet with metadata, downloaded as CSV and added the CSV back to the Google Drive"
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#data-import",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#data-import",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "Data import",
    "text": "Data import\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\n\n\ndata_dir &lt;-\n  \"~/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/\"\nfilename_pattern &lt;- \"Results\"\n\n\ncol_types &lt;- list(\n  Target = col_character(),\n  Cq = col_double()\n)\nraw_data &lt;- list.files(\n  paste0(data_dir, \"qpcr\"),\n  pattern = filename_pattern,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind()\n\n[1] \"/Users/dan/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/qpcr/2023-09-14_Cov2_extractions_Results_20230915_100200.csv\"\n[2] \"/Users/dan/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/qpcr/2023-09-14_Noro_Extractions_Results_20230915_100255.csv\"\n[3] \"/Users/dan/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/qpcr/2023-09-15_CrA_extractions_Results_20230915_105855.csv\" \n[4] \"/Users/dan/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/qpcr/2023-09-18_16S_extractions_Results_20230918_151203.csv\" \n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\nprint(raw_data)\n\n# A tibble: 369 × 21\n    Well `Well Position` Omit  Sample Target Task     Reporter Quencher\n   &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1     1 A1              FALSE 1A     Cov2   UNKNOWN  FAM      NFQ-MGB \n 2     2 A2              FALSE 1A     Cov2   UNKNOWN  FAM      NFQ-MGB \n 3     3 A3              FALSE 1A     Cov2   UNKNOWN  FAM      NFQ-MGB \n 4     4 A4              FALSE 3C     Cov2   UNKNOWN  FAM      NFQ-MGB \n 5     5 A5              FALSE 3C     Cov2   UNKNOWN  FAM      NFQ-MGB \n 6     6 A6              FALSE 3C     Cov2   UNKNOWN  FAM      NFQ-MGB \n 7     7 A7              FALSE 6B     Cov2   UNKNOWN  FAM      NFQ-MGB \n 8     8 A8              FALSE 6B     Cov2   UNKNOWN  FAM      NFQ-MGB \n 9     9 A9              FALSE 6B     Cov2   UNKNOWN  FAM      NFQ-MGB \n10    10 A10             FALSE 1000.0 Cov2   STANDARD FAM      NFQ-MGB \n# ℹ 359 more rows\n# ℹ 13 more variables: `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;\n\n\nThere were some wells mixed up between the intended plate layout and the actual plate for Norovirus. See Results doc.\n\ncorrected_samples &lt;- tribble(\n  ~`Well Position`, ~Sample,\n  \"F7\",             \"empty\",\n  \"F8\",             \"empty\",\n  \"F9\",             \"empty\",\n  \"F10\",            \"empty\",\n  \"F11\",            \"empty\",\n  \"F12\",            \"empty\",\n  \"G7\",             \"6C/100\",\n  \"G8\",             \"6C/10\",\n  \"G9\",             \"7C/100\",\n  \"G10\",            \"7C/10\",\n  \"G11\",            \"3C/100\",\n  \"G12\",            \"3C/10\",\n  \"H7\",             \"1C/100\",\n  \"H8\",             \"1C/10\",\n  \"H9\",             \"1C/100\",\n  \"H10\",            \"1C/10\",\n  \"H11\",            \"NTC\",\n  \"H12\",            \"NTC\",\n)\ncorrected_samples$Target &lt;- \"Noro\"\nprint(corrected_samples)\n\n# A tibble: 18 × 3\n   `Well Position` Sample Target\n   &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt; \n 1 F7              empty  Noro  \n 2 F8              empty  Noro  \n 3 F9              empty  Noro  \n 4 F10             empty  Noro  \n 5 F11             empty  Noro  \n 6 F12             empty  Noro  \n 7 G7              6C/100 Noro  \n 8 G8              6C/10  Noro  \n 9 G9              7C/100 Noro  \n10 G10             7C/10  Noro  \n11 G11             3C/100 Noro  \n12 G12             3C/10  Noro  \n13 H7              1C/100 Noro  \n14 H8              1C/10  Noro  \n15 H9              1C/100 Noro  \n16 H10             1C/10  Noro  \n17 H11             NTC    Noro  \n18 H12             NTC    Noro  \n\n\n\ncorrected_raw_data &lt;- raw_data |&gt;\n  left_join(corrected_samples, by = join_by(`Well Position`, Target)) |&gt;\n  mutate(Sample = ifelse(is.na(Sample.y), Sample.x, Sample.y)) |&gt;\n  print()\n\n# A tibble: 369 × 23\n    Well `Well Position` Omit  Sample.x Target Task     Reporter Quencher\n   &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1     1 A1              FALSE 1A       Cov2   UNKNOWN  FAM      NFQ-MGB \n 2     2 A2              FALSE 1A       Cov2   UNKNOWN  FAM      NFQ-MGB \n 3     3 A3              FALSE 1A       Cov2   UNKNOWN  FAM      NFQ-MGB \n 4     4 A4              FALSE 3C       Cov2   UNKNOWN  FAM      NFQ-MGB \n 5     5 A5              FALSE 3C       Cov2   UNKNOWN  FAM      NFQ-MGB \n 6     6 A6              FALSE 3C       Cov2   UNKNOWN  FAM      NFQ-MGB \n 7     7 A7              FALSE 6B       Cov2   UNKNOWN  FAM      NFQ-MGB \n 8     8 A8              FALSE 6B       Cov2   UNKNOWN  FAM      NFQ-MGB \n 9     9 A9              FALSE 6B       Cov2   UNKNOWN  FAM      NFQ-MGB \n10    10 A10             FALSE 1000.0   Cov2   STANDARD FAM      NFQ-MGB \n# ℹ 359 more rows\n# ℹ 15 more variables: `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;, Sample.y &lt;chr&gt;, Sample &lt;chr&gt;\n\ncorrected_raw_data |&gt;\n  count(Target, Sample) |&gt;\n  print(n = Inf)\n\n# A tibble: 131 × 3\n    Target Sample                    n\n    &lt;chr&gt;  &lt;chr&gt;                 &lt;int&gt;\n  1 16S    0.0034500000000000004     3\n  2 16S    0.0345                    3\n  3 16S    0.34500000000000003       3\n  4 16S    1A                        3\n  5 16S    1B                        3\n  6 16S    1C                        3\n  7 16S    2A                        3\n  8 16S    2B                        3\n  9 16S    2C                        3\n 10 16S    3.45                      3\n 11 16S    3.4500000000000004E-4     3\n 12 16S    3A                        3\n 13 16S    3B                        3\n 14 16S    3C                        3\n 15 16S    4A                        3\n 16 16S    4B                        3\n 17 16S    4C                        3\n 18 16S    5A                        3\n 19 16S    5B                        3\n 20 16S    5C                        3\n 21 16S    6A                        3\n 22 16S    6B                        3\n 23 16S    6C                        3\n 24 16S    7A                        3\n 25 16S    7B                        3\n 26 16S    7C                        3\n 27 16S    &lt;NA&gt;                      3\n 28 Cov2   0.010000000000000002      3\n 29 Cov2   0.1                       3\n 30 Cov2   1.0                       3\n 31 Cov2   10.0                      3\n 32 Cov2   100.0                     3\n 33 Cov2   1000.0                    3\n 34 Cov2   1A                        3\n 35 Cov2   1B                        3\n 36 Cov2   1C                        3\n 37 Cov2   2A                        3\n 38 Cov2   2B                        3\n 39 Cov2   2C                        3\n 40 Cov2   3A                        3\n 41 Cov2   3B                        3\n 42 Cov2   3C                        3\n 43 Cov2   3C/10                     2\n 44 Cov2   3C/100                    2\n 45 Cov2   4A                        3\n 46 Cov2   4B                        3\n 47 Cov2   4C                        3\n 48 Cov2   5A                        3\n 49 Cov2   5B                        3\n 50 Cov2   5C                        3\n 51 Cov2   6A                        3\n 52 Cov2   6B                        3\n 53 Cov2   6C                        3\n 54 Cov2   6C/10                     2\n 55 Cov2   6C/100                    2\n 56 Cov2   7A                        3\n 57 Cov2   7B                        3\n 58 Cov2   7C                        3\n 59 Cov2   7C/10                     2\n 60 Cov2   7C/100                    2\n 61 Cov2   &lt;NA&gt;                      3\n 62 CrA    1A                        3\n 63 CrA    1B                        3\n 64 CrA    1C                        3\n 65 CrA    1C/10                     2\n 66 CrA    2A                        3\n 67 CrA    2B                        3\n 68 CrA    2C                        3\n 69 CrA    3.0E7                     3\n 70 CrA    3000.0                    3\n 71 CrA    30000.0                   3\n 72 CrA    300000.0                  3\n 73 CrA    3000000.0                 3\n 74 CrA    3A                        3\n 75 CrA    3B                        3\n 76 CrA    3C                        3\n 77 CrA    3C/10                     2\n 78 CrA    3C/100                    2\n 79 CrA    4A                        3\n 80 CrA    4B                        3\n 81 CrA    4C                        3\n 82 CrA    5A                        3\n 83 CrA    5B                        3\n 84 CrA    5C                        3\n 85 CrA    6A                        3\n 86 CrA    6B                        3\n 87 CrA    6C                        3\n 88 CrA    6C/10                     2\n 89 CrA    6C/100                    2\n 90 CrA    7A                        3\n 91 CrA    7B                        3\n 92 CrA    7C                        3\n 93 CrA    7C/10                     2\n 94 CrA    7C/100                    4\n 95 CrA    &lt;NA&gt;                      2\n 96 Noro   1.0                       3\n 97 Noro   10.0                      3\n 98 Noro   100.0                     3\n 99 Noro   1000.0                    3\n100 Noro   10000.0                   3\n101 Noro   1A                        3\n102 Noro   1B                        3\n103 Noro   1C                        3\n104 Noro   1C/10                     2\n105 Noro   1C/100                    2\n106 Noro   2A                        3\n107 Noro   2B                        3\n108 Noro   2C                        3\n109 Noro   3A                        3\n110 Noro   3B                        3\n111 Noro   3C                        3\n112 Noro   3C/10                     1\n113 Noro   3C/100                    1\n114 Noro   4A                        3\n115 Noro   4B                        3\n116 Noro   4C                        3\n117 Noro   5A                        3\n118 Noro   5B                        3\n119 Noro   5C                        3\n120 Noro   6A                        3\n121 Noro   6B                        3\n122 Noro   6C                        3\n123 Noro   6C/10                     1\n124 Noro   6C/100                    1\n125 Noro   7A                        3\n126 Noro   7B                        3\n127 Noro   7C                        3\n128 Noro   7C/10                     1\n129 Noro   7C/100                    1\n130 Noro   NTC                       2\n131 Noro   empty                     6\n\n\n\nmetadata_file &lt;- paste0(\n  data_dir,\n  \"[2023-09-11] Extraction Experiment 2 templates and results\",\n  \" - sampleMetadata.csv\"\n)\nmetadata &lt;- read_csv(metadata_file)\n\nRows: 21 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Sample_ID, Extraction_kit, Short_kit, Elution_format, Brand, NA_Target\ndbl (2): Kit Batch, Elution_volume\nlgl (1): FP_sampleID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(metadata)\n\nRows: 21\nColumns: 9\n$ Sample_ID      &lt;chr&gt; \"1A\", \"1B\", \"1C\", \"2A\", \"2B\", \"2C\", \"3A\", \"3B\", \"3C\", \"…\n$ Extraction_kit &lt;chr&gt; \"Zymo quick-RNA\", \"Zymo quick-RNA\", \"Zymo quick-RNA\", \"…\n$ Short_kit      &lt;chr&gt; \"1_ZR\", \"1_ZR\", \"1_ZR\", \"2_ZD\", \"2_ZD\", \"2_ZD\", \"3_IPL\"…\n$ `Kit Batch`    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3…\n$ Elution_volume &lt;dbl&gt; 30, 30, 30, 100, 100, 100, 100, 100, 100, 60, 60, 60, 1…\n$ Elution_format &lt;chr&gt; \"15*2\", \"15*2\", \"15*2\", \"50*2\", \"50*2\", \"50*2\", \"50*2\",…\n$ Brand          &lt;chr&gt; \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Zymo\", \"Invitr…\n$ NA_Target      &lt;chr&gt; \"RNA\", \"RNA\", \"RNA\", \"DNA+RNA\", \"DNA+RNA\", \"DNA+RNA\", \"…\n$ FP_sampleID    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\n\ntidy_data &lt;- corrected_raw_data |&gt;\n  mutate(\n    Sample_ID = str_split_i(Sample, \"/\", 1),\n    dilution = as.integer(str_split_i(Sample, \"/\", 2)),\n    replicate = str_extract(Sample_ID, \"[A-Z]$\"),\n    quantity = as.double(Sample),\n  ) |&gt;\n  mutate(dilution = replace_na(dilution, 1)) |&gt;\n  left_join(\n    metadata,\n    by = join_by(Sample_ID)\n  )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `quantity = as.double(Sample)`.\nCaused by warning:\n! NAs introduced by coercion\n\nglimpse(tidy_data)\n\nRows: 369\nColumns: 35\n$ Well                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Sample.x                &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"3C\", \"3C\", \"3C\", \"6B\", \"6B\"…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"U…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"NO_…\n$ `Amp Score`             &lt;dbl&gt; 1.1701024, 1.0212594, 1.1604009, 1.1057531, 1.…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 32.44077, 35.24757, 33.72678, 34.44077, 34.704…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9451320, 0.9773032, 0.9759027, 0.9634213, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 33.80504, 33.80504, 33.80504, 34.31332, 34.313…\n$ `Cq SD`                 &lt;dbl&gt; 1.4050310, 1.4050310, 1.4050310, 0.4683633, 0.…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 29, 32, 30, 31, 31, 31, 31, 31, 31, 21, 21, 21…\n$ Sample.y                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"3C\", \"3C\", \"3C\", \"6B\", \"6B\"…\n$ Sample_ID               &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"3C\", \"3C\", \"3C\", \"6B\", \"6B\"…\n$ dilution                &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ replicate               &lt;chr&gt; \"A\", \"A\", \"A\", \"C\", \"C\", \"C\", \"B\", \"B\", \"B\", N…\n$ quantity                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, 1000, 1000…\n$ Extraction_kit          &lt;chr&gt; \"Zymo quick-RNA\", \"Zymo quick-RNA\", \"Zymo quic…\n$ Short_kit               &lt;chr&gt; \"1_ZR\", \"1_ZR\", \"1_ZR\", \"3_IPL\", \"3_IPL\", \"3_I…\n$ `Kit Batch`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 3, 3, 3, NA, NA, NA, 1, 1, 1…\n$ Elution_volume          &lt;dbl&gt; 30, 30, 30, 100, 100, 100, 80, 80, 80, NA, NA,…\n$ Elution_format          &lt;chr&gt; \"15*2\", \"15*2\", \"15*2\", \"50*2\", \"50*2\", \"50*2\"…\n$ Brand                   &lt;chr&gt; \"Zymo\", \"Zymo\", \"Zymo\", \"Invitrogen\", \"Invitro…\n$ NA_Target               &lt;chr&gt; \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA\", \"RNA…\n$ FP_sampleID             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\ntidy_data |&gt;\n  count(Sample_ID, Target, dilution, replicate) |&gt;\n  print(n = Inf)\n\n# A tibble: 131 × 5\n    Sample_ID             Target dilution replicate     n\n    &lt;chr&gt;                 &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;     &lt;int&gt;\n  1 0.0034500000000000004 16S           1 &lt;NA&gt;          3\n  2 0.010000000000000002  Cov2          1 &lt;NA&gt;          3\n  3 0.0345                16S           1 &lt;NA&gt;          3\n  4 0.1                   Cov2          1 &lt;NA&gt;          3\n  5 0.34500000000000003   16S           1 &lt;NA&gt;          3\n  6 1.0                   Cov2          1 &lt;NA&gt;          3\n  7 1.0                   Noro          1 &lt;NA&gt;          3\n  8 10.0                  Cov2          1 &lt;NA&gt;          3\n  9 10.0                  Noro          1 &lt;NA&gt;          3\n 10 100.0                 Cov2          1 &lt;NA&gt;          3\n 11 100.0                 Noro          1 &lt;NA&gt;          3\n 12 1000.0                Cov2          1 &lt;NA&gt;          3\n 13 1000.0                Noro          1 &lt;NA&gt;          3\n 14 10000.0               Noro          1 &lt;NA&gt;          3\n 15 1A                    16S           1 A             3\n 16 1A                    Cov2          1 A             3\n 17 1A                    CrA           1 A             3\n 18 1A                    Noro          1 A             3\n 19 1B                    16S           1 B             3\n 20 1B                    Cov2          1 B             3\n 21 1B                    CrA           1 B             3\n 22 1B                    Noro          1 B             3\n 23 1C                    16S           1 C             3\n 24 1C                    Cov2          1 C             3\n 25 1C                    CrA           1 C             3\n 26 1C                    CrA          10 C             2\n 27 1C                    Noro          1 C             3\n 28 1C                    Noro         10 C             2\n 29 1C                    Noro        100 C             2\n 30 2A                    16S           1 A             3\n 31 2A                    Cov2          1 A             3\n 32 2A                    CrA           1 A             3\n 33 2A                    Noro          1 A             3\n 34 2B                    16S           1 B             3\n 35 2B                    Cov2          1 B             3\n 36 2B                    CrA           1 B             3\n 37 2B                    Noro          1 B             3\n 38 2C                    16S           1 C             3\n 39 2C                    Cov2          1 C             3\n 40 2C                    CrA           1 C             3\n 41 2C                    Noro          1 C             3\n 42 3.0E7                 CrA           1 &lt;NA&gt;          3\n 43 3.45                  16S           1 &lt;NA&gt;          3\n 44 3.4500000000000004E-4 16S           1 &lt;NA&gt;          3\n 45 3000.0                CrA           1 &lt;NA&gt;          3\n 46 30000.0               CrA           1 &lt;NA&gt;          3\n 47 300000.0              CrA           1 &lt;NA&gt;          3\n 48 3000000.0             CrA           1 &lt;NA&gt;          3\n 49 3A                    16S           1 A             3\n 50 3A                    Cov2          1 A             3\n 51 3A                    CrA           1 A             3\n 52 3A                    Noro          1 A             3\n 53 3B                    16S           1 B             3\n 54 3B                    Cov2          1 B             3\n 55 3B                    CrA           1 B             3\n 56 3B                    Noro          1 B             3\n 57 3C                    16S           1 C             3\n 58 3C                    Cov2          1 C             3\n 59 3C                    Cov2         10 C             2\n 60 3C                    Cov2        100 C             2\n 61 3C                    CrA           1 C             3\n 62 3C                    CrA          10 C             2\n 63 3C                    CrA         100 C             2\n 64 3C                    Noro          1 C             3\n 65 3C                    Noro         10 C             1\n 66 3C                    Noro        100 C             1\n 67 4A                    16S           1 A             3\n 68 4A                    Cov2          1 A             3\n 69 4A                    CrA           1 A             3\n 70 4A                    Noro          1 A             3\n 71 4B                    16S           1 B             3\n 72 4B                    Cov2          1 B             3\n 73 4B                    CrA           1 B             3\n 74 4B                    Noro          1 B             3\n 75 4C                    16S           1 C             3\n 76 4C                    Cov2          1 C             3\n 77 4C                    CrA           1 C             3\n 78 4C                    Noro          1 C             3\n 79 5A                    16S           1 A             3\n 80 5A                    Cov2          1 A             3\n 81 5A                    CrA           1 A             3\n 82 5A                    Noro          1 A             3\n 83 5B                    16S           1 B             3\n 84 5B                    Cov2          1 B             3\n 85 5B                    CrA           1 B             3\n 86 5B                    Noro          1 B             3\n 87 5C                    16S           1 C             3\n 88 5C                    Cov2          1 C             3\n 89 5C                    CrA           1 C             3\n 90 5C                    Noro          1 C             3\n 91 6A                    16S           1 A             3\n 92 6A                    Cov2          1 A             3\n 93 6A                    CrA           1 A             3\n 94 6A                    Noro          1 A             3\n 95 6B                    16S           1 B             3\n 96 6B                    Cov2          1 B             3\n 97 6B                    CrA           1 B             3\n 98 6B                    Noro          1 B             3\n 99 6C                    16S           1 C             3\n100 6C                    Cov2          1 C             3\n101 6C                    Cov2         10 C             2\n102 6C                    Cov2        100 C             2\n103 6C                    CrA           1 C             3\n104 6C                    CrA          10 C             2\n105 6C                    CrA         100 C             2\n106 6C                    Noro          1 C             3\n107 6C                    Noro         10 C             1\n108 6C                    Noro        100 C             1\n109 7A                    16S           1 A             3\n110 7A                    Cov2          1 A             3\n111 7A                    CrA           1 A             3\n112 7A                    Noro          1 A             3\n113 7B                    16S           1 B             3\n114 7B                    Cov2          1 B             3\n115 7B                    CrA           1 B             3\n116 7B                    Noro          1 B             3\n117 7C                    16S           1 C             3\n118 7C                    Cov2          1 C             3\n119 7C                    Cov2         10 C             2\n120 7C                    Cov2        100 C             2\n121 7C                    CrA           1 C             3\n122 7C                    CrA          10 C             2\n123 7C                    CrA         100 C             4\n124 7C                    Noro          1 C             3\n125 7C                    Noro         10 C             1\n126 7C                    Noro        100 C             1\n127 NTC                   Noro          1 C             2\n128 empty                 Noro          1 &lt;NA&gt;          6\n129 &lt;NA&gt;                  16S           1 &lt;NA&gt;          3\n130 &lt;NA&gt;                  Cov2          1 &lt;NA&gt;          3\n131 &lt;NA&gt;                  CrA           1 &lt;NA&gt;          2\n\n\n\ntidy_data |&gt;\n  count(`Result Quality Issues`)\n\n# A tibble: 1 × 2\n  `Result Quality Issues`     n\n  &lt;lgl&gt;                   &lt;int&gt;\n1 NA                        369"
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#kit-comparison",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#kit-comparison",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "Kit comparison",
    "text": "Kit comparison\n\nWith equal Cq axes\n\ntidy_data |&gt;\n  filter(\n    !is.na(Extraction_kit),\n    dilution == 1,\n  ) |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = Extraction_kit,\n    color = replicate,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target)\n\nWarning: Removed 4 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\n\n\nWith free Cq axes\n\ntidy_data |&gt;\n  filter(\n    !is.na(Extraction_kit),\n    dilution == 1,\n  ) |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = Extraction_kit,\n    color = replicate,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\")\n\nWarning: Removed 4 rows containing non-finite values (`stat_summary()`).\n\n\n\n\n\nCheck that the overlapping points really do have three values\n\ntidy_data |&gt;\n  filter(!is.na(Extraction_kit), is.na(Cq)) |&gt;\n  count(Target, Extraction_kit, Sample_ID, dilution) |&gt;\n  print()\n\n# A tibble: 9 × 5\n  Target Extraction_kit                    Sample_ID dilution     n\n  &lt;chr&gt;  &lt;chr&gt;                             &lt;chr&gt;        &lt;int&gt; &lt;int&gt;\n1 Cov2   Invitrogen PureLink RNA           3C              10     2\n2 Cov2   Invitrogen PureLink RNA           3C             100     2\n3 Cov2   NucleoSpin Virus                  4B               1     2\n4 Cov2   QIAamp Viral RNA mini kit         6A               1     1\n5 Cov2   QIAamp Viral RNA mini kit         6C              10     1\n6 Cov2   QIAamp Viral RNA mini kit         6C             100     2\n7 Cov2   Qiagen AllPrep PowerViral DNA/RNA 7A               1     1\n8 Cov2   Qiagen AllPrep PowerViral DNA/RNA 7C              10     2\n9 Cov2   Qiagen AllPrep PowerViral DNA/RNA 7C             100     2\n\n\n\ntidy_data |&gt;\n  filter(\n    Extraction_kit == \"Zymo quick-RNA\",\n    Target == \"CrA\", replicate == \"A\"\n  ) |&gt;\n  print(width = Inf)\n\n# A tibble: 3 × 35\n   Well `Well Position` Omit  Sample.x Target Task    Reporter Quencher\n  &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   \n1     1 A1              FALSE 1A       CrA    UNKNOWN FAM      NFQ-MGB \n2     2 A2              FALSE 1A       CrA    UNKNOWN FAM      NFQ-MGB \n3     3 A3              FALSE 1A       CrA    UNKNOWN FAM      NFQ-MGB \n  `Amp Status` `Amp Score` `Curve Quality` `Result Quality Issues`    Cq\n  &lt;chr&gt;              &lt;dbl&gt; &lt;lgl&gt;           &lt;lgl&gt;                   &lt;dbl&gt;\n1 AMP                 1.41 NA              NA                       21.1\n2 AMP                 1.43 NA              NA                       21.0\n3 AMP                 1.42 NA              NA                       21.1\n  `Cq Confidence` `Cq Mean` `Cq SD` `Auto Threshold` Threshold `Auto Baseline`\n            &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt; &lt;lgl&gt;          \n1           0.989      21.1  0.0535 TRUE                 0.175 TRUE           \n2           0.989      21.1  0.0535 TRUE                 0.175 TRUE           \n3           0.989      21.1  0.0535 TRUE                 0.175 TRUE           \n  `Baseline Start` `Baseline End` Sample.y Sample Sample_ID dilution replicate\n             &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;    \n1                3             16 &lt;NA&gt;     1A     1A               1 A        \n2                3             15 &lt;NA&gt;     1A     1A               1 A        \n3                3             16 &lt;NA&gt;     1A     1A               1 A        \n  quantity Extraction_kit Short_kit `Kit Batch` Elution_volume Elution_format\n     &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;         \n1       NA Zymo quick-RNA 1_ZR                1             30 15*2          \n2       NA Zymo quick-RNA 1_ZR                1             30 15*2          \n3       NA Zymo quick-RNA 1_ZR                1             30 15*2          \n  Brand NA_Target FP_sampleID\n  &lt;chr&gt; &lt;chr&gt;     &lt;lgl&gt;      \n1 Zymo  RNA       NA         \n2 Zymo  RNA       NA         \n3 Zymo  RNA       NA         \n\n\n\n\nAdjusting for elution volume\nAssume the amplification efficiency is 100%, so that an increase in initial concentration by a factor of \\(x\\) decreases \\(C_q\\) by \\(\\log_{2}(x)\\).\nIf a method has elution volume \\(v\\) and we dilute it to total volume \\(V\\), this reduces its final concentration by a factor \\(v / V\\). We can put different methods on the same footing by adding \\(\\log_{2}(v/V)\\) to \\(C_q\\) (so that large elution volumes are penalized with a higher adjusted \\(C_q\\)).\n\nfinal_volume &lt;- 100\ntidy_data |&gt;\n  mutate(elution_adjusted_Cq = Cq + log2(Elution_volume / final_volume)) |&gt;\n  filter(\n    !is.na(Extraction_kit),\n    dilution == 1,\n  ) |&gt;\n  ggplot(mapping = aes(\n    x = elution_adjusted_Cq,\n    y = Extraction_kit,\n    color = replicate,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\")\n\nWarning: Removed 4 rows containing non-finite values (`stat_summary()`)."
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#standard-curves",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#standard-curves",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "Standard curves",
    "text": "Standard curves\n\ntidy_data |&gt;\n  filter(\n    Task == \"STANDARD\",\n    dilution == 1,\n  ) |&gt;\n  ggplot(mapping = aes(\n    x = quantity,\n    y = Cq,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(facets = ~Target, scales = \"free_x\")\n\nWarning: Removed 4 rows containing non-finite values (`stat_summary()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 4 rows containing non-finite values (`stat_smooth()`).\n\n\n\n\n\nNot sure what the units of the X-axis are\n\nfits &lt;- tibble()\nfor (target in unique(tidy_data$Target)) {\n  fit &lt;- lm(Cq ~ log10(quantity),\n    data = filter(tidy_data, Task == \"STANDARD\", Target == target)\n  ) |&gt;\n    tidy() |&gt;\n    mutate(Target = target, efficiency = 10^-(1 / estimate) - 1)\n  fits &lt;- bind_rows(fits, fit)\n}\nprint(fits |&gt; filter(term == \"log10(quantity)\"))\n\n# A tibble: 4 × 7\n  term            estimate std.error statistic  p.value Target efficiency\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 log10(quantity)    -2.61    0.143      -18.2 1.23e-10 Cov2         1.42\n2 log10(quantity)    -2.85    0.108      -26.4 5.34e-12 Noro         1.24\n3 log10(quantity)    -2.59    0.0703     -36.8 1.58e-14 CrA          1.43\n4 log10(quantity)    -2.68    0.148      -18.1 1.32e-10 16S          1.36\n\n\nThese match the Design & Analysis software."
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#dilution-series",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#dilution-series",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "Dilution series",
    "text": "Dilution series\n\ntidy_data |&gt;\n  filter(Task == \"UNKNOWN\", dilution &gt; 1) |&gt;\n  count(Target, Extraction_kit, dilution) |&gt;\n  print(n = Inf)\n\n# A tibble: 21 × 4\n   Target Extraction_kit                    dilution     n\n   &lt;chr&gt;  &lt;chr&gt;                                &lt;int&gt; &lt;int&gt;\n 1 Cov2   Invitrogen PureLink RNA                 10     2\n 2 Cov2   Invitrogen PureLink RNA                100     2\n 3 Cov2   QIAamp Viral RNA mini kit               10     2\n 4 Cov2   QIAamp Viral RNA mini kit              100     2\n 5 Cov2   Qiagen AllPrep PowerViral DNA/RNA       10     2\n 6 Cov2   Qiagen AllPrep PowerViral DNA/RNA      100     2\n 7 CrA    Invitrogen PureLink RNA                 10     2\n 8 CrA    Invitrogen PureLink RNA                100     2\n 9 CrA    QIAamp Viral RNA mini kit               10     2\n10 CrA    QIAamp Viral RNA mini kit              100     2\n11 CrA    Qiagen AllPrep PowerViral DNA/RNA       10     2\n12 CrA    Qiagen AllPrep PowerViral DNA/RNA      100     4\n13 CrA    Zymo quick-RNA                          10     2\n14 Noro   Invitrogen PureLink RNA                 10     1\n15 Noro   Invitrogen PureLink RNA                100     1\n16 Noro   QIAamp Viral RNA mini kit               10     1\n17 Noro   QIAamp Viral RNA mini kit              100     1\n18 Noro   Qiagen AllPrep PowerViral DNA/RNA       10     1\n19 Noro   Qiagen AllPrep PowerViral DNA/RNA      100     1\n20 Noro   Zymo quick-RNA                          10     1\n21 Noro   Zymo quick-RNA                         100     1\n\n\nLooks like we have dilution series for four kits.\n\ndilution_kits &lt;- c(\n  \"Invitrogen PureLink RNA\",\n  \"QIAamp Viral RNA mini kit\",\n  \"Qiagen AllPrep PowerViral DNA/RNA\",\n  \"Zymo quick-RNA\"\n)\ntidy_data |&gt;\n  filter(\n    Task == \"UNKNOWN\",\n    Target != \"16S\",\n    is.element(Extraction_kit, dilution_kits)\n  ) |&gt;\n  ggplot(mapping = aes(\n    x = dilution,\n    y = Cq,\n  )) +\n  geom_point(mapping = aes(color = replicate)) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(trans = \"log10\") +\n  facet_grid(cols = vars(Extraction_kit), rows = vars(Target), scale = \"free_y\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 13 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 13 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nfits &lt;- tibble()\nfor (target in unique(tidy_data$Target)) {\n  for (kit in dilution_kits) {\n    fit &lt;- lm(Cq ~ log10(dilution),\n      data = filter(tidy_data, Target == target, Extraction_kit == kit)\n    ) |&gt;\n      tidy() |&gt;\n      mutate(\n        Target = target,\n        Extraction_kit = kit,\n        # Changed sign because we have dilution factor not quantity\n        efficiency = 10^(1 / estimate) - 1\n      )\n    fits &lt;- bind_rows(fits, fit)\n  }\n}\nprint(fits |&gt; filter(Target != \"16S\", term == \"log10(dilution)\"))\n\n# A tibble: 12 × 8\n   term  estimate std.error statistic   p.value Target Extraction_kit efficiency\n   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;               &lt;dbl&gt;\n 1 log1…    NA      NA          NA    NA        Cov2   Invitrogen Pu…     NA    \n 2 log1…     2.51    0.869       2.88  2.35e- 2 Cov2   QIAamp Viral …      1.51 \n 3 log1…    NA      NA          NA    NA        Cov2   Qiagen AllPre…     NA    \n 4 log1…    NA      NA          NA    NA        Cov2   Zymo quick-RNA     NA    \n 5 log1…     2.92    0.0778     37.6   3.30e-11 Noro   Invitrogen Pu…      1.20 \n 6 log1…     3.07    0.151      20.3   7.80e- 9 Noro   QIAamp Viral …      1.12 \n 7 log1…     3.20    0.118      27.1   6.24e-10 Noro   Qiagen AllPre…      1.05 \n 8 log1…     2.69    0.0772     34.8   1.31e-12 Noro   Zymo quick-RNA      1.35 \n 9 log1…     3.59    0.205      17.5   2.24e- 9 CrA    Invitrogen Pu…      0.898\n10 log1…     3.58    0.0852     42.0   1.68e-13 CrA    QIAamp Viral …      0.902\n11 log1…     2.84    0.330       8.61  9.92e- 7 CrA    Qiagen AllPre…      1.25 \n12 log1…     5.14    0.644       7.98  2.25e- 5 CrA    Zymo quick-RNA      0.565"
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#amplification-curves",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#amplification-curves",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "Amplification curves",
    "text": "Amplification curves\n\namp_data &lt;- list.files(\n  paste0(data_dir, \"qpcr\"),\n  pattern = \"Amplification Data\",\n  full.names = TRUE,\n) |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    replicate = str_extract(Sample, \"[A-Z]\"),\n    Sample_ID = str_split_i(Sample, \"/\", 1),\n    dilution = as.integer(str_split_i(Sample, \"/\", 2)),\n    quantity = as.double(Sample),\n  ) |&gt;\n  mutate(dilution = replace_na(dilution, 1)) |&gt;\n  left_join(\n    metadata,\n    by = join_by(Sample_ID)\n  )\n\nWarning: The following named parsers don't match the column names: Cq\nThe following named parsers don't match the column names: Cq\nThe following named parsers don't match the column names: Cq\nThe following named parsers don't match the column names: Cq\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `quantity = as.double(Sample)`.\nCaused by warning:\n! NAs introduced by coercion\n\nprint(amp_data)\n\n# A tibble: 14,760 × 20\n    Well `Well Position` `Cycle Number` Target    Rn      dRn Sample Omit \n   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;lgl&gt;\n 1     1 A1                           1 Cov2   0.877  0.0311  1A     FALSE\n 2     1 A1                           2 Cov2   0.870  0.0262  1A     FALSE\n 3     1 A1                           3 Cov2   0.856  0.0145  1A     FALSE\n 4     1 A1                           4 Cov2   0.848  0.00816 1A     FALSE\n 5     1 A1                           5 Cov2   0.844  0.00638 1A     FALSE\n 6     1 A1                           6 Cov2   0.838  0.00299 1A     FALSE\n 7     1 A1                           7 Cov2   0.832 -0.00128 1A     FALSE\n 8     1 A1                           8 Cov2   0.829 -0.00202 1A     FALSE\n 9     1 A1                           9 Cov2   0.824 -0.00480 1A     FALSE\n10     1 A1                          10 Cov2   0.821 -0.00546 1A     FALSE\n# ℹ 14,750 more rows\n# ℹ 12 more variables: replicate &lt;chr&gt;, Sample_ID &lt;chr&gt;, dilution &lt;int&gt;,\n#   quantity &lt;dbl&gt;, Extraction_kit &lt;chr&gt;, Short_kit &lt;chr&gt;, `Kit Batch` &lt;dbl&gt;,\n#   Elution_volume &lt;dbl&gt;, Elution_format &lt;chr&gt;, Brand &lt;chr&gt;, NA_Target &lt;chr&gt;,\n#   FP_sampleID &lt;lgl&gt;\n\n\n\namp_data |&gt;\n  filter(!is.na(Extraction_kit)) |&gt;\n  ggplot(mapping = aes(\n    x = `Cycle Number`,\n    y = dRn,\n    color = as.factor(dilution),\n    group = Well\n  )) +\n  geom_line() +\n  facet_grid(\n    cols = vars(Extraction_kit), rows = vars(Target), scales = \"free_y\"\n  )"
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#background-fluorescence",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#background-fluorescence",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "Background fluorescence",
    "text": "Background fluorescence\n\nraw_qpcr_data &lt;- list.files(\n  paste0(data_dir, \"qpcr\"),\n  pattern = \"_Multicomponent_\",\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind() |&gt;\n  print()\n\n[1] \"/Users/dan/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/qpcr/2023-09-14_Cov2_extractions_Multicomponent_20230915_100200.csv\"\n[2] \"/Users/dan/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/qpcr/2023-09-14_Noro_Extractions_Multicomponent_20230915_100255.csv\"\n[3] \"/Users/dan/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/qpcr/2023-09-15_CrA_extractions_Multicomponent_20230915_105855.csv\" \n[4] \"/Users/dan/airport/[2023-09-06] Extraction-kit comparison 2: Settled Solids/qpcr/2023-09-18_16S_extractions_Multicomponent_20230918_151203.csv\" \n\n\nWarning: The following named parsers don't match the column names: Target, Cq\nThe following named parsers don't match the column names: Target, Cq\nThe following named parsers don't match the column names: Target, Cq\nThe following named parsers don't match the column names: Target, Cq\n\n\n# A tibble: 14,760 × 7\n    Well `Well Position` `Stage Number` `Step Number` `Cycle Number`     FAM\n   &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 A1                           2             2              1 326798.\n 2     1 A1                           2             2              2 323795.\n 3     1 A1                           2             2              3 318092 \n 4     1 A1                           2             2              4 314307.\n 5     1 A1                           2             2              5 312277.\n 6     1 A1                           2             2              6 309855.\n 7     1 A1                           2             2              7 307088.\n 8     1 A1                           2             2              8 305802.\n 9     1 A1                           2             2              9 303988.\n10     1 A1                           2             2             10 302549.\n# ℹ 14,750 more rows\n# ℹ 1 more variable: ROX &lt;dbl&gt;"
  },
  {
    "objectID": "notebooks/2023-09-15-qpcr_analysis.html#todo",
    "href": "notebooks/2023-09-15-qpcr_analysis.html#todo",
    "title": "2023-09-13 Extraction Experiment 2 qPCR Analysis",
    "section": "TODO",
    "text": "TODO\n\nBoxplots\nStandard curve data\nDiluted samples\nAmplification curves\nRelabel flipped norovirus dilution samples\nInvestigate the threshold\nInvestigate the efficiency caluculations: compare amp curves, standard curves, and dilution series\nCheck amplification curves, including autothreshold in linear phase. (Possibly adjust)\nCheck negative controls (may need to use raw Rns rather than delta to avoid baseline subtraction issues)\nBe aware of non-control samples with no amplification detected (no Cq values). Summary stats of which this is true for.\nPlot baseline start and stop (determines the interval used for baseline subtraction)\nBackground fluorescence"
  },
  {
    "objectID": "notebooks/2023-09-08_SequencingCosts.html",
    "href": "notebooks/2023-09-08_SequencingCosts.html",
    "title": "Airport experiment sequencing cost estimate",
    "section": "",
    "text": "We are planning to sequence samples from four sources (redacted because this is publicly visible). We will receive one sample from each source each day weekday for eight weeks. Thus we will have \\(8 \\times 5 \\times 4 = 160\\) samples total.\nWe will the following steps ourselves:\nThen we will deliver the RNA extracts to the BioMicroCenter for:\nThe goal of this doc is to estimate the cost of library prep and sequencing."
  },
  {
    "objectID": "notebooks/2023-09-08_SequencingCosts.html#sources",
    "href": "notebooks/2023-09-08_SequencingCosts.html#sources",
    "title": "Airport experiment sequencing cost estimate",
    "section": "Sources",
    "text": "Sources\n\nThe cost of library prep and sequencing come from the BioMicroCenter’s pricing page. We use the MIT prices.\nCharacteristics of the NovaSeq come from illumina (Table 1)."
  },
  {
    "objectID": "notebooks/2023-09-08_SequencingCosts.html#estimate",
    "href": "notebooks/2023-09-08_SequencingCosts.html#estimate",
    "title": "Airport experiment sequencing cost estimate",
    "section": "Estimate",
    "text": "Estimate\nEach NovaSeq flow cell has 4 lanes. When run in 2 x 150 bp mode, it generates 150 basepair forward and reverse read pairs for 300 bp per read pair. In one run, the flow cell generates 2400–3000 Gb of data. In the following we will make a conservative estimate by using the lower end of the range.\n\nlanes_per_flow_cell = 4\nbp_per_read_pair = 300\ngb_per_flow_cell = 2400\n\nread_pairs_per_flow_cell = gb_per_flow_cell * 1e9 / bp_per_read_pair\nread_pairs_per_lane = read_pairs_per_flow_cell / lanes_per_flow_cell\nprint(f\"We expect at least {read_pairs_per_lane/1e6} M read pairs per lane\")\n\nWe expect at least 2000.0 M read pairs per lane\n\n\nThe smallest unit of NovaSeq sequencing we can buy is one lane, which costs $5,940. We also pay $258.5 per sample for library preparation.\n\ncost_per_lane = 5940\nlibrary_cost_per_sample = 258.5\n\nWe’ll consider three options:\n\nSequencing all 160 samples in 4 lanes\nSequencing all 160 samples in 8 lanes\nSequencing MWF only (\\(160 \\times 3/5 = 96\\) samples) in 5 lanes\n\n\nfor num_samples, num_lanes in [(160, 4), (160, 8), (96, 5)]:\n    samples_per_lane = num_samples / num_lanes\n    read_pairs_per_sample = read_pairs_per_lane / samples_per_lane\n    sequencing_cost = cost_per_lane * num_lanes\n    library_cost = library_cost_per_sample * num_samples\n    total_cost = sequencing_cost + library_cost\n    cost_per_sample = total_cost / num_samples\n    cost_per_read_pair = cost_per_sample / read_pairs_per_sample\n    print(f\"\"\"\n    {num_samples} samples in {num_lanes} lanes:\n        Million read pairs per sample:\\t{read_pairs_per_sample / 1e6:3,.0f}\n        Sequencing cost:\\t${sequencing_cost:7,.0f}\n        Library prep cost:\\t${library_cost:7,.0f}\n        Total cost:\\t\\t\\t${total_cost:7,.0f}\n        Cost per sample:\\t${cost_per_sample:7,.0f}\n        Cost per M rp:\\t\\t${cost_per_read_pair * 1e6:10.2f}\n    \"\"\")\n\n\n    160 samples in 4 lanes:\n        Million read pairs per sample:   50\n        Sequencing cost:    $ 23,760\n        Library prep cost:  $ 41,360\n        Total cost:         $ 65,120\n        Cost per sample:    $    407\n        Cost per M rp:      $      8.14\n    \n\n    160 samples in 8 lanes:\n        Million read pairs per sample:  100\n        Sequencing cost:    $ 47,520\n        Library prep cost:  $ 41,360\n        Total cost:         $ 88,880\n        Cost per sample:    $    556\n        Cost per M rp:      $      5.56\n    \n\n    96 samples in 5 lanes:\n        Million read pairs per sample:  104\n        Sequencing cost:    $ 29,700\n        Library prep cost:  $ 24,816\n        Total cost:         $ 54,516\n        Cost per sample:    $    568\n        Cost per M rp:      $      5.45"
  },
  {
    "objectID": "notebooks/2024-02-08_OptimalSamplingInterval.html",
    "href": "notebooks/2024-02-08_OptimalSamplingInterval.html",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "",
    "text": "See previous notebook. The goal of this notebook is to use our simple, deterministic cost estimate to answer the question:\n\nHow often should we process and sequence samples?\n\nWe want to understand the tradeoff between:\n\ncatching the virus earlier by sampling more frequently, and\nsaving on processing costs by sampling less frequently.\n\nTo this end, we posit a two-component cost model:\n\nPer-read sequencing costs, and\nPer-sample processing costs\n\nand find the optimal sampling interval \\(\\delta t\\) that minimizes total costs, while sequencing to sufficient depth per sample \\(n\\) to detect a virus by cumulative incidence \\(\\hat{c}\\)."
  },
  {
    "objectID": "notebooks/2024-02-08_OptimalSamplingInterval.html#background",
    "href": "notebooks/2024-02-08_OptimalSamplingInterval.html#background",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "",
    "text": "See previous notebook. The goal of this notebook is to use our simple, deterministic cost estimate to answer the question:\n\nHow often should we process and sequence samples?\n\nWe want to understand the tradeoff between:\n\ncatching the virus earlier by sampling more frequently, and\nsaving on processing costs by sampling less frequently.\n\nTo this end, we posit a two-component cost model:\n\nPer-read sequencing costs, and\nPer-sample processing costs\n\nand find the optimal sampling interval \\(\\delta t\\) that minimizes total costs, while sequencing to sufficient depth per sample \\(n\\) to detect a virus by cumulative incidence \\(\\hat{c}\\)."
  },
  {
    "objectID": "notebooks/2024-02-08_OptimalSamplingInterval.html#a-two-component-cost-model",
    "href": "notebooks/2024-02-08_OptimalSamplingInterval.html#a-two-component-cost-model",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "A two-component cost model",
    "text": "A two-component cost model\nConsider the cost averaged over a long time interval \\(T\\) in which we will take many samples. If we collect and process samples every \\(\\delta t\\) days, we will take \\(T / \\delta t\\) samples in this interval. If we sample \\(n\\) reads per sample, our total sequencing depth is \\(n \\frac{\\delta t}{T}\\) reads. Assume that our costs can be divided into a per-sample cost \\(d_s\\) (including costs of collection, transportation, and processing for sequencing) and a per-read cost \\(d_r\\) of sequencing. (Note: the \\(d\\) is sort of awkward because we’ve already used \\(c\\) for “cumulative incidence”. You can think of it as standing for “dollars”.)\nWe will seek to minimize the total cost of detection: \\[\nd_{\\text{tot}} = d_s \\frac{T}{\\delta t} + d_r \\frac{nT}{\\delta t}.\n\\] Equivalently, we can divide by the arbitrary time-interval \\(T\\) to get the total rate of spending: \\[\n\\frac{d_{\\text{tot}}}{T} = \\frac{d_s}{\\delta t} + d_r \\frac{n}{\\delta t}.\n\\]\nIn our previous post, we found that the read depth per time required to detect a virus by the time it reaches cumulative incidence \\(\\hat{c}\\) is:\n\\[\n\\begin{align}\n\\frac{n}{\\delta t} & = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}}\\right) e^{r t_d} f(r \\delta t) \\\\\n                   & = A f(r \\delta t),\n\\end{align}\n\\]\nwhere the function \\(f\\) depends on the sampling scheme, and we’ve defined the constant \\(A\\) to contain all the terms that do not depend on \\(\\delta t\\). Substituting this into the rate of spending, we have: \\[\n\\frac{d_{\\text{tot}}}{T} = \\frac{d_s}{\\delta t} + A d_r f(r \\delta t).\n\\]\nIn the next section, we will find the value of \\(\\delta t\\) that minimizes the rate of spending.\n\nLimitations of the two-component model\n\nWe assume that we process each sample as it comes in. In practice, we could stockpile a set of \\(m\\) samples and process them simultaneously. This would require splitting out the cost of sampling from the cost of sample prep.\nWe do not consider the fact that sequencing (and presumably to some extent sample prep) unit costs decrease with greater depth. (I.e., it’s cheaper per-read to do bigger runs.)\nWe neglect the “batch” effects of sequencing. Typically you buy sequencing in units of “lanes” rather than asking for an arbitrary number of reads. This will introduce threshold effects, where we want to batach our samples to use lanes efficiently.\nWe do not account for fixed costs that accumulate per unit time regardless of our sampling and sequencing protocols. These do not affect the optimization here, but they do add to the total cost of the system."
  },
  {
    "objectID": "notebooks/2024-02-08_OptimalSamplingInterval.html#optimizing-the-sampling-interval",
    "href": "notebooks/2024-02-08_OptimalSamplingInterval.html#optimizing-the-sampling-interval",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "Optimizing the sampling interval",
    "text": "Optimizing the sampling interval\nTo find the optimal \\(\\delta t\\), we look for a zero of the derivative of spending rate:\n\\[\n\\begin{align}\n\\frac{d}{d \\delta t} \\frac{d_{\\text{tot}}}{T} & = - \\frac{d_s}{{\\delta t}^2} + A d_r r f'(r\\delta t).\n\\end{align}\n\\]\nSetting the right-hand side equal to zero and rearranging gives:\n\\[\n{(r \\delta t)}^2 f'(r \\delta t) = \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}\n\\]\nTo get any farther, we need to specify \\(f\\) and therefore a sampling scheme. [Note: If we give some general properties of \\(f\\), we can say some things here that are general to the sampling scheme]\n\nGrab sampling\nWe first consider grab sampling, where the entire sample is collected at the sampling time. In that case, we have: \\[\n\\begin{align}\nf(x) & = \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & = 1 + \\frac{x^2}{12} + \\mathcal{O}(x^3).\n\\end{align}\n\\] We are particularly interested in the small-\\(x\\) regime: The depth required becomes exponentially large when \\(r \\delta t \\gg 1\\), so it is likely that the optimal interval satisfies \\(r \\delta t \\lesssim 1\\). We can check this for self-consistency in any specific numerical examples.\nThis gives us the derivative: \\[\nf'(x) \\approx \\frac{x}{6}.\n\\]\nUsing this in our optimization equation yields: \\[\n{(r \\delta t)}^3 \\approx 6 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}.\n\\]\n\n\nContinuous sampling\nIn the case of continuous sampling, where the sample taken at time \\(t\\) is a composite sample uniformly collected over the interval \\([t - \\delta t, t)\\), we have: \\[\n\\begin{align}\nf(x) & = \\frac{e^x - 1}{x} \\\\\n     & = 1 + \\frac{x}{2} + \\mathcal{O}(x^2) \\\\\nf'(x) & \\approx \\frac{1}{2}\n\\end{align}\n\\] for small \\(x\\). Note the difference in functional form from the grab sample case.\nSubstituting this into the optimization equation yields: \\[\n{(r \\delta t)}^2 \\approx 2 \\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d}.\n\\]\n\n\nWindowed composite sampling\nAn intermediate (and more realistic) model of sampling is windowed composite sampling. In this scheme, the sample at time \\(t\\) is a composite sample taken over a window of width \\(w\\) (e.g., 24hours) from \\(t - w\\) to \\(t\\). Notably, when the sampling interval (\\(\\delta t\\)) increases, the length of the window does not. In this case we have [TODO: add derivation to other notebook]:\n\\[\n\\begin{align}\nf(x) & = \\frac{rw}{1 - e^{-rw}} \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & \\approx \\left(1 + \\frac{rw}{2}\\right) \\frac{e^{-x}{(e^x - 1)}^2}{x^2} \\\\\n     & \\approx \\left(1 + \\frac{rw}{2}\\right) + \\left(1 + \\frac{rw}{2}\\right) \\left(\\frac{x^2}{12}\\right) \\\\\nf'(x) & \\approx \\left(1 + \\frac{rw}{2}\\right) \\left(\\frac{x}{6}\\right) \\\\\n\\end{align}\n\\] for small \\(rw\\) and \\(x\\). Note that as \\(rw \\to 0\\), we recover grab sampling.\nSince we’re keeping only the leading term in \\(x = r \\delta t\\), and \\(w \\leq \\delta t\\), for consistency we should also drop the \\(\\frac{rw}{2}\\) (or keep more terms of the expansion). Thus, we’ll treat windowed composite sampling for small windows as equivalent to grab sampling. The key reason for this is that changing the sampling interval does not change the window. Note that for \\(\\delta t \\approx w\\), i.e. \\(x \\approx rw\\), \\(f(x) \\approx 1 + \\frac{rw}{2}\\), just as with continuous sampling, but \\(f'(x)\\) still behaves like grab sampling.\n\n\nGeneral properties\nIn general, we have: \\[\nr \\delta t \\approx {\\left( a\\frac{d_s}{d_r} \\frac{b \\hat{c}}{\\hat{K}} \\left(\\frac{r}{r + \\beta}\\right) e^{-r t_d} \\right)}^{1/b},\n\\] where \\(a\\) and \\(b\\) are positive constants that depend on the sampling scheme. We can observe some general features:\n\nFaster-growing viruses (higher \\(r\\)) decreases the optimal sampling interval.\nIncreasing the cost per sample \\(d_s\\) increases the optimal sampling interval.\nIncreasing the cost per read \\(d_r\\) decreases the optimal sampling interval.\nIncreasing the P2RA factor \\(b\\) or the target cumulative incidence \\(c\\) increases the optimal sampling interval.\nIncreasing the detection threshold \\(\\hat{K}\\) decreases the optimal sampling interval.\nIncreasing the delay between sampling and detection \\(t_d\\) decreases the optimal sampling interval.\n\nOne general trend is: the more optimistic we are about our method (higher \\(b\\), smaller \\(\\hat{K}\\), shorter \\(t_d\\)), the longer we can wait between samples."
  },
  {
    "objectID": "notebooks/2024-02-08_OptimalSamplingInterval.html#a-numerical-example",
    "href": "notebooks/2024-02-08_OptimalSamplingInterval.html#a-numerical-example",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "A numerical example",
    "text": "A numerical example\n\nOptimal \\(\\delta t\\)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Optional\n\n\ndef optimal_interval(\n    per_sample_cost: float,\n    per_read_cost: float,\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_scheme: str,\n    delay: float,\n    composite_window: Optional[float] = None,\n) -&gt; float:\n    constant_term = (\n        (per_sample_cost / per_read_cost)\n        * ((p2ra_factor * cumulative_incidence_target) / read_threshold)\n        * (growth_rate / (growth_rate + recovery_rate))\n        * np.exp(-growth_rate * delay)\n    )\n    if sampling_scheme == \"continuous\":\n        a = 2\n        b = 2\n    elif sampling_scheme == \"grab\":\n        a = 6\n        b = 3\n    elif sampling_scheme == \"composite\":\n        if not composite_window:\n            raise ValueError(\"For composite sampling, must provide a composite_window\")\n        a = (\n            6\n            * (1 - np.exp(-growth_rate * composite_window))\n            / (growth_rate * composite_window)\n        )\n        b = 3\n    else:\n        raise ValueError(\"sampling_scheme must be continuous or grab\")\n    return (a * constant_term) ** (1 / b) / growth_rate\n\n\nI asked the NAO in Twist for info on sequencing and sample-processing costs. Based on their answers, a reasonable order-of-magnitude cost estimate is $500. As discussed above, our cost model is highly simplified and the specifics of when samples are collected, transported, processed, and batched for sequencing will make this calculation much more complicated in practice.\nLet’s use these numbers plus our virus model from the last post to find the optimal sampling interval:\n\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Weekly doubling\nr = np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n# Delay from sampling to detecting of 1 week\nt_d = 7.0\n\ndelta_t_grab = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\ndelta_t_cont = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"continuous\", t_d)\ndelta_t_24hr = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1)\n\nprint(f\"Optimal sampling interval with grab sampling:\\t\\t{delta_t_grab:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_grab:.2f}\")\nprint(f\"Optimal sampling interval with continuous sampling:\\t{delta_t_cont:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_cont:.2f}\")\nprint(\n    f\"Optimal sampling interval with 24-hour composite sampling:\\t{delta_t_24hr:.2f} days\"\n)\nprint(f\"\\tr delta_t = {r*delta_t_24hr:.2f}\")\n\nOptimal sampling interval with grab sampling:       5.98 days\n    r delta_t = 0.59\nOptimal sampling interval with continuous sampling: 2.66 days\n    r delta_t = 0.26\nOptimal sampling interval with 24-hour composite sampling:  5.89 days\n    r delta_t = 0.58\n\n\nWe should check that \\(r \\delta_t\\) is small enough that our approximation for \\(f(x)\\) is accurate:\n\n\nCode\nx = np.arange(0.01, 3, 0.01)\nplt.plot(x, np.exp(-x) * ((np.exp(x) - 1) / x) ** 2, label=\"exact\")\nplt.plot(x, 1 + x**2 / 12, label=\"approx\")\nplt.ylim([0, 2])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Grab/24hr-composite sampling\")\nplt.show()\n\nplt.plot(x, (np.exp(x) - 1) / x, label=\"exact\")\nplt.plot(x, 1 + x / 2, label=\"approx\")\nplt.ylim([0, 5])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Continuous sampling\")\nplt.show()\n\n\n\n\n\n\n\n\nLooks fine in both cases.\n\n\nCost sensitivity to \\(\\delta t\\)\nIn a real system, we won’t be able to optimize \\(\\delta t\\) exactly. Let’s see how the cost varies with the sampling interval:\n\n\nCode\ndef depth_required(\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_interval: float,\n    sampling_scheme: str,\n    delay: float,\n    composite_window: Optional[float] = None,\n) -&gt; float:\n    leading_term = (\n        (growth_rate + recovery_rate)\n        * read_threshold\n        / (p2ra_factor * cumulative_incidence_target)\n    )\n    x = growth_rate * sampling_interval\n    if sampling_scheme == \"continuous\":\n        sampling_term = (np.exp(x) - 1) / x\n    elif sampling_scheme == \"grab\":\n        sampling_term = np.exp(-x) * ((np.exp(x) - 1) / x) ** 2\n    elif sampling_scheme == \"composite\":\n        if not composite_window:\n            raise ValueError(\"For composite sampling, must provide a composite_window\")\n        rw = growth_rate * composite_window\n        sampling_term = (\n            (rw / (1 - np.exp(-rw))) * np.exp(-x) * ((np.exp(x) - 1) / x) ** 2\n        )\n    else:\n        raise ValueError(\"sampling_scheme must be continuous, grab, or composite\")\n    delay_term = np.exp(growth_rate * delay)\n    return leading_term * sampling_term * delay_term\n\n\ndef cost_per_time(\n    per_sample_cost: float,\n    per_read_cost: float,\n    sampling_interval: float,\n    sample_depth_per_time: float,\n) -&gt; float:\n    return sample_cost_per_time(per_sample_cost, sampling_interval) + seq_cost_per_time(\n        per_read_cost, sample_depth_per_time\n    )\n\n\ndef sample_cost_per_time(per_sample_cost, sampling_interval):\n    return per_sample_cost / sampling_interval\n\n\ndef seq_cost_per_time(per_read_cost, sample_depth_per_time):\n    return per_read_cost * sample_depth_per_time\n\n\n\n\nCode\ndelta_t = np.arange(1.0, 21, 1)\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nn_24hr = depth_required(\n    r, beta, k_hat, b, c_hat, delta_t, \"composite\", t_d, composite_window=1.0\n)\ncost_cont = cost_per_time(d_s, d_r, delta_t, n_cont)\ncost_grab = cost_per_time(d_s, d_r, delta_t, n_grab)\ncost_24hr = cost_per_time(d_s, d_r, delta_t, n_24hr)\nplt.plot(delta_t, cost_cont, label=\"continuous\")\nplt.plot(delta_t, cost_grab, label=\"grab\")\nplt.plot(delta_t, cost_24hr, label=\"24hr composite\")\nplt.ylim([0, 5000])\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend()\n\n\n\n\n\nFirst, note that the cost of 24hr composite sampling is quite close to grab sampling, and that when the sampling interval is 1 day, it is exactly the same as continuous sampling.\nIt looks like the cost curve is pretty flat for the grab/24hr sampling, suggesting that we could choose a range of sampling intervals without dramatically increasing the cost. For continuous sampling, the cost increases more steeply with increasing sampling interval.\nFinally, let’s break the costs down between sampling and sequencing:\n\n\nCode\nplt.plot(delta_t, cost_grab, label=\"Total\")\nplt.plot(delta_t, sample_cost_per_time(d_s, delta_t), label=\"Sampling\")\nplt.plot(delta_t, seq_cost_per_time(d_r, n_grab), label=\"Sequencing\")\nplt.legend()\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.title(\"Grab sampling\")\n\n\n\n\n\nWe can observe a few things:\n\nSequencing costs are always quite a bit higher than sampling costs.\nIncreasing the sampling interval from one day to about five generates a significant savings in sampling cost, any longer than that gives strongly diminishing returns. (This makes sense from the functional form \\(d_s / \\delta t\\).)\nThe required sequencing depth increases slowly in this range.\n\n\n\nSensitivity of optimal \\(\\delta t\\) to P2RA factor\nWe have a lot of uncertainty in the P2RA factor, even for a specific known virus with a fixed protocol. Let’s see how the optimal sampling interval varies with it. (We’ll only do this for grab sampling.)\n\n\nCode\nra_i_01 = np.logspace(-9, -6, 100)\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n\ndelta_t_opt = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\n\nplt.semilogx(ra_i_01, delta_t_opt)\nplt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\nplt.ylabel(\"Optimal sampling interval, $\\delta t$\")\nplt.ylim([0, 13])\n\n\n\n\n\nAs expected, the theory predicts that with higher P2RA factors, we can get away with wider sampling intervals. Also, for this range of P2RA factors, it never recommends daily sampling.\nHowever, we can also see that the cost per day depends much more strongly on the P2RA factor than on optimizing the sampling interval:\n\n\nCode\ndelta_t = np.arange(1.0, 21, 1)\nfor ra_i_01 in [1e-8, 1e-7, 1e-6]:\n    b = ra_i_01 * 100 * (r + beta) * 7\n    n = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\n    cost = cost_per_time(d_s, d_r, delta_t, n)\n    plt.plot(delta_t, cost, label=f\"{ra_i_01}\")\nplt.yscale(\"log\")\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend(title=r\"$RA_i(1\\%)$\")"
  },
  {
    "objectID": "notebooks/2024-02-08_OptimalSamplingInterval.html#a-second-example-faster-growth-and-longer-delay",
    "href": "notebooks/2024-02-08_OptimalSamplingInterval.html#a-second-example-faster-growth-and-longer-delay",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "A second example: Faster growth and longer delay",
    "text": "A second example: Faster growth and longer delay\nLet’s consider a more pessimistic scenario: doubling both the growth rate and the delay to detection.\n\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Twice-weekly doubling\nr = 2 * np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n# Delay from sampling to detecting of 2 weeks\nt_d = 14.0\n\ndelta_t_grab = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\ndelta_t_cont = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"continuous\", t_d)\ndelta_t_24hr = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1)\n\nprint(f\"Optimal sampling interval with grab sampling:\\t\\t{delta_t_grab:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_grab:.2f}\")\nprint(f\"Optimal sampling interval with continuous sampling:\\t{delta_t_cont:.2f} days\")\nprint(f\"\\tr delta_t = {r*delta_t_cont:.2f}\")\nprint(\n    f\"Optimal sampling interval with 24-hour composite sampling:\\t{delta_t_24hr:.2f} days\"\n)\nprint(f\"\\tr delta_t = {r*delta_t_24hr:.2f}\")\n\nOptimal sampling interval with grab sampling:       1.88 days\n    r delta_t = 0.37\nOptimal sampling interval with continuous sampling: 0.66 days\n    r delta_t = 0.13\nOptimal sampling interval with 24-hour composite sampling:  1.82 days\n    r delta_t = 0.36\n\n\nWe should check that \\(r \\delta_t\\) is small enough that our approximation for \\(f(x)\\) is accurate:\n\n\nCode\nx = np.arange(0.01, 3, 0.01)\nplt.plot(x, np.exp(-x) * ((np.exp(x) - 1) / x) ** 2, label=\"exact\")\nplt.plot(x, 1 + x**2 / 12, label=\"approx\")\nplt.ylim([0, 2])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Grab sampling\")\nplt.show()\n\nplt.plot(x, (np.exp(x) - 1) / x, label=\"exact\")\nplt.plot(x, 1 + x / 2, label=\"approx\")\nplt.ylim([0, 5])\nplt.legend()\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$f(x)$\")\nplt.title(\"Continuous sampling\")\nplt.show()\n\n\n\n\n\n\n\n\nLooks fine in both cases.\n\nCost sensitivity to \\(\\delta t\\)\nIn a real system, we won’t be able to optimize \\(\\delta t\\) exactly. Let’s see how the cost varies with the sampling interval:\n\n\nCode\ndelta_t = np.arange(1.0, 21, 1)\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nn_24hr = depth_required(\n    r, beta, k_hat, b, c_hat, delta_t, \"composite\", t_d, composite_window=1.0\n)\ncost_cont = cost_per_time(d_s, d_r, delta_t, n_cont)\ncost_grab = cost_per_time(d_s, d_r, delta_t, n_grab)\ncost_24hr = cost_per_time(d_s, d_r, delta_t, n_24hr)\nplt.plot(delta_t, cost_cont, label=\"continuous\")\nplt.plot(delta_t, cost_grab, label=\"grab\")\nplt.plot(delta_t, cost_24hr, label=\"24hr composite\")\nplt.ylim([0, 100000])\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend()\n\n\n\n\n\nIt looks like the cost curve is pretty flat for the grab sampling, suggesting that we could choose a range of sampling intervals without dramatically increasing the cost. For continuous sampling, the cost increases more steeply with increasing sampling interval.\nFinally, let’s break the costs down between sampling and sequencing:\n\n\nCode\nplt.plot(delta_t, cost_grab, label=\"Total\")\nplt.plot(delta_t, sample_cost_per_time(d_s, delta_t), label=\"Sampling\")\nplt.plot(delta_t, seq_cost_per_time(d_r, n_grab), label=\"Sequencing\")\nplt.legend()\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.title(\"Grab sampling\")\n\n\n\n\n\nIn this faster growth + more delay example, sequencing costs completely dwarf sampling costs.\n\n\nSensitivity of optimal \\(\\delta t\\) to P2RA factor\n\n\nCode\nra_i_01 = np.logspace(-9, -6, 100)\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n\ndelta_t_opt = optimal_interval(d_s, d_r, r, beta, k_hat, b, c_hat, \"grab\", t_d)\n\nplt.semilogx(ra_i_01, delta_t_opt)\nplt.xlabel(\"P2RA factor, $RA_i(1\\%)$\")\nplt.ylabel(\"Optimal sampling interval, $\\delta t$\")\nplt.ylim([0, 5])\n\n\n\n\n\nIn this case, daily sampling is sometimes favored when the P2RA factor is small enough.\nHowever, we can also see that the cost per day depends much more strongly on the P2RA factor than on optimizing the sampling interval:\n\n\nCode\ndelta_t = np.arange(1.0, 21, 1)\nfor ra_i_01 in [1e-8, 1e-7, 1e-6]:\n    b = ra_i_01 * 100 * (r + beta) * 7\n    n = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\n    cost = cost_per_time(d_s, d_r, delta_t, n)\n    plt.plot(delta_t, cost, label=f\"{ra_i_01}\")\nplt.yscale(\"log\")\nplt.ylabel(\"Cost per day\")\nplt.xlabel(r\"Sampling interval $\\delta t$\")\nplt.legend(title=r\"$RA_i(1\\%)$\")"
  },
  {
    "objectID": "notebooks/2024-02-08_OptimalSamplingInterval.html#cost-sensitivity-to-the-latency-t_d",
    "href": "notebooks/2024-02-08_OptimalSamplingInterval.html#cost-sensitivity-to-the-latency-t_d",
    "title": "NAO Cost Estimate MVP – Optimizing the sampling interval",
    "section": "Cost sensitivity to the latency, \\(t_d\\)",
    "text": "Cost sensitivity to the latency, \\(t_d\\)\nAs a final application, let’s calculate what the optimal cost would be as a function of delay/latency time \\(t_d\\). We’ll use 24-hr composite sampling. And for some realism, we’ll round the optimal sampling interval to the nearest day.\n\nd_s = 500\nd_r = 5000 * 1e-9\n\n# Bi-weekly doubling\nr = 2 * np.log(2) / 7\n# Recovery in two weeks\nbeta = 1 / 14\n# Detect when 100 cumulative reads\nk_hat = 100\n# Median P2RA factor for SARS-CoV-2 in Rothman\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\n# Goal of detecting by 1% cumulative incidence\nc_hat = 0.01\n\n\n\nCode\nt_d = np.arange(1.0, 22.0, 1.0)\ndelta_t_opt = optimal_interval(\n    d_s, d_r, r, beta, k_hat, b, c_hat, \"composite\", t_d, 1.0\n)\ndelta_t_round = np.round(delta_t_opt)\nn = depth_required(r, beta, k_hat, b, c_hat, delta_t_round, \"composite\", t_d, 1.0)\ncost = cost_per_time(d_s, d_r, delta_t_round, n)\n\nplt.plot(t_d, delta_t_round, \"o\")\nplt.ylim([0, 5])\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(r\"Optimal sampling interval $\\delta t$ (days)\")\nplt.show()\n\n\n\n\n\nShorter latency means that we can sample less often.\n\n\nCode\nplt.plot(t_d, n, \"o\")\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(\"Depth per day (reads)\")\nplt.show()\n\n\n\n\n\nLonger latency means that we have to sequence exponentially more reads per day. This leads to exponentially higher costs:\n\n\nCode\nplt.plot(t_d, cost, \"o\")\nplt.xlabel(r\"Latency $t_d$ (days)\")\nplt.ylabel(\"Cost per day (dollars)\")\nplt.show()"
  },
  {
    "objectID": "notebooks/2023-08-18_SimplePrevalence.html",
    "href": "notebooks/2023-08-18_SimplePrevalence.html",
    "title": "Simple deterministic model of local and global prevalence",
    "section": "",
    "text": "This is building on Mike’s notes. The objective is to have a very simple deterministic model of an exponentially-growing virus spreading from somewhere else in the world to a monitored city. The main difference from Mike’s notes is that our flight model conserves the number of people who are infected.\nSome assumptions:"
  },
  {
    "objectID": "notebooks/2023-08-18_SimplePrevalence.html#dynamics",
    "href": "notebooks/2023-08-18_SimplePrevalence.html#dynamics",
    "title": "Simple deterministic model of local and global prevalence",
    "section": "Dynamics",
    "text": "Dynamics\nWe’ll use the variable \\(P\\) to be the absolute prevalence, i.e. the raw number of people infected, and \\(p\\) to be the relative prevalence, i.e., the fraction of a given population that is infected. The subscript \\(l\\) refers to the local population of the monitored city and \\(nl\\) refers to the non-local population. We use the subscript \\(g\\) to refer to the total. So \\(P_{g}\\) is the total number of people currently infected.\nParameters:\n\n\\(r\\), the exponential growth rate per day of the virus\n\\(f_{in}\\), the rate per person per day of flying to the focal city\n\\(f_{out}\\), the rate per person per day of flying out of the focal city\n\\(N_l\\), \\(N_{nl}\\), \\(N_g\\), the local, non-local and global population sizes\n\nOur model is a pair of ODEs (dots represent time derivatives):\n\\[\n\\dot{P}_{nl} = (r - f_{in}) P_{nl} + f_{out} P_l\n\\]\n\\[\n\\dot{P}_{l} = f_{in} P_{nl} + (r - f_{out}) P_l\n\\]\nwith initial conditions \\(P_{nl}(0) = 1\\) and \\(P_{l}(0) = 0\\).\n(An aside about the initial conditions. While it’s reasonable to model a virus that starts by infecting one individual, it is not accurate to extend the deterministic model to the earliest stages of the pandemic. In particular, the early stages will be both noisy and superexponential because conditional on not going extinct the prevalence has to grow quickly to get away from the zero bound. In the medium-term – after stochastic effects dominate and before saturation sets in – the prevalence will grow approximately exponentially. You can think of this model as extrapolating that regime backwards in time to an “effective time zero”. One thing to check is whether this causes any problems for the local dynamics.)\nNote that our flight model conserves the total prevalence: the rate of infected individuals flying from global to local is exactly equal to the reverse rate. Thus, we have exponential growth of the global prevalence:\n\\[\nP_{g} \\equiv P_{nl} + P_{l}\n\\]\n\\[\n\\dot{P}_{g} = r P_{g}\n\\]\nWe have found one eigenvector of the system of ODEs. The other takes on a natural meaning if we make a further assumption about the rates of flights. We assume that the rate of flying to the focal city is proportional to its size. In particular, we set \\(N_{nl} f_{in} = N_l f_{out}\\). With this assumption, some algebra shows that the second eigenvector is the difference between the non-local and local prevalence:\n\\[\n\\Delta p \\equiv p_{nl} - p_l\n\\]\n\\[\n\\dot{\\Delta p} = (r - F) \\Delta p\n\\]\nwhere \\(F \\equiv f_{in} + f_{out}\\). Note that if \\(N_{l} \\ll N_{nl}\\) then \\(F \\approx f_{out}\\), the rate at which people fly from the focal city every day.\nThis equation shows that there are two regimes:\n\nIn the slow-growth regime: \\(r &lt; F\\), \\(\\Delta p\\) shrinks exponentially at rate \\(F - r\\). Mixing via air travel closes the gap between the local and non-local prevalence.\nIn the fast-growth regime: \\(r &gt; F\\), \\(\\Delta p\\) grows exponentially at rate \\(r - F\\). The local prevalence will never catch up with the non-local prevalence until saturation effects slow the non-local spread (which is outside the scope of this model).\n\nIn the slow-growth regime, there’s no intrinsic advantage to monitoring air travelers (aside from whatever sample properties like fewer non-human contributions to the wastewater) because the virus gets established locally before it reaches high prevalence globally. Of course this conclusion depends on our simple model: having a more detailed model of the flight network may suggest that there are particular places it would be good to monitor. Also, stochastic effects may matter a lot in this regime, because it relies on establishment of the virus locally from a small number of introductions.\nIn the fast-growth regime, there may be a significant advantage to monitoring air travelers if it’s possible to catch the virus while it’s in it’s exponential phase globally. We would need a non-linear model with saturation effects (e.g. Charlie’s) to estimate the advantage if we can’t catch it while it’s growing exponentially."
  },
  {
    "objectID": "notebooks/2023-08-18_SimplePrevalence.html#monitoring-in-the-fast-growth-regime",
    "href": "notebooks/2023-08-18_SimplePrevalence.html#monitoring-in-the-fast-growth-regime",
    "title": "Simple deterministic model of local and global prevalence",
    "section": "Monitoring in the fast-growth regime",
    "text": "Monitoring in the fast-growth regime\nCharlie estimates that \\(F \\approx 1 / 300\\) (a person flies on average once every 300 days). This paper says that the doubling time of SARS-CoV-2 in the US before mitigation efforts was 2.68 days (\\(r = 0.26\\)). Thus, for a covid-like spread, we might expect \\(r / F \\sim 80\\). In this section, we consider monitoring for such a pathogen in the fast-growth regime where \\(r \\gg F\\).\nSolving our differential equations, we have:\n\\[\np_g = \\frac{1}{N_g} e^{rt}\n\\]\n\\[\np_l = \\frac{1}{N_l} \\frac{f_{in}}{F} \\left( 1 - e^{-Ft} \\right) e^{r t}\n    \\approx \\frac{1}{N_g} \\left( 1 - e^{-Ft} \\right) e^{r t}\n\\]\nThe global population is 8 billion people, so we can get a crude upper bound on the time our model will be valid for by solving for when exponential growth would infect everyone:\n\n\nCode\nimport numpy as np\ndoubling_time = 2.68\nr = np.log(2) / doubling_time\nn_g = 8e9\nsaturation_time = np.log(n_g) / r\nprint(f\"Saturation time: {saturation_time:0.2f} days\")\n\n\nSaturation time: 88.16 days\n\n\nThis is several times shorter than the mixing time \\(1 / F\\), so it’s safe to simplify our equation to:\n\\[\np_l \\approx \\frac{1}{N_g} Ft e^{r t}\n\\]\nSo in short times the ratio \\[\n\\frac{p_l}{p_g} \\approx Ft\n\\]\n\nCumulative reads\nWe assume that the cumulative reads are proportional to the time integral of the prevalence:\n\\[\n\\int_0^t p_g dt = \\frac{1}{rN_g} (e^{rt} - 1) \\approx \\frac{1}{rN_g} e^{rt}\n\\]\n\\[\n\\int_0^t p_l dt = \\frac{F}{r^2N_g} \\left(e^{rt} (rt - 1) + 1\\right)\n\\approx \\frac{1}{rN_g} Ft e^{rt}\n\\]\nTo be continued…"
  },
  {
    "objectID": "notebooks/2023-08-18_SimplePrevalence.html#scratch-disregard",
    "href": "notebooks/2023-08-18_SimplePrevalence.html#scratch-disregard",
    "title": "Simple deterministic model of local and global prevalence",
    "section": "Scratch (disregard)",
    "text": "Scratch (disregard)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef integral_global(t, r, n):\n    return (np.exp(r * t) - 1) / (n * r)\n\ndef integral_local(t, r, n, f):\n    return integral_global(t, r, n) - integral_global(t, r - f, n)\n\n\n\n\nCode\nrs = [1 / 250, 1 / 800]\nfs = [1 / 300, 1 / 300]\nn = 10\nt_max = 3200\nt = np.linspace(1, t_max, t_max)\nfig, axes = plt.subplots(1, len(rs), sharey=True)\nfor r, f, ax in zip(rs, fs, axes):\n    ax.semilogy(t, integral_global(t, r, n))\n    ax.semilogy(t, integral_local(t, r, n, f))\naxes[0].set_ylim([1, 1e4])\nplt.show()\n\n\n\n\n\nFigure 1: test\n\n\n\n\n\n\nCode\nr = 1 / 60\nf = 1 / 150\nn = 10\nt_max = 300\nt = np.linspace(1, t_max, t_max)\nfig = plt.figure(figsize=(5,5))\nax = fig.add_subplot()\nax.semilogy(t, integral_global(t, r, n), label=\"Airport\")\nax.semilogy(t, integral_local(t, r, n, f), label=\"WWTP\")\nax.legend(frameon=False)\nylim = [1, 1e3]\nax.set_ylim(ylim)\nax.hlines([50], 0, 300, linestyle=\"dashed\", color=\"grey\")\nax.vlines(133, *ylim, linestyle=\"dotted\", color=\"C0\")\nax.vlines(169, *ylim, linestyle=\"dotted\", color=\"C1\")\nax.set_xlabel(\"Days since start of pandemic\")\nax.set_ylabel(\"Total reads matching virus\")\nax.text(0, 55, \"Detection threshold\", color=\"grey\")\nax.text(131, 1.1, \"Detection\\nin airport\", color=\"C0\", ha=\"right\")\n# ax.text(270, 600, \"Airport reads\", color=\"C0\", ha=\"right\")\nax.text(171, 1.1, \"Detection\\nin WWTP\", color=\"C1\")\n# ax.text(215, 100, \"WWTP reads\", color=\"C1\")\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\nplt.show()\n\n\n\n\n\nFigure 2: test\n\n\n\n\n\n\nCode\nr = 0.259\nf = 1 / 300\nn = 1e8\nt_max = 300\nt = np.linspace(1, t_max, t_max)\nfig = plt.figure(figsize=(5,5))\nax = fig.add_subplot()\nax.semilogy(t, integral_global(t, r, n), label=\"Airport\")\nax.semilogy(t, integral_local(t, r, n, f), label=\"WWTP\")\nax.legend(frameon=False)\n# ylim = [1, 1e3]\n# ax.set_ylim(ylim)\nax.hlines([50], 0, 300, linestyle=\"dashed\", color=\"grey\")\nax.vlines(133, *ylim, linestyle=\"dotted\", color=\"C0\")\nax.vlines(169, *ylim, linestyle=\"dotted\", color=\"C1\")\nax.set_xlabel(\"Days since start of pandemic\")\nax.set_ylabel(\"Total reads matching virus\")\nax.text(0, 55, \"Detection threshold\", color=\"grey\")\nax.text(131, 1.1, \"Detection\\nin airport\", color=\"C0\", ha=\"right\")\nax.text(171, 1.1, \"Detection\\nin WWTP\", color=\"C1\")\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\nplt.show()\n\n\n\n\n\ntest"
  },
  {
    "objectID": "notebooks/2024-02-02_CostEstimateMVP.html",
    "href": "notebooks/2024-02-02_CostEstimateMVP.html",
    "title": "NAO Cost Estimate MVP",
    "section": "",
    "text": "See Google Doc for background. See also Simple detection cost model for P2RA version."
  },
  {
    "objectID": "notebooks/2024-02-02_CostEstimateMVP.html#background",
    "href": "notebooks/2024-02-02_CostEstimateMVP.html#background",
    "title": "NAO Cost Estimate MVP",
    "section": "",
    "text": "See Google Doc for background. See also Simple detection cost model for P2RA version."
  },
  {
    "objectID": "notebooks/2024-02-02_CostEstimateMVP.html#epidemic-model",
    "href": "notebooks/2024-02-02_CostEstimateMVP.html#epidemic-model",
    "title": "NAO Cost Estimate MVP",
    "section": "Epidemic model",
    "text": "Epidemic model\n\nPrevalence and incidence\nConsider a population of \\(N\\) individuals. We focus on the phase of the epidemic when it is growing exponentially. This will be approximately true at an intermediate time when the virus is common enough that we can neglect noise in reproduction, but rare enough that it is not running out of susceptible people to infect. Let the rate of new infections per interaction per unit time to be \\(\\alpha\\) and the rate of recovery of an infected person to be \\(\\beta\\). Then, we have the following ODE for the prevalence \\(P(t)\\):\n\\[\n\\frac{dP}{dt} \\approx \\alpha N P - \\beta P.\n\\]\nThe first term on the right-hand side is the incidence per unit time: \\(I \\approx \\alpha N P\\). (Note that we are assuming that the number of susceptible people is approximately \\(N\\), which will cease to be true as the virus becomes common.)\nSolving this equation and choosing \\(t = 0\\) to be the time when one person is infected gives: \\[\n\\begin{align}\nP(t) & = e^{(\\alpha N - \\beta) t} \\\\\n     & = e^{rt} \\\\\nI(t) & = \\alpha N e^{rt} \\\\\n     & = (r + \\beta) e^{rt},\n\\end{align}\n\\] where we have defined the exponential growth rate \\(r \\equiv \\alpha N - \\beta\\).\n\n\nCumulative incidence\nAs a yardstick for measuring the progress of the epidemic, we are interested in the cumulative incidence, \\(C(t)\\), the number of people who have ever been infected. Integrating \\(I(t)\\) with the initial condition \\(C(0) = 1\\) to account for the single individual infected at \\(t = 0\\), we have: \\[\n\\begin{align}\nC(t) & = 1 + \\int_0^t I(t') dt' \\\\\n     & = 1 + \\frac{r + \\beta}{r} (e^{rt} - 1).\n\\end{align}\n\\] Note that when sick individuals never recover (\\(\\beta = 0\\)), \\(C(t) = P(t)\\), as expected.\nA useful feature of the exponential growth regime is that all of our quantities of interest grow exponentially at the same rate and are thus in proportion to one another: \\[\n\\begin{align}\nI(t) & = (r + \\beta) P(t) \\\\\nC(t) & \\sim \\frac{r + \\beta}{r} P(t), \\text{as } t \\to \\infty.\n\\end{align}\n\\]\nRearranging, we see that the fraction of cumulative incidence that are currently infected is controled by the ratio of growth rate to recovery rate: \\[\n\\frac{P(t)}{C(t)} \\sim \\frac{r}{r + \\beta}.\n\\] This ranges from zero when the epidemic grows slowly and individuals recover quickly, to one wher the epidemic grows much faster than recovery. For a virus like SARS-CoV-2, where both doubling and recovery times were on the order of a week, we expect on the order of half of the cumulative infections to be currently sick during the exponetial phase.\n\n\nDomain of validity\nAs described above, the approximation of deterministic exponential growth is valid when the number of infected individuals is large enough that the growth is roughly deteriminstic but small enough that most of the population is still susceptible. Here we’ll approximate those bounds.\n\nDeterministic growth\nA simple stochastic model of the start of an epidemic is that infections and recoveries in a small increment of time \\(dt\\) are independent of one another, that is: \\[\n\\begin{align}\n\\text{\\# Births} & \\sim \\text{Poisson}(\\alpha N P dt) \\\\\n\\text{\\# Deaths} & \\sim \\text{Poisson}(\\beta P dt).\n\\end{align}\n\\] This implies that the change in prevalence, \\(dP = (\\text{\\# Births}) - (\\text{\\# Deaths})\\), during \\([t, t+dt)\\) has mean and variance: \\[\n\\begin{align}\n\\mathbb{E}(dP) & = (\\alpha N - \\beta) P dt \\\\\n               & = r P dt \\\\\n\\mathbb{V}(dP) & = (\\alpha N + \\beta) P dt \\\\\n               & = (r + 2 \\beta) P dt.\n\\end{align}\n\\]\nA deterministic approximation is good when the coefficient of variation of \\(dP\\) is much smaller than one, that is: \\[\n\\begin{align}\n\\frac{\\sqrt{\\mathbb{V}(dP)}}{\\mathbb{E}(dP)} & \\ll 1 \\\\\n\\frac{\\sqrt{(r + 2 \\beta) P dt}}{r P dt} & \\ll 1 \\\\.\n\\end{align}\n\\] Some algebra gives the condition that the prevalence is larger than a critical value: \\[\nP \\gg \\frac{r + 2 \\beta}{r^2 dt}.\n\\] It remains to choose a suitable increment of time. We expect the prevalence to change significantly (\\(\\mathbb{E}(\\frac{dP}{P}) = 1\\)) on the timescale \\(r^{-1}\\) so \\(dt = r^{-1}\\) is a reasonable choice as long as \\(r \\gtrsim \\beta\\). This gives the condition \\[\nP \\gg \\frac{r + 2 \\beta}{r} = 1 + 2 \\beta / r.\n\\] When this condition is met, we expect the epidemic to grow approximately deterministically at rate \\(r\\).\nNote that this condition is violated at \\(t = 0\\). We must thus reinterpret \\(t = 0\\) as the effective time that the epidemic would have infected a single person if our derterministic approximation for \\(P \\gg 1 + 2\\beta/r\\) were extended backwards in time. [TODO: estimate the error in cumulative infections introduced by making this approximation.]\n\n\nExponential growth\nAs the virus spreads, the number of susceptible people declines, reducing the rate of spread and resulting in sub-exponential growth. Let the number of susceptible people be \\(S(t)\\), then the incidence per time is: \\[\nI(t) = \\alpha S(t) P(t).\n\\] Above, we assumed that \\(S(t) = N\\). \\(I(t)\\) will thus be reduced by half by the time \\(S(t) = N / 2\\). This is a convenient upper bound on the domain of validity.\nAssuming that anyone previously infected is no longer susceptible, we have: \\[\n\\begin{align}\nS(t) & \\gg N / 2 \\\\\nN - C(t) & \\gg N / 2 \\\\\nC(t) & \\ll N / 2.\n\\end{align}\n\\]\nUsing \\(C = \\frac{r + \\beta}{r} P\\) in the exponential regime, we can put this together with our lower bound of the deterministic approximation to get the region of validity: \\[\n1 + 2\\beta / r \\ll P \\ll \\frac{N}{2 (1 + \\beta / r)}.\n\\] This shows that as long as the population is large (\\(N \\gg 1\\)), and the growth rate is not too slow compared to the recovery rate (\\(r \\gtrsim \\beta\\)), there will be a wide regime of approximately deterministic exponential growth."
  },
  {
    "objectID": "notebooks/2024-02-02_CostEstimateMVP.html#sampling-sequencing-and-detection",
    "href": "notebooks/2024-02-02_CostEstimateMVP.html#sampling-sequencing-and-detection",
    "title": "NAO Cost Estimate MVP",
    "section": "Sampling, sequencing, and detection",
    "text": "Sampling, sequencing, and detection\nConsider a set of samples taken at times \\(\\{t_0, t_1, \\ldots\\}\\), where \\(t_0 \\ge 0\\) is the the first sampling time after the virus begins to spread. (Presumably sampling has been ongoing but we’ll ignore the earlier samples.) We sequence sample \\(i\\) to a total depth of \\(n_i\\) reads and find that \\(k_i\\) match the sequence of the pandemic virus.\nWe consider the virus to be detected when the cumulative number of viral reads reaches a threshold \\(\\hat{K}\\). We’ll define \\(\\hat{t}\\) to be the smallest sample time \\(t_i\\) such that: \\[\nK_i \\equiv \\sum_{j=0}^i k_j \\geq \\hat{K}.\n\\] We also want to consider the effect of the delay, \\(t_d\\), between when the critical sample is collected and when we have processed, sequenced, and analyzed the data.\nWe will assess the success of a method by the (population-scaled) cumulative incidence at the time of detection, accounting for the delay: \\[\nc(\\hat{t} + t_d) \\equiv C(\\hat{t} + t_d) / N\n\\]\nBecause we’re interested in population-scale quantities and because we’ve shown that our model is not accurate when there are only a few individuals, we will make two assumptions:\n\nThe prevalence at detection \\(P(t) = e^{rt} \\sim N\\), as \\(N \\to \\infty\\).\nWe can neglect any smaller terms that do not scale with \\(N\\).\n\nUsing the equations above for cumulative incidence, we have the following simplification: \\[\n\\begin{align}\nc(t) & = \\frac{1 + \\frac{r + \\beta}{r}\\left(e^{rt} - 1\\right)}{N} \\\\\n     & = \\frac{r + \\beta}{r} \\frac{e^{rt}}{N} + \\mathcal{O}(N^{-1}).\n\\end{align}\n\\]\nNote that we can immediately see the cost of delay: \\[\nc(\\hat{t} + t_d) = \\frac{r + \\beta}{r} \\frac{e^{r\\hat{t}}}{N} e^{r t_d}.\n\\] With an exponentially growing pandemic, post-sampling delay multiplies our cumulative incidence at detection by \\(e^{rt_d}\\).\nIn the rest of this section, we answer the question:\n\nIf we want to detect the virus by the time it reaches a particular cumulative incidence, \\(\\hat{c}\\), how much sequencing do we need to do per unit time?\n\n\nSampling to relative abundance\nHere we make two more important deterministic assumptions:\n\nThe viral read counts are deterministic, given the expected relative abundance, \\(a_i\\), in each sample: \\(k_i = n_i a_i\\).\nThe expected relative abundance is itself a deterministic functional of the prevalence: \\[\na_i = b \\int \\frac{P(t)}{N} \\rho_i(t) dt.\n\\] Here, \\(\\rho_i\\) is a density that says how much of sample \\(i\\) was collected at different times, and \\(b\\) is a fixed conversion factor between prevalence and relative abundance. (Note \\(b = RA_p(1)\\) in the P2RA manuscript.)\n\nFrom here on, it will simplify thing to specify two concrete sampling schemes. Both schemes collect evenly-spaced samples with the same depth:\n\n\\(n_i = n\\)\n\\(t_i - t_{i-1} = \\delta t\\)\n\\(0 \\leq t_0 &lt; \\delta t\\), since we don’t know how long before the first sample the pandemic began.\n\nOur two schemes differ in when the material for the samples is collected. They are:\n\nContinuous sampling: \\(\\rho_i(t) = \\frac{1}{\\delta t}\\) for \\(t \\in [t_{i-1}, t_i)\\).\nGrab sampling: \\(\\rho_i(t) = \\delta(t - t_i)\\), i.e., the whole sample is collected at \\(t_i\\).\n\n\n\nContinuous sampling\nFirst, we use the assumptions in the previous section to calculate the cumulative reads by sample \\(i\\): \\[\n\\begin{align}\nK_i & = \\sum_{j=0}^{i} n a_i \\\\\n    & = n \\sum_{j=0}^{i} b \\int_{t_{j-1}}^{t_j} \\frac{P(t)}{N} \\frac{dt}{\\delta t} \\\\\n    & = \\frac{n b}{\\delta t} \\int_{0}^{t_i} \\frac{P(t)}{N} dt.\n\\end{align}\n\\]\nAt this point, we could make the substitution \\(I(t) = (r + \\beta) P(t)\\) from our exponential model. This would give us: \\[\n\\begin{align}\nK_i & = \\frac{nb}{\\delta t} (r + \\beta) \\int_{0}^{t_i} \\frac{I(t)}{N} dt \\\\\n    & = \\frac{n b (r + \\beta)}{\\delta t} c(t_i),\n\\end{align}\n\\] which is the result from the simple calculation in the P2RA manuscript, using the conversion factor \\(r + \\beta\\) to convert between \\(RA_p\\) and \\(RA_i\\), which is only valid in the exponential growth case.\nInstead, we’ll proceed in a more general way, which will extend to the grab sampling model and account for the discrete nature of our sampling.\nUsing our exponential growth model for \\(P\\), we have: \\[\n\\begin{align}\nK_i & = \\frac{nb}{\\delta t}\n\\end{align}\n\\]\nOne complication is that the sample \\(i\\) that \\(K_i \\geq \\hat{K}\\) depends on the precise timing of our samples. In the figure below, the three colored series show \\(K_i\\) for three series of samples that differ only by the relative timing of their sample collection (e.g., all samples are collected weekly, but one series collects on Mondays, another on Tuesdays, etc.) Note that the red figure, whose first sample was taken latest relative to the start of the pandemic, crosses the threshold (solid black line) one sample earlier than the others.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nt_max = 8\nr = 1.0\nk_hat = 100\ndt = 1.0\ntime = np.arange(0, t_max, 0.1)\nplt.plot(time, np.exp(r * time), \"k:\")\nfor t_0 in [0.25, 0.5, 0.75]:\n    t = np.arange(t_0, t_max, dt)\n    k = np.exp(r * t)\n    plt.plot(t, k, \"o\", label=t_0)\nplt.hlines(k_hat, 0, t_max, \"k\")\nplt.yscale(\"log\")\nplt.ylabel(\"Cumulative reads $K$\")\nplt.xlabel(\"Time\")\nplt.title(\"Effect of sample timing\")\nplt.legend(title=\"$t_0$\")\n\n\n\n\n\nWe account for this effect by splitting \\(\\hat{t}\\) into two components:\n\nThe earliest time possible to detect, \\(t^{\\star}\\), found by allowing \\(K\\) to vary continously in time (dotted line above)\nThe residual time waiting for the next sample to be collected. Because we have no reason to expect this waiting time to take any particular value, we average it over a uniform distribution.\n\nFirst, we will find an implicit equation for \\(t^{\\star}\\): \\[\n\\begin{align}\n\\hat{K} & = \\frac{nb}{\\delta t} \\int_0^{t^{\\star}} \\frac{P(t)}{N} dt \\\\\n        & = \\frac{nb}{\\delta t} \\left(\\frac{e^{r t^{\\star}}}{rN} + \\mathcal{O}(N^{-1})\\right)\\\\\n\\end{align}\n\\]\nNext we average our target cumulative incidence over the sample waiting time: \\[\n\\begin{align}\n\\hat{c} & = \\int_{t^{\\star}}^{t^{\\star} + \\delta t} c(\\hat{t} + t_d) \\frac{d \\hat{t}}{\\delta t} \\\\\n        & = \\int_{t^{\\star}}^{t^{\\star} + \\delta t}\n                \\frac{r + \\beta}{r} \\frac{e^{r(\\hat{t} + t_d)}}{N} \\frac{d \\hat{t}}{\\delta t}\n                + \\mathcal{O}(N^{-1}) \\\\\n        & = (r + \\beta)\n            \\left(\\frac{e^{r t^{\\star}}}{r N} \\right)\n            \\left(\\frac{e^{r \\delta t} - 1}{r \\delta t}\\right)\n            e^{r t_d}\n            + \\mathcal{O}(N^{-1}) \\\\\n\\end{align}\n\\] The the third term in parentheses is the cost incurred from having widely spaced samples. As \\(r \\delta t \\to 0\\), this cost goes to zero. When \\(r \\delta t\\) is large, this cost grows exponentially.\nFinally, we notice that the second term in parentheses appears in our implicit equation for \\(t^{\\star}\\) above. Substituting and rearranging gives the sequencing effort required to detect by cumulative incidence \\(\\hat{c}\\): \\[\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}} \\right)\n    \\left(\\frac{e^{r \\delta t} - 1}{r \\delta t}\\right)\n    e^{r t_d}\n    + \\mathcal{O}(N^{-1})\n\\] Some observations:\n\nFaster-growing and faster-recovering viruses require more sequencing because the current prevalence is a smaller fraction of the cumulative incidence.\nHigher detection thresholds, lower P2RA factors (\\(b\\)), and lower target cumulative incidence, require more sequencing.\nThere is a cost associated with longer sampling intervals and delays between collection and analysis. The latter grows faster than the former.\n\n\n\nGrab sampling\nNow we turn to grab sampling at the collection times \\(\\{t_0, t_1, \\ldots\\}\\). The analysis is the same as for continuous sampling, except that we will have a different implicit equation for \\(t^{\\star}\\).\nWith grab sampling, \\(\\rho_i(t) = \\delta(t - t_i)\\), we have cumulative counts: \\[\n\\begin{align}\nK_i & = n \\sum_{j=0}^{i} b \\frac{P(t_i)}{N} \\\\\n    & = \\frac{n b}{N} \\sum_{j=0}^{i} e^{r (t_0 + j \\delta t)} \\\\\n    & = \\frac{n b}{N} e^{r t_0} \\frac{e^{r(i+1)\\delta t} - 1}{e^{r\\delta t} - 1} \\\\\n    & = \\frac{n b}{\\delta t} \\left(\\frac{e^{r t_i}}{rN}\\right)\n        \\left(\\frac{r \\delta t \\, e^{r \\delta t}}{e^{r \\delta t} - 1}\\right)\n        + \\mathcal{O}(N^{-1}). \\\\\n\\end{align}\n\\] Making the continuous time substitution as before, gives \\[\n\\hat{K} = \\frac{n b}{\\delta t} \\left(\\frac{e^{r t^{\\star}}}{rN}\\right)\n            \\left(\\frac{r \\delta t \\, e^{r \\delta t}}{e^{r \\delta t} - 1}\\right)\n            + \\mathcal{O}(N^{-1}).\n\\] The first two terms are identical to the continuous sampling case. The third term is the effect of grab sampling: because the sample is collected entirely at the end of the interval, the prevalence is higher than the average over the interval and you get more reads.\nUsing this equation for \\(t^{\\star}\\), we find that the required sequencing effort for grab sampling is: \\[\n\\frac{n}{\\delta t} = (r + \\beta) \\left(\\frac{\\hat{K}}{b \\hat{c}} \\right)\n    \\left(\\frac\n        {e^{-r\\delta t} {\\left(e^{r \\delta t} - 1\\right)}^2}\n        {{\\left(r \\delta t\\right)}^2}\n        \\right)\n    e^{r t_d}\n    + \\mathcal{O}(N^{-1})\n\\] This is similar to the continous sampling case but with weaker dependence on the sampling interval:\n\n\nCode\nt = np.arange(0.01, 2, 0.01)\nplt.plot(t, (np.exp(t) - 1) / t, label=r\"Continuous sampling\")\nplt.plot(t, np.exp(-t) * ((np.exp(t) - 1) / t) ** 2, label=r\"Grab sampling\")\nplt.legend()\nplt.title(\"Effect of wider sampling intervals\")\nplt.xlabel(\"Sampling interval, $r \\delta t$\")\nplt.ylabel(\"Multiplicative effect on required sequencing depth\")"
  },
  {
    "objectID": "notebooks/2024-02-02_CostEstimateMVP.html#numerical-example",
    "href": "notebooks/2024-02-02_CostEstimateMVP.html#numerical-example",
    "title": "NAO Cost Estimate MVP",
    "section": "Numerical example",
    "text": "Numerical example\nA function that calculates the depth required per unit time to detect by a given cumulative incidence:\n\ndef depth_required(\n    growth_rate: float,\n    recovery_rate: float,\n    read_threshold: int,\n    p2ra_factor: float,\n    cumulative_incidence_target: float,\n    sampling_interval: float,\n    sampling_scheme: str,\n    delay: float,\n) -&gt; float:\n    leading_term = (\n        (growth_rate + recovery_rate)\n        * read_threshold\n        / (p2ra_factor * cumulative_incidence_target)\n    )\n    x = growth_rate * sampling_interval\n    if sampling_scheme == \"continuous\":\n        sampling_term = (np.exp(x) - 1) / x\n    elif sampling_scheme == \"grab\":\n        sampling_term = np.exp(-x) * ((np.exp(x) - 1) / x) ** 2\n    else:\n        raise ValueError(\"sampling_scheme must be continuous or grab\")\n    delay_term = np.exp(growth_rate * delay)\n    return leading_term * sampling_term * delay_term\n\nWe’ll measure time in days. Let’s assume:\n\nA virus with a doubling time of 1 week and recovery timescale of two weeks\nA read threshold of 100 viral reads\nAn \\(RA_i(1%)\\) of 1e-7 (approximate median SARS-CoV2 in Rothman)\nA cumulative incidence target of 1%\nA delay of one week for sample processing and sequencing\nVary the sampling scheme and sampling interval (from daily to monthly)\n\n\nr = np.log(2) / 7\nbeta = 1 / 14\nk_hat = 100\nra_i_01 = 1e-7\n# Convert from weekly incidence to prevalence and per 1% to per 1\nb = ra_i_01 * 100 * (r + beta) * 7\nc_hat = 0.01\ndelta_t = np.arange(1.0, 30, 1)\nt_d = 7.0\n\nn_cont = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"continuous\", t_d)\nn_grab = depth_required(r, beta, k_hat, b, c_hat, delta_t, \"grab\", t_d)\nplt.plot(delta_t, 30 * n_cont, label=\"continuous\")\nplt.plot(delta_t, 30 * n_grab, label=\"grab\")\nplt.legend(title=\"Sampling scheme\")\nplt.ylabel(\"Reads required per month\")\nplt.xlabel(\"Sampling interval (days)\")"
  },
  {
    "objectID": "notebooks/2024-02-02_CostEstimateMVP.html#next-steps",
    "href": "notebooks/2024-02-02_CostEstimateMVP.html#next-steps",
    "title": "NAO Cost Estimate MVP",
    "section": "Next steps",
    "text": "Next steps\n\nVary the other parameters, make plots of interest?\nConsider per-sample and per-read costs. Find the optimal sampling interval for each scheme.\nRelax deterministic assumptions:\n\nPoisson read count noise\n“Excess” read count noise\nRandomness in the spread of the virus\n\nConsider multiple sites\n\nAccounting for global spread\nDetermining how to spread monitoring effort across locations"
  },
  {
    "objectID": "notebooks/2023-10-10-qpcr_analysis.html",
    "href": "notebooks/2023-10-10-qpcr_analysis.html",
    "title": "2023-10-10 Analyze prefilter experiment qPCR",
    "section": "",
    "text": "Determine which prefiltration method produces better nucleic acid results."
  },
  {
    "objectID": "notebooks/2023-10-10-qpcr_analysis.html#objectives",
    "href": "notebooks/2023-10-10-qpcr_analysis.html#objectives",
    "title": "2023-10-10 Analyze prefilter experiment qPCR",
    "section": "",
    "text": "Determine which prefiltration method produces better nucleic acid results."
  },
  {
    "objectID": "notebooks/2023-10-10-qpcr_analysis.html#preliminary-work",
    "href": "notebooks/2023-10-10-qpcr_analysis.html#preliminary-work",
    "title": "2023-10-10 Analyze prefilter experiment qPCR",
    "section": "Preliminary work",
    "text": "Preliminary work\nExported csv files from Olivia’s eds file uploads."
  },
  {
    "objectID": "notebooks/2023-10-10-qpcr_analysis.html#data-import",
    "href": "notebooks/2023-10-10-qpcr_analysis.html#data-import",
    "title": "2023-10-10 Analyze prefilter experiment qPCR",
    "section": "Data import",
    "text": "Data import\n\nlibrary(here)\n\nhere() starts at /Users/dan/notebook\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(broom)\n\n\ndata_dir &lt;-\n  here(\"~\", \"airport\", \"[2023-09-22] New Processing Tests\")\nfilename_pattern &lt;- \"_Results_\"\n\n\ncol_types &lt;- list(\n  Target = col_character(),\n  Cq = col_double()\n)\nraw_data &lt;- list.files(\n  here(data_dir, \"qpcr\"),\n  pattern = filename_pattern,\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind()\n\n[1] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_Cov2_PMMV_Results_20231010_125053.csv\"\n[2] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_CrA_16S_Results_20231010_125152.csv\"  \n[3] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_Noro_Results_20231010_125241.csv\"     \n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\nOne or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\nprint(raw_data)\n\n# A tibble: 165 × 21\n    Well `Well Position` Omit  Sample   Target Task     Reporter Quencher\n   &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1     1 A1              FALSE 1A       Cov2   UNKNOWN  FAM      NFQ-MGB \n 2     2 A2              FALSE 1A       Cov2   UNKNOWN  FAM      NFQ-MGB \n 3     3 A3              FALSE 1A       Cov2   UNKNOWN  FAM      NFQ-MGB \n 4     4 A4              FALSE 10000.0  Cov2   STANDARD FAM      NFQ-MGB \n 5     5 A5              FALSE 10000.0  Cov2   STANDARD FAM      NFQ-MGB \n 6     6 A6              FALSE 10000.0  Cov2   STANDARD FAM      NFQ-MGB \n 7     7 A7              FALSE 1A       PMMV   UNKNOWN  FAM      NFQ-MGB \n 8     8 A8              FALSE 1A       PMMV   UNKNOWN  FAM      NFQ-MGB \n 9     9 A9              FALSE 1A       PMMV   UNKNOWN  FAM      NFQ-MGB \n10    10 A10             FALSE 800000.0 PMMV   STANDARD FAM      NFQ-MGB \n# ℹ 155 more rows\n# ℹ 13 more variables: `Amp Status` &lt;chr&gt;, `Amp Score` &lt;dbl&gt;,\n#   `Curve Quality` &lt;lgl&gt;, `Result Quality Issues` &lt;lgl&gt;, Cq &lt;dbl&gt;,\n#   `Cq Confidence` &lt;dbl&gt;, `Cq Mean` &lt;dbl&gt;, `Cq SD` &lt;dbl&gt;,\n#   `Auto Threshold` &lt;lgl&gt;, Threshold &lt;dbl&gt;, `Auto Baseline` &lt;lgl&gt;,\n#   `Baseline Start` &lt;dbl&gt;, `Baseline End` &lt;dbl&gt;\n\n\n\nraw_data |&gt; count(Target)\n\n# A tibble: 5 × 2\n  Target     n\n  &lt;chr&gt;  &lt;int&gt;\n1 16S       36\n2 Cov2      36\n3 CrA       36\n4 Noro      21\n5 PMMV      36\n\n\n\ntidy_data &lt;- raw_data |&gt;\n  mutate(\n    group = str_extract(Sample, \"^[0-9]\"),\n    replicate = str_extract(Sample, \"[A-Z]$\"),\n    quantity = as.double(Sample),\n  ) |&gt;\n  glimpse()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `quantity = as.double(Sample)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nRows: 165\nColumns: 24\n$ Well                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"10000.0\", \"10000.0\", \"10000…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"STANDARD\", \"…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.3915582, 1.4014582, 1.4073581, 1.4090574, 1.…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.15919, 32.98389, 32.66178, 22.39386, 22.220…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9759085, 0.9891340, 0.9883204, 0.9891666, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 32.93495, 32.93495, 32.93495, 22.31821, 22.318…\n$ `Cq SD`                 &lt;dbl&gt; 0.25229116, 0.25229116, 0.25229116, 0.08875503…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.2999157, 0.2999157, 0.2999157, 0.2999157, 0.…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 27, 27, 26, 16, 16, 16, 24, 23, 23, 20, 20, 19…\n$ group                   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ replicate               &lt;chr&gt; \"A\", \"A\", \"A\", NA, NA, NA, \"A\", \"A\", \"A\", NA, …\n$ quantity                &lt;dbl&gt; NA, NA, NA, 1e+04, 1e+04, 1e+04, NA, NA, NA, 8…\n\n\n\namp_data &lt;- list.files(\n  here(data_dir, \"qpcr\"),\n  pattern = \"Amplification Data\",\n  recursive = TRUE,\n  full.names = TRUE,\n) |&gt;\n  print() |&gt;\n  map(function(f) {\n    read_csv(f,\n      skip = 23,\n      col_types = col_types,\n    )\n  }) |&gt;\n  list_rbind() |&gt;\n  left_join(tidy_data, by = join_by(Well, `Well Position`, Sample, Omit, Target)) |&gt;\n  glimpse()\n\n[1] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_Cov2_PMMV_Amplification Data_20231010_125053.csv\"\n[2] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_CrA_16S_Amplification Data_20231010_125152.csv\"  \n[3] \"/Users/dan/airport/[2023-09-22] New Processing Tests/qpcr/2023-10-09_Noro_Amplification Data_20231010_125241.csv\"     \n\n\nWarning: The following named parsers don't match the column names: Cq\nThe following named parsers don't match the column names: Cq\nThe following named parsers don't match the column names: Cq\n\n\nRows: 6,600\nColumns: 27\n$ Well                    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Well Position`         &lt;chr&gt; \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\"…\n$ `Cycle Number`          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n$ Target                  &lt;chr&gt; \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\", \"Cov2\"…\n$ Rn                      &lt;dbl&gt; 0.6443956, 0.6382574, 0.6284555, 0.6179680, 0.…\n$ dRn                     &lt;dbl&gt; 2.836028e-02, 2.374099e-02, 1.545804e-02, 6.48…\n$ Sample                  &lt;chr&gt; \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\", \"1A\"…\n$ Omit                    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ Task                    &lt;chr&gt; \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\", \"U…\n$ Reporter                &lt;chr&gt; \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM\", \"FAM…\n$ Quencher                &lt;chr&gt; \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"NFQ-MGB\", \"N…\n$ `Amp Status`            &lt;chr&gt; \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP\", \"AMP…\n$ `Amp Score`             &lt;dbl&gt; 1.391558, 1.391558, 1.391558, 1.391558, 1.3915…\n$ `Curve Quality`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Result Quality Issues` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Cq                      &lt;dbl&gt; 33.15919, 33.15919, 33.15919, 33.15919, 33.159…\n$ `Cq Confidence`         &lt;dbl&gt; 0.9759085, 0.9759085, 0.9759085, 0.9759085, 0.…\n$ `Cq Mean`               &lt;dbl&gt; 32.93495, 32.93495, 32.93495, 32.93495, 32.934…\n$ `Cq SD`                 &lt;dbl&gt; 0.2522912, 0.2522912, 0.2522912, 0.2522912, 0.…\n$ `Auto Threshold`        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ Threshold               &lt;dbl&gt; 0.2999157, 0.2999157, 0.2999157, 0.2999157, 0.…\n$ `Auto Baseline`         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE…\n$ `Baseline Start`        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ `Baseline End`          &lt;dbl&gt; 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27…\n$ group                   &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ replicate               &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ quantity                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nGroups 1 and 2 are two different prefiltration protocols.\n\ntidy_data |&gt;\n  count(group, Target, replicate) |&gt;\n  print(n = Inf)\n\n# A tibble: 40 × 4\n   group Target replicate     n\n   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;\n 1 0     16S    &lt;NA&gt;          9\n 2 1     16S    A             3\n 3 1     16S    B             3\n 4 1     16S    C             3\n 5 1     16S    &lt;NA&gt;          6\n 6 1     Cov2   A             3\n 7 1     Cov2   B             3\n 8 1     Cov2   C             3\n 9 1     Cov2   &lt;NA&gt;         15\n10 1     CrA    A             3\n11 1     CrA    B             3\n12 1     CrA    C             3\n13 1     CrA    &lt;NA&gt;         15\n14 1     Noro   A             3\n15 1     Noro   B             3\n16 1     Noro   C             3\n17 1     PMMV   A             3\n18 1     PMMV   B             3\n19 1     PMMV   C             3\n20 2     16S    A             3\n21 2     16S    B             3\n22 2     16S    C             3\n23 2     Cov2   A             3\n24 2     Cov2   B             3\n25 2     Cov2   C             3\n26 2     CrA    A             3\n27 2     CrA    B             3\n28 2     CrA    C             3\n29 2     Noro   A             3\n30 2     Noro   B             3\n31 2     Noro   C             3\n32 2     PMMV   A             3\n33 2     PMMV   B             3\n34 2     PMMV   C             3\n35 8     PMMV   &lt;NA&gt;         15\n36 &lt;NA&gt;  16S    &lt;NA&gt;          3\n37 &lt;NA&gt;  Cov2   &lt;NA&gt;          3\n38 &lt;NA&gt;  CrA    &lt;NA&gt;          3\n39 &lt;NA&gt;  Noro   &lt;NA&gt;          3\n40 &lt;NA&gt;  PMMV   &lt;NA&gt;          3"
  },
  {
    "objectID": "notebooks/2023-10-10-qpcr_analysis.html#kit-comparison",
    "href": "notebooks/2023-10-10-qpcr_analysis.html#kit-comparison",
    "title": "2023-10-10 Analyze prefilter experiment qPCR",
    "section": "Kit comparison",
    "text": "Kit comparison\n\ntidy_data |&gt;\n  filter(Task == \"UNKNOWN\") |&gt;\n  ggplot(mapping = aes(\n    x = Cq,\n    y = group,\n    color = replicate,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  facet_wrap(facets = ~Target, scales = \"free_x\")"
  },
  {
    "objectID": "notebooks/2023-10-10-qpcr_analysis.html#standard-curves",
    "href": "notebooks/2023-10-10-qpcr_analysis.html#standard-curves",
    "title": "2023-10-10 Analyze prefilter experiment qPCR",
    "section": "Standard curves",
    "text": "Standard curves\n\ntidy_data |&gt;\n  filter(Task == \"STANDARD\") |&gt;\n  ggplot(mapping = aes(\n    x = quantity,\n    y = Cq,\n  )) +\n  stat_summary(\n    fun.min = min,\n    fun.max = max,\n    fun = median,\n    position = position_dodge(width = 0.2),\n    size = 0.2\n  ) +\n  scale_x_log10() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(facets = ~Target, scales = \"free\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nfits &lt;- tibble()\n# Note: no standard for norovirus\nfor (target in c(\"16S\", \"Cov2\", \"CrA\", \"PMMV\")) {\n  fit &lt;- lm(Cq ~ log10(quantity),\n    data = filter(tidy_data, Task == \"STANDARD\", Target == target)\n  ) |&gt;\n    tidy() |&gt;\n    mutate(Target = target, efficiency = 10^-(1 / estimate) - 1)\n  fits &lt;- bind_rows(fits, fit)\n}\nprint(fits |&gt; filter(term == \"log10(quantity)\"))\n\n# A tibble: 4 × 7\n  term            estimate std.error statistic  p.value Target efficiency\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 log10(quantity)    -2.92    0.125      -23.3 5.44e-12 16S          1.20\n2 log10(quantity)    -2.54    0.121      -21.1 1.99e-11 Cov2         1.47\n3 log10(quantity)    -2.84    0.0806     -35.3 2.69e-14 CrA          1.25\n4 log10(quantity)    -2.72    0.0829     -32.8 6.90e-14 PMMV         1.33"
  },
  {
    "objectID": "notebooks/2023-10-10-qpcr_analysis.html#amplification-curves",
    "href": "notebooks/2023-10-10-qpcr_analysis.html#amplification-curves",
    "title": "2023-10-10 Analyze prefilter experiment qPCR",
    "section": "Amplification curves",
    "text": "Amplification curves\n\namp_data |&gt;\n  filter(Task == \"UNKNOWN\") |&gt;\n  ggplot(mapping = aes(\n    x = `Cycle Number`,\n    y = dRn,\n    color = as.factor(group),\n    group = Well\n  )) +\n  geom_line() +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10() +\n  facet_wrap(~Target, scales = \"free\")\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 49 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\namp_data |&gt;\n  filter(Task == \"NTC\") |&gt;\n  ggplot(mapping = aes(\n    x = `Cycle Number`,\n    y = dRn,\n    group = Well\n  )) +\n  geom_line() +\n  geom_line(mapping = aes(\n    x = `Cycle Number`,\n    y = Threshold\n  ), color = \"Grey\") +\n  scale_y_log10() +\n  facet_wrap(~Target, scales = \"free\")\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 149 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nplot_amp &lt;- function(data, color) {\n  ggplot(data, aes(x = `Cycle Number`, y = dRn)) +\n    geom_line(mapping = aes(\n      color = as.factor({{ color }}),\n      group = Well,\n    )) +\n    scale_y_log10(limits = c(1e-3, 1e1))\n}\n\nruler &lt;- function(y0_from, num_rules) {\n  y0 &lt;- 10^seq(from = y0_from, by = -1, length.out = num_rules)\n  rules &lt;- crossing(`Cycle Number` = amp_data$`Cycle Number`, y0 = y0) |&gt;\n    mutate(dRn = y0 * 2^`Cycle Number`)\n  geom_line(\n    data = rules,\n    mapping = aes(group = y0),\n    color = \"black\"\n  )\n}\n\nplot_amp_with_ruler &lt;- function(target, y0_from, num_rules) {\n  amp_data |&gt;\n    filter(!is.na(quantity), Target == target) |&gt;\n    plot_amp(quantity) +\n    ruler(y0_from, num_rules) +\n    geom_line(mapping = aes(\n      x = `Cycle Number`,\n      y = Threshold\n    ), color = \"Grey\") +\n    labs(title = target)\n}\n\n\nplot_amp_with_ruler(\"16S\", -5.5, 5)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 51 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 134 rows containing missing values (`geom_line()`).\n\n\n\n\nplot_amp_with_ruler(\"Cov2\", -6.5, 5)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 133 rows containing missing values (`geom_line()`).\n\n\n\n\nplot_amp_with_ruler(\"CrA\", -4.5, 5)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 34 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 133 rows containing missing values (`geom_line()`).\n\n\n\n\nplot_amp_with_ruler(\"PMMV\", -8, 5)\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 51 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 136 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "notebooks/2024-02-22_StochasticMVP.html#numerical-calculations",
    "href": "notebooks/2024-02-22_StochasticMVP.html#numerical-calculations",
    "title": "NAO Cost Estimate MVP – Adding noise",
    "section": "Numerical calculations",
    "text": "Numerical calculations\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial.polynomial import Polynomial\nfrom numpy.polynomial.hermite_e import HermiteE\nfrom scipy.special import factorial\nfrom scipy.stats import poisson, norm, gamma\n\n\ndef cgf_x_series(mu: float, nu: float, order: int = 4) -&gt; Polynomial:\n    coeffs = np.zeros(order + 1)\n    # No zeroth coefficient\n    k = np.arange(1, order + 1)\n    coeffs[1:] = nu * (mu / nu) ** k / k**2\n    return Polynomial(coeffs)\n\n\ndef expm1_series(order: int = 4) -&gt; Polynomial:\n    k = np.arange(order + 1)\n    return Polynomial(1.0 / factorial(k)) - 1\n\n\ndef cgf_y_series(mu: float, nu: float, order: int = 4):\n    return cgf_x_series(mu, nu, order)(expm1_series(order)).cutdeg(order)\n\n\ndef cumulant_from_cgf(cgf: Polynomial, order: int) -&gt; float:\n    return cgf.deriv(order)(0)\n\nSpot-check cumulants:\n\nmu = 2.0\nnu = 10.0\ncgf = cgf_x_series(mu, nu)(expm1_series()).cutdeg(4)\ncoeffs = [0, 1, (1, 1 / 2), (1, 3 / 2, 2 / 3), (1, 7 / 2, 4, 3 / 2)]\npredicted_cumulants = [mu * Polynomial(c)(mu / nu) for c in coeffs]\nfor k in range(5):\n    print(cumulant_from_cgf(cgf, k), predicted_cumulants[k])\n\n0.0 0.0\n2.0 2.0\n2.2 2.2\n2.6533333333333333 2.6533333333333333\n3.7439999999999998 3.744\n\n\nCheck variance:\n\nnu = 10.0\nmu = np.arange(1, 100)\nk2 = [cumulant_from_cgf(cgf_y_series(m, nu), 2) for m in mu]\nplt.plot(mu, k2)\nplt.plot(mu, mu + (1 / 2) * mu**2 / nu, \":\")\nplt.plot(mu, (1 / 2) * mu**2 / nu, \"--\")\n\n\n\n\nNote that it is growing quadratically, but the \\(\\mu\\) term is not negligible.\n\ndef cornish_fisher(*cumulants):\n    # cumulants = (k_1, k_2, ...)\n    order = len(cumulants)\n    if order &lt; 2:\n        raise ValueError(\"Order of approximation must be &gt;= 2\")\n    if order &gt; 4:\n        raise ValueError(\"Order of approximation must be &lt;= 4\")\n    sigma = np.sqrt(cumulants[1])\n    poly = HermiteE((0, 1))\n    if order &gt;= 3:\n        gamma_1 = cumulants[2] / sigma**3\n        h_1 = HermiteE((0, 0, 1)) / 6\n        poly += gamma_1 * h_1\n    if order &gt;= 4:\n        gamma_2 = cumulants[3] / sigma**4\n        h_2 = HermiteE((0, 0, 0, 1)) / 24\n        h_11 = -HermiteE((0, 1, 0, 2)) / 36\n        poly += gamma_2 * h_2 + gamma_1**2 * h_11\n    return cumulants[0] + sigma * poly\n\n\nChecking the Cornish-Fisher expansion against common distributions\nCheck against Poisson distribution:\n\norder = 4\np = np.arange(0.01, 1.0, 0.01)\nx = norm.ppf(p)\n\nfor lamb in [1, 2, 4, 8]:\n    poisson_cumulants = [lamb] * order\n    for o in range(2, order + 1):\n        cf_poisson = cornish_fisher(*poisson_cumulants[:o])\n        plt.plot(p, cf_poisson(x), label=o)\n    plt.plot(p, poisson(lamb).ppf(p), color=\"k\", linestyle=\"--\", label=\"exact\")\n    plt.legend()\n    plt.title(f\"$\\lambda$ = {lamb}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen \\(\\lambda &gt; 1\\), you quickly converge to a Gaussian distribution, and the higher-order corrections don’t matter.\nCheck against Gamma distribution:\n\nscale = 1\nk = np.arange(1, order + 1)\n\nfor shape in [1, 2, 4, 8]:\n    gamma_cumulants = factorial(k - 1) * shape * scale**k\n    for o in range(2, order + 1):\n        cf_gamma = cornish_fisher(*gamma_cumulants[:o])\n        plt.plot(p, cf_gamma(x), label=o)\n    plt.plot(\n        p, gamma(shape, scale=scale).ppf(p), color=\"k\", linestyle=\"--\", label=\"exact\"\n    )\n    plt.legend()\n    plt.title(f\"shape = {shape}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe shape parameter controls deviations from Gaussian. For shape &gt; 1, three terms gets you close and 4 gets you very close.\n\n\nCornish-Fisher expansions for the cumulative count distribution\nFor the cumulative count distribution, there are two parameters, \\(\\mu\\) (the mean) and \\(\\nu\\) (which determines the shape).\nFor small \\(\\nu\\), the noise is quickly dominated by the latent variable so the distribution goes to a constant shape, scaled by \\(\\mu\\).\nEven with \\(\\nu = 2\\), the CF expansion converges quickly. 3 terms is quite good, 4 is indistinguisable from higher:\n\norder = 6\nnu = 2.0\nfor mu in [1, 2, 4, 8, 32]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.legend()\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith larger \\(\\nu\\), the shape of the distribution is closer to Gaussian:\n\norder = 6\nnu = 10.0\nfor mu in [1, 10, 100, 1000]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.legend()\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith \\(\\nu = 100\\), there is a wide Poisson-dominated regime the convergence to Gaussian is quite fast.\n\norder = 4\nnu = 100.0\nfor mu in [1, 10, 100, 1000]:\n    cgf = cgf_y_series(mu, nu)\n    cumulants = [cumulant_from_cgf(cgf, k) for k in [1, 2, 3, 4]]\n    for o in range(2, order + 1):\n        cf = cornish_fisher(*cumulants[:o])\n        plt.plot(p, cf(x), label=o)\n    plt.legend()\n    plt.title(r\"$\\nu$\" f\" = {nu}, \" r\"$\\mu$ = \" f\"{mu}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPercentiles\nThe following show the mean (dashed grey lines) and the 10th percentile (black) of cumulative counts as a function of the mean \\(\\mu\\)“. Colored lines are the latent and poisson regime approximations. Each plot has a different shape parameter \\(\\nu\\).\n\norder = 4\nks = range(1, order + 1)\n# 10th percentile\np = 0.1\nx = norm.ppf(p)\n\nmus = np.arange(1, 100)\nfor nu in [4, 10, 100]:\n    cgfs = [cgf_y_series(mu, nu) for mu in mus]\n    cumulants = [[cumulant_from_cgf(cgf, k) for k in ks] for cgf in cgfs]\n    cumulants_latent = [\n        [factorial(k - 1) / k * nu * (mu / nu) ** k for k in ks] for mu in mus\n    ]\n    cumulants_poisson = [[mu for _ in ks] for mu in mus]\n    cf = [cornish_fisher(*cs)(x) for cs in cumulants]\n    cf_l = [cornish_fisher(*cs)(x) for cs in cumulants_latent]\n    cf_p = [cornish_fisher(*cs)(x) for cs in cumulants_poisson]\n    plt.plot(mus, cf, \"k\", label=\"full\")\n    plt.plot(mus, cf_l, color=\"C0\", label=\"latent\")\n    plt.plot(mus, cf_p, color=\"C1\", label=\"poisson\")\n    plt.plot(mus, mus, \"--\", color=\"grey\", label=\"mean\")\n    plt.xlabel(r\"$\\mu$\")\n    plt.ylabel(\"Cumulative counts\")\n    plt.title(r\"$\\nu = $\" f\"{nu}\")\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nSmaller \\(\\nu\\) means that we have a greater reduction of the 10th percentile from the mean.\nSmaller \\(\\nu\\) means that the 10th percentile increase linearly (as expected in the latent regime).\nAs \\(\\nu\\) gets larger, there is a wider Poisson-dominated regime where the tenth percentile increases like \\(\\mu + \\mu^{1/2} const.\\).\nUnless \\(\\nu\\) is very large, we can probably get away with the latent-regime approximation"
  }
]